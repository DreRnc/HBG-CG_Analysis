{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from data.load_data import load_monk, load_MLCup\n",
    "from src.MetricFunctions import get_metric_instance\n",
    "from src.model import MLP\n",
    "from src.Optimizers2 import HBG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "file_train = '/data/ML-CUP22-TR.csv'\n",
    "file_test = '/data/ML-CUP22-TS.csv'\n",
    "\n",
    "labels_train = ['x1','x2','x3','x4','x5','x6','x7','x8','x9','y1','y2']\n",
    "labels_test = ['x1','x2','x3','x4','x5','x6','x7','x8','x9']\n",
    "\n",
    "X_train, y_train = load_MLCup(path + file_train, labels_train)\n",
    "X_test = load_MLCup(path + file_test, labels_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Objective function: 447.80764435103464 - Loss: 447.8075295000699 - Gradient norm: 70.6049604133016\n",
      "Epoch 2 - Objective function: 447.30928869274186 - Loss: 447.30917385787944 - Gradient norm: 70.56253001493054\n",
      "Epoch 3 - Objective function: 446.36353490471737 - Loss: 446.3634198721973 - Gradient norm: 70.48320241392\n",
      "Epoch 4 - Objective function: 445.0171307133112 - Loss: 445.01701488202644 - Gradient norm: 70.37300328038184\n",
      "Epoch 5 - Objective function: 443.3128012259421 - Loss: 443.31268350789674 - Gradient norm: 70.23817505221308\n",
      "Epoch 6 - Objective function: 441.2894396186641 - Loss: 441.28931838324564 - Gradient norm: 70.08496143622567\n",
      "Epoch 7 - Objective function: 438.98230268376057 - Loss: 438.9821757346246 - Gradient norm: 69.91944204072408\n",
      "Epoch 8 - Objective function: 436.42321120129714 - Loss: 436.42307577701786 - Gradient norm: 69.74740833752536\n",
      "Epoch 9 - Objective function: 433.64075415797225 - Loss: 433.6406069495083 - Gradient norm: 69.57427342242548\n",
      "Epoch 10 - Objective function: 430.6604952631923 - Loss: 430.66033244274223 - Gradient norm: 69.40500921529909\n",
      "Epoch 11 - Objective function: 427.5051799249124 - Loss: 427.50499718195897 - Gradient norm: 69.24410577522167\n",
      "Epoch 12 - Objective function: 424.19494077357996 - Loss: 424.1947333549054 - Gradient norm: 69.09554829415906\n",
      "Epoch 13 - Objective function: 420.747499899394 - Loss: 420.7472626506365 - Gradient norm: 68.96280808289941\n",
      "Epoch 14 - Objective function: 417.17836614514863 - Loss: 417.1780935520979 - Gradient norm: 68.84884449037226\n",
      "Epoch 15 - Objective function: 413.5010260338672 - Loss: 413.5007122621967 - Gradient norm: 68.75611522009548\n",
      "Epoch 16 - Objective function: 409.72712717643304 - Loss: 409.7267661089631 - Gradient norm: 68.6865929427386\n",
      "Epoch 17 - Objective function: 405.86665327672625 - Loss: 405.8662385476216 - Gradient norm: 68.64178646758835\n",
      "Epoch 18 - Objective function: 401.9280901143203 - Loss: 401.9276151398694 - Gradient norm: 68.62276504175688\n",
      "Epoch 19 - Objective function: 397.9185821268573 - Loss: 397.91804013266784 - Gradient norm: 68.63018460562213\n",
      "Epoch 20 - Objective function: 393.84407942925947 - Loss: 393.8434634738464 - Gradient norm: 68.66431505533686\n",
      "Epoch 21 - Objective function: 389.709475291581 - Loss: 389.7087782864316 - Gradient norm: 68.72506775535787\n",
      "Epoch 22 - Objective function: 385.5187342505049 - Loss: 385.51794897678343 - Gradient norm: 68.81202271118015\n",
      "Epoch 23 - Objective function: 381.2750111518374 - Loss: 381.2741302739476 - Gradient norm: 68.92445495874199\n",
      "Epoch 24 - Objective function: 376.98076151454654 - Loss: 376.97977759080794 - Gradient norm: 69.06135985511553\n",
      "Epoch 25 - Objective function: 372.6378436733022 - Loss: 372.63674916402096 - Gradient norm: 69.22147706709669\n",
      "Epoch 26 - Objective function: 368.24761319886295 - Loss: 368.24640047209056 - Gradient norm: 69.4033131515439\n",
      "Epoch 27 - Objective function: 363.81101011690146 - Loss: 363.8096714521799 - Gradient norm: 69.60516270477363\n",
      "Epoch 28 - Objective function: 359.328639448853 - Loss: 359.32716703924336 - Gradient norm: 69.82512812874113\n",
      "Epoch 29 - Objective function: 354.8008455858749 - Loss: 354.7992315385619 - Gradient norm: 70.06113811971795\n",
      "Epoch 30 - Objective function: 350.2277809816119 - Loss: 350.2260173173659 - Gradient norm: 70.3109650312638\n",
      "Epoch 31 - Objective function: 345.60946961356876 - Loss: 345.6075482653392 - Gradient norm: 70.57224129799535\n",
      "Epoch 32 - Objective function: 340.945865618712 - Loss: 340.9437784296089 - Gradient norm: 70.84247513050639\n",
      "Epoch 33 - Objective function: 336.2369074584507 - Loss: 336.2346461793628 - Gradient norm: 71.1190657053308\n",
      "Epoch 34 - Objective function: 331.4825679132794 - Loss: 331.48012420035946 - Gradient norm: 71.39931807761496\n",
      "Epoch 35 - Objective function: 326.68290014981454 - Loss: 326.68026556204967 - Gradient norm: 71.68045803875984\n",
      "Epoch 36 - Objective function: 321.83808004438004 - Loss: 321.8352460414525 - Gradient norm: 71.95964712730417\n",
      "Epoch 37 - Objective function: 316.9484448892806 - Loss: 316.94540282990715 - Gradient norm: 72.23399797938869\n",
      "Epoch 38 - Objective function: 312.0145285519654 - Loss: 312.01126969289135 - Gradient norm: 72.50059017596007\n",
      "Epoch 39 - Objective function: 307.03709310494423 - Loss: 307.0336086007535 - Gradient norm: 72.75648670821704\n",
      "Epoch 40 - Objective function: 302.0171568970391 - Loss: 302.013437800927 - Gradient norm: 72.99875114154457\n",
      "Epoch 41 - Objective function: 296.9560189957672 - Loss: 296.9520562614076 - Gradient norm: 73.22446551233693\n",
      "Epoch 42 - Objective function: 291.8552798977065 - Loss: 291.8510643823311 - Gradient norm: 73.43074894283954\n",
      "Epoch 43 - Objective function: 286.7168583798642 - Loss: 286.71238084865394 - Gradient norm: 73.6147769077752\n",
      "Epoch 44 - Objective function: 281.54300435142466 - Loss: 281.5382554832992 - Gradient norm: 73.77380103456989\n",
      "Epoch 45 - Objective function: 276.33630756271407 - Loss: 276.3312779575873 - Gradient norm: 73.90516926811753\n",
      "Epoch 46 - Objective function: 271.09970203737254 - Loss: 271.09438222492287 - Gradient norm: 74.00634618301198\n",
      "Epoch 47 - Objective function: 265.8364661148603 - Loss: 265.83084656484846 - Gradient norm: 74.07493318289718\n",
      "Epoch 48 - Objective function: 260.55021802342355 - Loss: 260.5442891575699 - Gradient norm: 74.10868828990704\n",
      "Epoch 49 - Objective function: 255.24490694794093 - Loss: 255.23865915335438 - Gradient norm: 74.1055451988807\n",
      "Epoch 50 - Objective function: 249.92479961163988 - Loss: 249.91822325577007 - Gradient norm: 74.06363125274949\n",
      "Epoch 51 - Objective function: 244.5944624539864 - Loss: 244.5875479010506 - Gradient norm: 73.98128398852337\n",
      "Epoch 52 - Objective function: 239.25873955715824 - Loss: 239.2514771859729 - Gradient norm: 73.85706590859441\n",
      "Epoch 53 - Objective function: 233.92272654800507 - Loss: 233.9151067711323 - Gradient norm: 73.68977715007549\n",
      "Epoch 54 - Objective function: 228.59174077858498 - Loss: 228.58375406268286 - Gradient norm: 73.47846575551885\n",
      "Epoch 55 - Objective function: 223.2712881632806 - Loss: 223.26292505052893 - Gradient norm: 73.22243529093043\n",
      "Epoch 56 - Objective function: 217.96702712113267 - Loss: 217.95827825158625 - Gradient norm: 72.92124961024003\n",
      "Epoch 57 - Objective function: 212.68473013537675 - Loss: 212.6755862700833 - Gradient norm: 72.57473462747706\n",
      "Epoch 58 - Objective function: 207.4302434954809 - Loss: 207.42069554018659 - Gradient norm: 72.18297702652205\n",
      "Epoch 59 - Objective function: 202.2094458278278 - Loss: 202.19948485708045 - Gradient norm: 71.74631991076946\n",
      "Epoch 60 - Objective function: 197.0282060476494 - Loss: 197.0178233291022 - Gradient norm: 71.26535546843942\n",
      "Epoch 61 - Objective function: 191.89234137559 - Loss: 191.88152839430083 - Gradient norm: 70.74091480062626\n",
      "Epoch 62 - Objective function: 186.8075760566977 - Loss: 186.79632453921656 - Gradient norm: 70.17405512560056\n",
      "Epoch 63 - Objective function: 181.7795013978023 - Loss: 181.76780333583994 - Gradient norm: 69.56604463177334\n",
      "Epoch 64 - Objective function: 176.81353770191436 - Loss: 176.80138537538753 - Gradient norm: 68.9183453008917\n",
      "Epoch 65 - Objective function: 171.9148986268921 - Loss: 171.9022846261479 - Gradient norm: 68.23259406079626\n",
      "Epoch 66 - Objective function: 167.08855843219214 - Loss: 167.07547567922296 - Gradient norm: 67.51058265237113\n",
      "Epoch 67 - Objective function: 162.33922250446804 - Loss: 162.32566427294114 - Gradient norm: 66.75423660773497\n",
      "Epoch 68 - Objective function: 157.67130147286565 - Loss: 157.65726140680556 - Gradient norm: 65.96559373646772\n",
      "Epoch 69 - Objective function: 153.08888914098065 - Loss: 153.07436127196013 - Gradient norm: 65.14678250452984\n",
      "Epoch 70 - Objective function: 148.59574437748174 - Loss: 148.58072314019466 - Gradient norm: 64.30000066779328\n",
      "Epoch 71 - Objective function: 144.19527702411486 - Loss: 144.1797572702258 - Gradient norm: 63.42749449043417\n",
      "Epoch 72 - Objective function: 139.89053780067786 - Loss: 139.87451481086228 - Gradient norm: 62.531538839761524\n",
      "Epoch 73 - Objective function: 135.68421211370776 - Loss: 135.6676816078201 - Gradient norm: 61.61441840543237\n",
      "Epoch 74 - Objective function: 131.5786176107608 - Loss: 131.56157575608748 - Gradient norm: 60.67841024449284\n",
      "Epoch 75 - Objective function: 127.57570526651016 - Loss: 127.55814868408707 - Gradient norm: 59.72576780624764\n",
      "Epoch 76 - Objective function: 123.67706374121342 - Loss: 123.65898951020701 - Gradient norm: 58.758706544364365\n",
      "Epoch 77 - Objective function: 119.8839267167164 - Loss: 119.86533237688741 - Gradient norm: 57.77939117937421\n",
      "Epoch 78 - Objective function: 116.19718288997562 - Loss: 116.17806644226269 - Gradient norm: 56.789924634048134\n",
      "Epoch 79 - Objective function: 112.61738828865042 - Loss: 112.59774819392693 - Gradient norm: 55.79233862790272\n",
      "Epoch 80 - Objective function: 109.14478056690822 - Loss: 109.1246157429812 - Gradient norm: 54.78858588590675\n",
      "Epoch 81 - Objective function: 105.77929494126609 - Loss: 105.75860475819867 - Gradient norm: 53.7805338906107\n",
      "Epoch 82 - Objective function: 102.52058143496765 - Loss: 102.49936570881785 - Gradient norm: 52.76996008644313\n",
      "Epoch 83 - Objective function: 99.36802311391226 - Loss: 99.34628209899006 - Gradient norm: 51.75854842963195\n",
      "Epoch 84 - Objective function: 96.32075501633123 - Loss: 96.29848939608306 - Gradient norm: 50.74788716676762\n",
      "Epoch 85 - Objective function: 93.3776835010985 - Loss: 93.35489437773386 - Gradient norm: 49.739467718960455\n",
      "Epoch 86 - Objective function: 90.53750576469444 - Loss: 90.51419464767295 - Gradient norm: 48.73468454630763\n",
      "Epoch 87 - Objective function: 87.79872930343244 - Loss: 87.77489809693147 - Gradient norm: 47.73483586838973\n",
      "Epoch 88 - Objective function: 85.15969112474754 - Loss: 85.13534211423101 - Gradient norm: 46.74112512016923\n",
      "Epoch 89 - Objective function: 82.61857653840416 - Loss: 82.59371237641236 - Gradient norm: 45.75466302838951\n",
      "Epoch 90 - Objective function: 80.17343738479723 - Loss: 80.14806107607515 - Gradient norm: 44.77647020083924\n",
      "Epoch 91 - Objective function: 77.82220958262742 - Loss: 77.79632446870578 - Gradient norm: 43.807480129173044\n",
      "Epoch 92 - Objective function: 75.56272990175816 - Loss: 75.53633964509724 - Gradient norm: 42.848542514944285\n",
      "Epoch 93 - Objective function: 73.3927518887685 - Loss: 73.36586045657015 - Gradient norm: 41.90042683775567\n",
      "Epoch 94 - Objective function: 71.30996089243415 - Loss: 71.28257254022128 - Gradient norm: 40.96382609367486\n",
      "Epoch 95 - Objective function: 69.31198815402877 - Loss: 69.28410740908623 - Gradient norm: 40.039360641066956\n",
      "Epoch 96 - Objective function: 67.39642394292466 - Loss: 67.36805558768876 - Gradient norm: 39.12758209958673\n",
      "Epoch 97 - Objective function: 65.56082973153096 - Loss: 65.53197878700834 - Gradient norm: 38.22897725612698\n",
      "Epoch 98 - Objective function: 63.80274941522055 - Loss: 63.77342112451051 - Gradient norm: 37.34397193895127\n",
      "Epoch 99 - Objective function: 62.11971959267866 - Loss: 62.089919404666446 - Gradient norm: 36.4729348280039\n",
      "Epoch 100 - Objective function: 60.50927893019275 - Loss: 60.47901248347448 - Gradient norm: 35.61618117546808\n",
      "Epoch 101 - Objective function: 58.968976639937594 - Loss: 58.938249747031186 - Gradient norm: 34.773976416040384\n",
      "Epoch 102 - Objective function: 57.49638010744923 - Loss: 57.465198739339336 - Gradient norm: 33.94653965112749\n",
      "Epoch 103 - Objective function: 56.08908170737523 - Loss: 56.05745197843419 - Gradient norm: 33.13404699528492\n",
      "Epoch 104 - Objective function: 54.74470484939174 - Loss: 54.712633002712565 - Gradient norm: 32.33663477674972\n",
      "Epoch 105 - Objective function: 53.460909298026046 - Loss: 53.428401691198324 - Gradient norm: 31.554402586918094\n",
      "Epoch 106 - Objective function: 52.23539581116073 - Loss: 52.202458902514856 - Gradient norm: 30.787416176136023\n",
      "Epoch 107 - Objective function: 51.06591014233606 - Loss: 51.03255047767652 - Gradient norm: 30.035710195255433\n",
      "Epoch 108 - Objective function: 49.950246451733996 - Loss: 49.916470651578074 - Gradient norm: 29.29929078411287\n",
      "Epoch 109 - Objective function: 48.886250170015394 - Loss: 48.85206491734919 - Gradient norm: 28.57813800945602\n",
      "Epoch 110 - Objective function: 47.87182035808703 - Loss: 47.83723238664737 - Gradient norm: 27.87220815592327\n",
      "Epoch 111 - Objective function: 46.904911604476304 - Loss: 46.86992768756331 - Gradient norm: 27.181435874511756\n",
      "Epoch 112 - Objective function: 45.98353550035824 - Loss: 45.94816244018026 - Gradient norm: 26.505736193587197\n",
      "Epoch 113 - Objective function: 45.10576173047492 - Loss: 45.07000634802495 - Gradient norm: 25.84500639792894\n",
      "Epoch 114 - Objective function: 44.269718816261964 - Loss: 44.233587941721844 - Gradient norm: 25.19912778159352\n",
      "Epoch 115 - Objective function: 43.473594545494656 - Loss: 43.43709500916144 - Gradient norm: 24.56796728054673\n",
      "Epoch 116 - Objective function: 42.71563612072694 - Loss: 42.67877474445363 - Gradient norm: 23.951378991080574\n",
      "Epoch 117 - Objective function: 41.994150056747564 - Loss: 41.956933645888824 - Gradient norm: 23.349205580015905\n",
      "Epoch 118 - Objective function: 41.30750185524727 - Loss: 41.269937191099274 - Gradient norm: 22.761279592611636\n",
      "Epoch 119 - Objective function: 40.65411548289685 - Loss: 40.6162093156192 - Gradient norm: 22.18742466397085\n",
      "Epoch 120 - Objective function: 40.0324726770977 - Loss: 39.9942317191042 - Gradient norm: 21.627456639565363\n",
      "Epoch 121 - Objective function: 39.4411121017921 - Loss: 39.40254302159641 - Gradient norm: 21.081184610302646\n",
      "Epoch 122 - Objective function: 38.87862837392369 - Loss: 38.83973779042525 - Gradient norm: 20.548411867341432\n",
      "Epoch 123 - Objective function: 38.34367097942338 - Loss: 38.304465456618296 - Gradient norm: 20.028936781631398\n",
      "Epoch 124 - Objective function: 37.834943095967105 - Loss: 37.79542913806849 - Gradient norm: 19.522553612913406\n",
      "Epoch 125 - Objective function: 37.351200338213836 - Loss: 37.3113843851656 - Gradient norm: 19.02905325267523\n",
      "Epoch 126 - Objective function: 36.89124943978266 - Loss: 36.85113786315093 - Gradient norm: 18.548223905315645\n",
      "Epoch 127 - Objective function: 36.453946884870824 - Loss: 36.41354598409666 - Gradient norm: 18.079851711531784\n",
      "Epoch 128 - Objective function: 36.038197501144616 - Loss: 35.99751350014224 - Gradient norm: 17.62372131771175\n",
      "Epoch 129 - Objective function: 35.64295302435411 - Loss: 35.60199206843844 - Gradient norm: 17.179616394888633\n",
      "Epoch 130 - Objective function: 35.26721064402445 - Loss: 35.22597879715244 - Gradient norm: 16.747320110595147\n",
      "Epoch 131 - Objective function: 34.910011538561626 - Loss: 34.86851478087169 - Gradient norm: 16.326615556749765\n",
      "Epoch 132 - Objective function: 34.5704394071718 - Loss: 34.52868363280612 - Gradient norm: 15.917286136507837\n",
      "Epoch 133 - Objective function: 34.24761900513035 - Loss: 34.205610020325274 - Gradient norm: 15.519115912823299\n",
      "Epoch 134 - Objective function: 33.940714688143665 - Loss: 33.89845820957346 - Gradient norm: 15.131889921289822\n",
      "Epoch 135 - Objective function: 33.648928970820336 - Loss: 33.60643062418031 - Gradient norm: 14.755394449664443\n",
      "Epoch 136 - Objective function: 33.37150110360456 - Loss: 33.3287664224197 - Gradient norm: 14.389417286320922\n",
      "Epoch 137 - Objective function: 33.107705671920215 - Loss: 33.0647400965661 - Gradient norm: 14.033747939735928\n",
      "Epoch 138 - Objective function: 32.856851220723776 - Loss: 32.81366009764685 - Gradient norm: 13.688177830976233\n",
      "Epoch 139 - Objective function: 32.61827890716589 - Loss: 32.57486748829083 - Gradient norm: 13.352500461032044\n",
      "Epoch 140 - Objective function: 32.39136118361099 - Loss: 32.34773462592306 - Gradient norm: 13.026511554727161\n",
      "Epoch 141 - Objective function: 32.17550051285726 - Loss: 32.13166387814832 - Gradient norm: 12.710009182832808\n",
      "Epoch 142 - Objective function: 31.9701281170341 - Loss: 31.92608637180127 - Gradient norm: 12.40279386391769\n",
      "Epoch 143 - Objective function: 31.77470276132641 - Loss: 31.730460776812766 - Gradient norm: 12.10466864738118\n",
      "Epoch 144 - Objective function: 31.58870957338202 - Loss: 31.544272125749416 - Gradient norm: 11.81543917904037\n",
      "Epoch 145 - Objective function: 31.411658898997764 - Loss: 31.36703066962204 - Gradient norm: 11.53491375057366\n",
      "Epoch 146 - Objective function: 31.24308519444835 - Loss: 31.198270770327834 - Gradient norm: 11.262903334063846\n",
      "Epoch 147 - Objective function: 31.082545955617494 - Loss: 31.037549829886057 - Gradient norm: 10.999221602831438\n",
      "Epoch 148 - Objective function: 30.929620683910326 - Loss: 30.88444725644676 - Gradient norm: 10.74368493970391\n",
      "Epoch 149 - Objective function: 30.783909888769536 - Loss: 30.738563466895275 - Gradient norm: 10.496112433828646\n",
      "Epoch 150 - Objective function: 30.64503412647909 - Loss: 30.599518925736955 - Gradient norm: 10.256325867104762\n",
      "Epoch 151 - Objective function: 30.512633074821828 - Loss: 30.46695321982862 - Gradient norm: 10.024149691282506\n",
      "Epoch 152 - Objective function: 30.386364643054563 - Loss: 30.340524168420842 - Gradient norm: 9.799410996756997\n",
      "Epoch 153 - Objective function: 30.265904116577616 - Loss: 30.21990696788842 - Gradient norm: 9.58193947406478\n",
      "Epoch 154 - Objective function: 30.15094333560284 - Loss: 30.104793370453276 - Gradient norm: 9.371567369077047\n",
      "Epoch 155 - Objective function: 30.04118990706292 - Loss: 29.994890896143126 - Gradient norm: 9.168129432870385\n",
      "Epoch 156 - Objective function: 29.936366448955408 - Loss: 29.889922077179495 - Gradient norm: 8.97146286724442\n",
      "Epoch 157 - Objective function: 29.83620986627455 - Loss: 29.789623733948623 - Gradient norm: 8.78140726684438\n",
      "Epoch 158 - Objective function: 29.740470657653272 - Loss: 29.69374628167781 - Gradient norm: 8.597804558834154\n",
      "Epoch 159 - Objective function: 29.64891225181426 - Loss: 29.602053066916532 - Gradient norm: 8.42049894105112\n",
      "Epoch 160 - Objective function: 29.561310372912775 - Loss: 29.51431973290514 - Gradient norm: 8.249336819556193\n",
      "Epoch 161 - Objective function: 29.47745243384403 - Loss: 29.43033361290429 - Gradient norm: 8.084166746471306\n",
      "Epoch 162 - Objective function: 29.39713695658284 - Loss: 29.349893150553076 - Gradient norm: 7.924839358968928\n",
      "Epoch 163 - Objective function: 29.320173018623866 - Loss: 29.27280734632437 - Gradient norm: 7.771207320245466\n",
      "Epoch 164 - Objective function: 29.246379724594462 - Loss: 29.19889522914971 - Gradient norm: 7.623125263269894\n",
      "Epoch 165 - Objective function: 29.175585702120642 - Loss: 29.12798535229432 - Gradient norm: 7.4804497380514805\n",
      "Epoch 166 - Objective function: 29.107628621036998 - Loss: 29.059915312573445 - Gradient norm: 7.343039163114277\n",
      "Epoch 167 - Objective function: 29.04235473504547 - Loss: 28.99453129201501 - Gradient norm: 7.210753781802064\n",
      "Epoch 168 - Objective function: 28.979618444943696 - Loss: 28.931687621089505 - Gradient norm: 7.0834556239651425\n",
      "Epoch 169 - Objective function: 28.919281882561055 - Loss: 28.871246362645504 - Gradient norm: 6.961008473499629\n",
      "Epoch 170 - Objective function: 28.861214514560615 - Loss: 28.813076915708987 - Gradient norm: 6.843277842122939\n",
      "Epoch 171 - Objective function: 28.805292765284985 - Loss: 28.757055638324875 - Gradient norm: 6.730130949674419\n",
      "Epoch 172 - Objective function: 28.75139965784641 - Loss: 28.703065488640988 - Gradient norm: 6.621436711131243\n",
      "Epoch 173 - Objective function: 28.69942447268347 - Loss: 28.650995683457207 - Gradient norm: 6.517065730426463\n",
      "Epoch 174 - Objective function: 28.649262422830038 - Loss: 28.60074137348536 - Gradient norm: 6.416890301050855\n",
      "Epoch 175 - Objective function: 28.60081434516527 - Loss: 28.55220333458896 - Gradient norm: 6.320784413314616\n",
      "Epoch 176 - Objective function: 28.553986406937607 - Loss: 28.50528767429572 - Gradient norm: 6.228623768041323\n",
      "Epoch 177 - Objective function: 28.50868982687892 - Loss: 28.459905552899148 - Gradient norm: 6.140285796365966\n",
      "Epoch 178 - Objective function: 28.464840610249205 - Loss: 28.415972918489796 - Gradient norm: 6.0556496852148385\n",
      "Epoch 179 - Objective function: 28.42235929717572 - Loss: 28.373410255280014 - Gradient norm: 5.974596407957866\n",
      "Epoch 180 - Objective function: 28.38117072367374 - Loss: 28.332142344609586 - Gradient norm: 5.897008759646607\n",
      "Epoch 181 - Objective function: 28.341203794759565 - Loss: 28.292098038042923 - Gradient norm: 5.822771396184901\n",
      "Epoch 182 - Objective function: 28.302391269088883 - Loss: 28.25321004199102 - Gradient norm: 5.751770876724825\n",
      "Epoch 183 - Objective function: 28.264669554576237 - Loss: 28.215414713313933 - Gradient norm: 5.683895708540007\n",
      "Epoch 184 - Objective function: 28.227978514472984 - Loss: 28.178651865381326 - Gradient norm: 5.619036393601452\n",
      "Epoch 185 - Objective function: 28.192261283402782 - Loss: 28.142864584090127 - Gradient norm: 5.557085476068349\n",
      "Epoch 186 - Objective function: 28.15746409287431 - Loss: 28.107999053359045 - Gradient norm: 5.497937589908193\n",
      "Epoch 187 - Objective function: 28.12353610581132 - Loss: 28.074004389640184 - Gradient norm: 5.441489505875373\n",
      "Epoch 188 - Objective function: 28.090429259659818 - Loss: 28.04083248500752 - Gradient norm: 5.38764017710579\n",
      "Epoch 189 - Objective function: 28.058098117651443 - Loss: 28.00843785840134 - Gradient norm: 5.336290782624817\n",
      "Epoch 190 - Objective function: 28.026499727820156 - Loss: 27.976777514625937 - Gradient norm: 5.2873447681161965\n",
      "Epoch 191 - Objective function: 27.995593489388042 - Loss: 27.94581081071626 - Gradient norm: 5.240707883358646\n",
      "Epoch 192 - Objective function: 27.96534102615261 - Loss: 27.915499329305987 - Gradient norm: 5.196288215803367\n",
      "Epoch 193 - Objective function: 27.93570606652503 - Loss: 27.885806758646527 - Gradient norm: 5.153996219837261\n",
      "Epoch 194 - Objective function: 27.90665432988483 - Loss: 27.856698778942494 - Gradient norm: 5.113744741352052\n",
      "Epoch 195 - Objective function: 27.878153418931923 - Loss: 27.8281429546845 - Gradient norm: 5.075449037316719\n",
      "Epoch 196 - Objective function: 27.850172717731898 - Loss: 27.8001086326753 - Gradient norm: 5.039026790128072\n",
      "Epoch 197 - Objective function: 27.822683295164627 - Loss: 27.772566845459362 - Gradient norm: 5.004398116590228\n",
      "Epoch 198 - Objective function: 27.795657813500235 - Loss: 27.745490219879848 - Gradient norm: 4.971485571447157\n",
      "Epoch 199 - Objective function: 27.769070441839446 - Loss: 27.718852890500134 - Gradient norm: 4.9402141454616855\n",
      "Epoch 200 - Objective function: 27.74289677416806 - Loss: 27.6926304176396 - Gradient norm: 4.910511258098972\n",
      "Epoch 201 - Objective function: 27.717113751787437 - Loss: 27.66679970978561 - Gradient norm: 4.882306744931048\n",
      "Epoch 202 - Objective function: 27.691699589894473 - Loss: 27.641338950155124 - Gradient norm: 4.855532839931658\n",
      "Epoch 203 - Objective function: 27.66663370809555 - Loss: 27.61622752719051 - Gradient norm: 4.83012415287635\n",
      "Epoch 204 - Objective function: 27.641896664649643 - Loss: 27.591445968784726 - Gradient norm: 4.806017642101909\n",
      "Epoch 205 - Objective function: 27.617470094246002 - Loss: 27.566975880041237 - Gradient norm: 4.783152582911404\n",
      "Epoch 206 - Objective function: 27.59333664913119 - Loss: 27.542799884383605 - Gradient norm: 4.761470531936554\n",
      "Epoch 207 - Objective function: 27.56947994340987 - Loss: 27.518901567838988 - Gradient norm: 4.740915287788447\n",
      "Epoch 208 - Objective function: 27.545884500352408 - Loss: 27.49526542632872 - Gradient norm: 4.721432848340295\n",
      "Epoch 209 - Objective function: 27.522535702550655 - Loss: 27.471876815807374 - Gradient norm: 4.702971364993773\n",
      "Epoch 210 - Objective function: 27.499419744771558 - Loss: 27.448721905099806 - Gradient norm: 4.68548109428245\n",
      "Epoch 211 - Objective function: 27.47652358936564 - Loss: 27.425787631293442 - Gradient norm: 4.668914347163741\n",
      "Epoch 212 - Objective function: 27.453834924094895 - Loss: 27.403061657550165 - Gradient norm: 4.653225436344577\n",
      "Epoch 213 - Objective function: 27.431342122251433 - Loss: 27.38053233320925 - Gradient norm: 4.638370621976155\n",
      "Epoch 214 - Objective function: 27.409034204944813 - Loss: 27.358188656059294 - Gradient norm: 4.624308056040781\n",
      "Epoch 215 - Objective function: 27.386900805442536 - Loss: 27.336020236663504 - Gradient norm: 4.610997725738626\n",
      "Epoch 216 - Objective function: 27.364932135453767 - Loss: 27.314017264628543 - Gradient norm: 4.598401396165743\n",
      "Epoch 217 - Objective function: 27.343118953252315 - Loss: 27.292170476712926 - Gradient norm: 4.586482552556269\n",
      "Epoch 218 - Objective function: 27.32145253354027 - Loss: 27.270471126676295 - Gradient norm: 4.57520634234282\n",
      "Epoch 219 - Objective function: 27.299924638958853 - Loss: 27.248910956776204 - Gradient norm: 4.564539517269267\n",
      "Epoch 220 - Objective function: 27.278527493157686 - Loss: 27.227482170823645 - Gradient norm: 4.554450375770257\n",
      "Epoch 221 - Objective function: 27.257253755338795 - Loss: 27.206177408713515 - Gradient norm: 4.544908705811857\n",
      "Epoch 222 - Objective function: 27.236096496195575 - Loss: 27.18498972235036 - Gradient norm: 4.535885728368209\n",
      "Epoch 223 - Objective function: 27.215049175171515 - Loss: 27.163912552894132 - Gradient norm: 4.5273540416898985\n",
      "Epoch 224 - Objective function: 27.194105618967214 - Loss: 27.142939709254534 - Gradient norm: 4.519287566501455\n",
      "Epoch 225 - Objective function: 27.173260001228208 - Loss: 27.122065347766394 - Gradient norm: 4.511661492247665\n",
      "Epoch 226 - Objective function: 27.15250682334931 - Loss: 27.1012839529819 - Gradient norm: 4.504452224491813\n",
      "Epoch 227 - Objective function: 27.131840896335333 - Loss: 27.080590319519377 - Gradient norm: 4.497637333553143\n",
      "Epoch 228 - Objective function: 27.111257323660258 - Loss: 27.059979534910866 - Gradient norm: 4.491195504456224\n",
      "Epoch 229 - Objective function: 27.090751485070932 - Loss: 27.039446963394415 - Gradient norm: 4.485106488251237\n",
      "Epoch 230 - Objective function: 27.070319021283616 - Loss: 27.01898823059953 - Gradient norm: 4.479351054751643\n",
      "Epoch 231 - Objective function: 27.04995581952486 - Loss: 26.998599209077177 - Gradient norm: 4.473910946724177\n",
      "Epoch 232 - Objective function: 27.029657999870505 - Loss: 26.978276004628146 - Gradient norm: 4.4687688355556165\n",
      "Epoch 233 - Objective function: 27.009421902339334 - Loss: 26.95801494338635 - Gradient norm: 4.463908278411331\n",
      "Epoch 234 - Objective function: 26.98924407469999 - Loss: 26.937812559615576 - Gradient norm: 4.459313676892074\n",
      "Epoch 235 - Objective function: 26.96912126095229 - Loss: 26.91766558418092 - Gradient norm: 4.454970237187871\n",
      "Epoch 236 - Objective function: 26.949050390445752 - Loss: 26.89757093365764 - Gradient norm: 4.450863931721164\n",
      "Epoch 237 - Objective function: 26.929028567600717 - Loss: 26.87752570004282 - Gradient norm: 4.446981462265424\n",
      "Epoch 238 - Objective function: 26.909053062198712 - Loss: 26.85752714103654 - Gradient norm: 4.443310224520287\n",
      "Epoch 239 - Objective function: 26.889121300211013 - Loss: 26.837572670861434 - Gradient norm: 4.4398382741198406\n",
      "Epoch 240 - Objective function: 26.869230855135665 - Loss: 26.817659851590943 - Gradient norm: 4.436554294046866\n",
      "Epoch 241 - Objective function: 26.849379439815184 - Loss: 26.797786384958417 - Gradient norm: 4.433447563422647\n",
      "Epoch 242 - Objective function: 26.82956489870829 - Loss: 26.777950104620537 - Gradient norm: 4.430507927639296\n",
      "Epoch 243 - Objective function: 26.8097852005908 - Loss: 26.758148968850026 - Gradient norm: 4.4277257697993635\n",
      "Epoch 244 - Objective function: 26.790038431662005 - Loss: 26.738381053634097 - Gradient norm: 4.425091983425845\n",
      "Epoch 245 - Objective function: 26.770322789034136 - Loss: 26.718644546156142 - Gradient norm: 4.422597946404287\n",
      "Epoch 246 - Objective function: 26.750636574583805 - Loss: 26.698937738639632 - Gradient norm: 4.420235496117831\n",
      "Epoch 247 - Objective function: 26.730978189145404 - Loss: 26.679259022534133 - Gradient norm: 4.417996905735328\n",
      "Epoch 248 - Objective function: 26.711346127027614 - Loss: 26.659606883024644 - Gradient norm: 4.415874861612366\n",
      "Epoch 249 - Objective function: 26.69173897083514 - Loss: 26.639979893846302 - Gradient norm: 4.41386244176489\n",
      "Epoch 250 - Objective function: 26.672155386578723 - Loss: 26.620376712387593 - Gradient norm: 4.411953095375252\n",
      "Epoch 251 - Objective function: 26.652594119057607 - Loss: 26.600796075066143 - Gradient norm: 4.410140623290794\n",
      "Epoch 252 - Objective function: 26.633053987499224 - Loss: 26.581236792961928 - Gradient norm: 4.40841915947559\n",
      "Epoch 253 - Objective function: 26.61353388144193 - Loss: 26.561697747693714 - Gradient norm: 4.406783153376482\n",
      "Epoch 254 - Objective function: 26.59403275684732 - Loss: 26.54217788752516 - Gradient norm: 4.40522735316537\n",
      "Epoch 255 - Objective function: 26.574549632429303 - Loss: 26.522676223687956 - Gradient norm: 4.403746789820449\n",
      "Epoch 256 - Objective function: 26.555083586188 - Loss: 26.503191826909845 - Gradient norm: 4.402336762010039\n",
      "Epoch 257 - Objective function: 26.53563375213704 - Loss: 26.483723824136238 - Gradient norm: 4.4009928217435705\n",
      "Epoch 258 - Objective function: 26.516199317213534 - Loss: 26.46427139543464 - Gradient norm: 4.3997107607553465\n",
      "Epoch 259 - Objective function: 26.496779518360466 - Loss: 26.44483377107165 - Gradient norm: 4.398486597587714\n",
      "Epoch 260 - Objective function: 26.477373639772146 - Loss: 26.42541022875316 - Gradient norm: 4.397316565341371\n",
      "Epoch 261 - Objective function: 26.45798101029335 - Loss: 26.40600009101837 - Gradient norm: 4.396197100061636\n",
      "Epoch 262 - Objective function: 26.438601000963832 - Loss: 26.386602722779312 - Gradient norm: 4.395124829730597\n",
      "Epoch 263 - Objective function: 26.419233022700016 - Loss: 26.3672175289977 - Gradient norm: 4.394096563836186\n",
      "Epoch 264 - Objective function: 26.399876524106162 - Loss: 26.347843952491363 - Gradient norm: 4.393109283490287\n",
      "Epoch 265 - Objective function: 26.380530989407916 - Loss: 26.32848147186321 - Gradient norm: 4.392160132069149\n",
      "Epoch 266 - Objective function: 26.361195936501314 - Loss: 26.30912959954574 - Gradient norm: 4.391246406350362\n",
      "Epoch 267 - Objective function: 26.34187091511088 - Loss: 26.28978787995479 - Gradient norm: 4.390365548121819\n",
      "Epoch 268 - Objective function: 26.322555505050598 - Loss: 26.270455887746262 - Gradient norm: 4.389515136239031\n",
      "Epoch 269 - Objective function: 26.303249314582168 - Loss: 26.251133226170264 - Gradient norm: 4.388692879108251\n",
      "Epoch 270 - Objective function: 26.283951978864994 - Loss: 26.231819525517064 - Gradient norm: 4.387896607573824\n",
      "Epoch 271 - Objective function: 26.264663158492862 - Loss: 26.212514441649866 - Gradient norm: 4.387124268189136\n",
      "Epoch 272 - Objective function: 26.24538253811234 - Loss: 26.193217654619406 - Gradient norm: 4.386373916851502\n",
      "Epoch 273 - Objective function: 26.226109825118463 - Loss: 26.173928867355922 - Gradient norm: 4.38564371278221\n",
      "Epoch 274 - Objective function: 26.20684474842329 - Loss: 26.154647804434138 - Gradient norm: 4.384931912833804\n",
      "Epoch 275 - Objective function: 26.18758705729326 - Loss: 26.13537421090708 - Gradient norm: 4.384236866107604\n",
      "Epoch 276 - Objective function: 26.168336520251543 - Loss: 26.116107851205037 - Gradient norm: 4.383557008865168\n",
      "Epoch 277 - Objective function: 26.149092924041636 - Loss: 26.09684850809584 - Gradient norm: 4.382890859718283\n",
      "Epoch 278 - Objective function: 26.12985607264895 - Loss: 26.077595981703208 - Gradient norm: 4.3822370150827625\n",
      "Epoch 279 - Objective function: 26.110625786376882 - Loss: 26.0583500885797 - Gradient norm: 4.3815941448820706\n",
      "Epoch 280 - Objective function: 26.09140190097461 - Loss: 26.03911066083143 - Gradient norm: 4.3809609884875\n",
      "Epoch 281 - Objective function: 26.072184266813398 - Loss: 26.01987754529141 - Gradient norm: 4.380336350882271\n",
      "Epoch 282 - Objective function: 26.052972748108935 - Loss: 26.000650602739025 - Gradient norm: 4.379719099037557\n",
      "Epoch 283 - Objective function: 26.033767222187016 - Loss: 25.981429707162864 - Gradient norm: 4.379108158489077\n",
      "Epoch 284 - Objective function: 26.014567578790093 - Loss: 25.962214745064543 - Gradient norm: 4.378502510103429\n",
      "Epoch 285 - Objective function: 25.995373719422467 - Loss: 25.943005614801265 - Gradient norm: 4.377901187023937\n",
      "Epoch 286 - Objective function: 25.976185556731924 - Loss: 25.92380222596484 - Gradient norm: 4.377303271786282\n",
      "Epoch 287 - Objective function: 25.95700301392574 - Loss: 25.90460449879519 - Gradient norm: 4.376707893594704\n",
      "Epoch 288 - Objective function: 25.937826024219117 - Loss: 25.885412363626337 - Gradient norm: 4.376114225750041\n",
      "Epoch 289 - Objective function: 25.91865453031433 - Loss: 25.866225760363157 - Gradient norm: 4.375521483221333\n",
      "Epoch 290 - Objective function: 25.89948848390864 - Loss: 25.847044637986997 - Gradient norm: 4.374928920353124\n",
      "Epoch 291 - Objective function: 25.88032784522956 - Loss: 25.82786895408868 - Gradient norm: 4.374335828701064\n",
      "Epoch 292 - Objective function: 25.861172582595817 - Loss: 25.808698674427287 - Gradient norm: 4.3737415349887465\n",
      "Epoch 293 - Objective function: 25.842022672002603 - Loss: 25.789533772513273 - Gradient norm: 4.373145399179144\n",
      "Epoch 294 - Objective function: 25.822878096729696 - Loss: 25.770374229214518 - Gradient norm: 4.372546812654304\n",
      "Epoch 295 - Objective function: 25.803738846971196 - Loss: 25.751220032384065 - Gradient norm: 4.371945196497354\n",
      "Epoch 296 - Objective function: 25.7846049194857 - Loss: 25.732071176508327 - Gradient norm: 4.3713399998711795\n",
      "Epoch 297 - Objective function: 25.765476317265623 - Loss: 25.712927662374508 - Gradient norm: 4.370730698488384\n",
      "Epoch 298 - Objective function: 25.746353049224748 - Loss: 25.6937894967563 - Gradient norm: 4.3701167931675196\n",
      "Epoch 299 - Objective function: 25.72723512990278 - Loss: 25.674656692116656 - Gradient norm: 4.3694978084707765\n",
      "Epoch 300 - Objective function: 25.708122579186206 - Loss: 25.65552926632688 - Gradient norm: 4.3688732914186055\n",
      "Epoch 301 - Objective function: 25.689015422044214 - Loss: 25.63640724240087 - Gradient norm: 4.368242810277015\n",
      "Epoch 302 - Objective function: 25.66991368827913 - Loss: 25.61729064824388 - Gradient norm: 4.3676059534134675\n",
      "Epoch 303 - Objective function: 25.65081741229028 - Loss: 25.598179516414795 - Gradient norm: 4.366962328217573\n",
      "Epoch 304 - Objective function: 25.63172663285074 - Loss: 25.579073883901298 - Gradient norm: 4.366311560082942\n",
      "Epoch 305 - Objective function: 25.61264139289605 - Loss: 25.559973791907108 - Gradient norm: 4.3656532914468\n",
      "Epoch 306 - Objective function: 25.5935617393244 - Loss: 25.540879285650625 - Gradient norm: 4.364987180884103\n",
      "Epoch 307 - Objective function: 25.57448772280743 - Loss: 25.52179041417437 - Gradient norm: 4.364312902253137\n",
      "Epoch 308 - Objective function: 25.555419397611224 - Loss: 25.502707230164525 - Gradient norm: 4.363630143889676\n",
      "Epoch 309 - Objective function: 25.536356821426864 - Loss: 25.483629789780153 - Gradient norm: 4.362938607847\n",
      "Epoch 310 - Objective function: 25.517300055209844 - Loss: 25.464558152491318 - Gradient norm: 4.362238009179183\n",
      "Epoch 311 - Objective function: 25.49824916302814 - Loss: 25.445492380925828 - Gradient norm: 4.361528075265214\n",
      "Epoch 312 - Objective function: 25.479204211918173 - Loss: 25.42643254072399 - Gradient norm: 4.360808545171659\n",
      "Epoch 313 - Objective function: 25.46016527174831 - Loss: 25.407378700400887 - Gradient norm: 4.360079169051687\n",
      "Epoch 314 - Objective function: 25.44113241508957 - Loss: 25.38833093121591 - Gradient norm: 4.359339707578405\n",
      "Epoch 315 - Objective function: 25.422105717092926 - Loss: 25.369289307048913 - Gradient norm: 4.358589931410568\n",
      "Epoch 316 - Objective function: 25.403085255373007 - Loss: 25.350253904282813 - Gradient norm: 4.357829620688835\n",
      "Epoch 317 - Objective function: 25.384071109897725 - Loss: 25.331224801692112 - Gradient norm: 4.357058564560825\n",
      "Epoch 318 - Objective function: 25.365063362883564 - Loss: 25.31220208033718 - Gradient norm: 4.356276560733372\n",
      "Epoch 319 - Objective function: 25.3460620986961 - Loss: 25.293185823463705 - Gradient norm: 4.3554834150504025\n",
      "Epoch 320 - Objective function: 25.32706740375559 - Loss: 25.27417611640734 - Gradient norm: 4.354678941095004\n",
      "Epoch 321 - Objective function: 25.30807936644722 - Loss: 25.255173046502936 - Gradient norm: 4.353862959814297\n",
      "Epoch 322 - Objective function: 25.28909807703585 - Loss: 25.236176702998385 - Gradient norm: 4.353035299165823\n",
      "Epoch 323 - Objective function: 25.270123627584855 - Loss: 25.217187176972548 - Gradient norm: 4.352195793784211\n",
      "Epoch 324 - Objective function: 25.251156111879038 - Loss: 25.198204561257263 - Gradient norm: 4.351344284666973\n",
      "Epoch 325 - Objective function: 25.232195625351128 - Loss: 25.179228950363 - Gradient norm: 4.350480618878341\n",
      "Epoch 326 - Objective function: 25.213242265011896 - Loss: 25.160260440408134 - Gradient norm: 4.349604649270103\n",
      "Epoch 327 - Objective function: 25.194296129383513 - Loss: 25.141299129051493 - Gradient norm: 4.348716234218476\n",
      "Epoch 328 - Objective function: 25.175357318436046 - Loss: 25.12234511542806 - Gradient norm: 4.347815237376089\n",
      "Epoch 329 - Objective function: 25.156425933526886 - Loss: 25.10339850008765 - Gradient norm: 4.34690152743822\n",
      "Epoch 330 - Objective function: 25.137502077342955 - Loss: 25.084459384936334 - Gradient norm: 4.34597497792245\n",
      "Epoch 331 - Objective function: 25.118585853845527 - Loss: 25.065527873180574 - Gradient norm: 4.345035466960994\n",
      "Epoch 332 - Objective function: 25.099677368217456 - Loss: 25.046604069273723 - Gradient norm: 4.344082877104939\n",
      "Epoch 333 - Objective function: 25.08077672681281 - Loss: 25.027688078864987 - Gradient norm: 4.343117095139743\n",
      "Epoch 334 - Objective function: 25.061884037108616 - Loss: 25.008780008750488 - Gradient norm: 4.34213801191131\n",
      "Epoch 335 - Objective function: 25.042999407658698 - Loss: 24.98987996682649 - Gradient norm: 4.34114552216206\n",
      "Epoch 336 - Objective function: 25.024122948049495 - Loss: 24.97098806204452 - Gradient norm: 4.340139524376389\n",
      "Epoch 337 - Objective function: 25.005254768857597 - Loss: 24.952104404368374 - Gradient norm: 4.339119920634996\n",
      "Epoch 338 - Objective function: 24.98639498160913 - Loss: 24.933229104732852 - Gradient norm: 4.338086616477552\n",
      "Epoch 339 - Objective function: 24.967543698740716 - Loss: 24.914362275004166 - Gradient norm: 4.3370395207732235\n",
      "Epoch 340 - Objective function: 24.94870103356194 - Loss: 24.89550402794184 - Gradient norm: 4.335978545598603\n",
      "Epoch 341 - Objective function: 24.92986710021931 - Loss: 24.87665447716211 - Gradient norm: 4.334903606122603\n",
      "Epoch 342 - Objective function: 24.911042013661586 - Loss: 24.857813737102735 - Gradient norm: 4.33381462049792\n",
      "Epoch 343 - Objective function: 24.89222588960631 - Loss: 24.83898192298904 - Gradient norm: 4.332711509758664\n",
      "Epoch 344 - Objective function: 24.87341884450767 - Loss: 24.820159150801217 - Gradient norm: 4.3315941977238195\n",
      "Epoch 345 - Objective function: 24.854620995525405 - Loss: 24.801345537242803 - Gradient norm: 4.330462610906166\n",
      "Epoch 346 - Objective function: 24.83583246049489 - Loss: 24.78254119971024 - Gradient norm: 4.329316678426363\n",
      "Epoch 347 - Objective function: 24.81705335789808 - Loss: 24.763746256263424 - Gradient norm: 4.3281563319318686\n",
      "Epoch 348 - Objective function: 24.798283806835638 - Loss: 24.74496082559731 - Gradient norm: 4.326981505520434\n",
      "Epoch 349 - Objective function: 24.779523926999747 - Loss: 24.72618502701435 - Gradient norm: 4.325792135667881\n",
      "Epoch 350 - Objective function: 24.76077383864793 - Loss: 24.707418980397854 - Gradient norm: 4.32458816115991\n",
      "Epoch 351 - Objective function: 24.742033662577608 - Loss: 24.688662806186166 - Gradient norm: 4.323369523027717\n",
      "Epoch 352 - Objective function: 24.72330352010141 - Loss: 24.669916625347554 - Gradient norm: 4.32213616448717\n",
      "Epoch 353 - Objective function: 24.704583533023293 - Loss: 24.65118055935594 - Gradient norm: 4.320888030881345\n",
      "Epoch 354 - Objective function: 24.685873823615207 - Loss: 24.632454730167215 - Gradient norm: 4.319625069626206\n",
      "Epoch 355 - Objective function: 24.66717451459453 - Loss: 24.613739260196272 - Gradient norm: 4.318347230159276\n",
      "Epoch 356 - Objective function: 24.64848572910201 - Loss: 24.595034272294615 - Gradient norm: 4.31705446389105\n",
      "Epoch 357 - Objective function: 24.62980759068034 - Loss: 24.576339889728594 - Gradient norm: 4.315746724159073\n",
      "Epoch 358 - Objective function: 24.611140223253283 - Loss: 24.55765623615816 - Gradient norm: 4.3144239661844335\n",
      "Epoch 359 - Objective function: 24.592483751105252 - Loss: 24.538983435616153 - Gradient norm: 4.313086147030596\n",
      "Epoch 360 - Objective function: 24.573838298861457 - Loss: 24.52032161248811 - Gradient norm: 4.3117332255643825\n",
      "Epoch 361 - Objective function: 24.55520399146841 - Loss: 24.50167089149248 - Gradient norm: 4.310365162418988\n",
      "Epoch 362 - Objective function: 24.536580954174976 - Loss: 24.483031397661346 - Gradient norm: 4.308981919958916\n",
      "Epoch 363 - Objective function: 24.5179693125138 - Loss: 24.46440325632157 - Gradient norm: 4.3075834622466935\n",
      "Epoch 364 - Objective function: 24.499369192283027 - Loss: 24.44578659307627 - Gradient norm: 4.306169755011265\n",
      "Epoch 365 - Objective function: 24.4807807195286 - Loss: 24.42718153378676 - Gradient norm: 4.3047407656179715\n",
      "Epoch 366 - Objective function: 24.462204020526684 - Loss: 24.408588204554793 - Gradient norm: 4.30329646303999\n",
      "Epoch 367 - Objective function: 24.443639221766635 - Loss: 24.390006731705192 - Gradient norm: 4.301836817831168\n",
      "Epoch 368 - Objective function: 24.425086449934113 - Loss: 24.371437241768767 - Gradient norm: 4.300361802100144\n",
      "Epoch 369 - Objective function: 24.40654583189462 - Loss: 24.352879861465574 - Gradient norm: 4.298871389485689\n",
      "Epoch 370 - Objective function: 24.38801749467726 - Loss: 24.334334717688446 - Gradient norm: 4.297365555133175\n",
      "Epoch 371 - Objective function: 24.369501565458833 - Loss: 24.315801937486842 - Gradient norm: 4.295844275672112\n",
      "Epoch 372 - Objective function: 24.350998171548127 - Loss: 24.297281648050937 - Gradient norm: 4.294307529194672\n",
      "Epoch 373 - Objective function: 24.332507440370502 - Loss: 24.27877397669596 - Gradient norm: 4.292755295235156\n",
      "Epoch 374 - Objective function: 24.31402949945274 - Loss: 24.260279050846837 - Gradient norm: 4.291187554750303\n",
      "Epoch 375 - Objective function: 24.295564476408053 - Loss: 24.241796998022995 - Gradient norm: 4.289604290100441\n",
      "Epoch 376 - Objective function: 24.277112498921383 - Loss: 24.223327945823453 - Gradient norm: 4.288005485031368\n",
      "Epoch 377 - Objective function: 24.25867369473487 - Loss: 24.204872021912095 - Gradient norm: 4.286391124656958\n",
      "Epoch 378 - Objective function: 24.240248191633537 - Loss: 24.186429354003163 - Gradient norm: 4.284761195442411\n",
      "Epoch 379 - Objective function: 24.221836117431206 - Loss: 24.16800006984698 - Gradient norm: 4.2831156851881325\n",
      "Epoch 380 - Objective function: 24.203437599956516 - Loss: 24.1495842972158 - Gradient norm: 4.281454583014171\n",
      "Epoch 381 - Objective function: 24.185052767039217 - Loss: 24.13118216388991 - Gradient norm: 4.279777879345198\n",
      "Epoch 382 - Objective function: 24.16668174649659 - Loss: 24.11279379764389 - Gradient norm: 4.278085565895972\n",
      "Epoch 383 - Objective function: 24.148324666120036 - Loss: 24.09441932623301 - Gradient norm: 4.276377635657279\n",
      "Epoch 384 - Objective function: 24.12998165366186 - Loss: 24.076058877379893 - Gradient norm: 4.274654082882286\n",
      "Epoch 385 - Objective function: 24.111652836822167 - Loss: 24.057712578761215 - Gradient norm: 4.272914903073297\n",
      "Epoch 386 - Objective function: 24.093338343235963 - Loss: 24.039380557994672 - Gradient norm: 4.271160092968882\n",
      "Epoch 387 - Objective function: 24.075038300460374 - Loss: 24.02106294262603 - Gradient norm: 4.269389650531339\n",
      "Epoch 388 - Objective function: 24.056752835962005 - Loss: 24.002759860116367 - Gradient norm: 4.267603574934476\n",
      "Epoch 389 - Objective function: 24.038482077104483 - Loss: 23.984471437829445 - Gradient norm: 4.265801866551685\n",
      "Epoch 390 - Objective function: 24.020226151136086 - Loss: 23.966197803019213 - Gradient norm: 4.263984526944287\n",
      "Epoch 391 - Objective function: 24.001985185177563 - Loss: 23.94793908281749 - Gradient norm: 4.26215155885012\n",
      "Epoch 392 - Objective function: 23.983759306210025 - Loss: 23.929695404221743 - Gradient norm: 4.260302966172366\n",
      "Epoch 393 - Objective function: 23.96554864106303 - Loss: 23.91146689408299 - Gradient norm: 4.258438753968583\n",
      "Epoch 394 - Objective function: 23.947353316402726 - Loss: 23.893253679093903 - Gradient norm: 4.256558928439923\n",
      "Epoch 395 - Objective function: 23.929173458720207 - Loss: 23.87505588577696 - Gradient norm: 4.254663496920553\n",
      "Epoch 396 - Objective function: 23.911009194319906 - Loss: 23.856873640472763 - Gradient norm: 4.252752467867208\n",
      "Epoch 397 - Objective function: 23.89286064930816 - Loss: 23.838707069328496 - Gradient norm: 4.250825850848917\n",
      "Epoch 398 - Objective function: 23.874727949581903 - Loss: 23.820556298286473 - Gradient norm: 4.248883656536853\n",
      "Epoch 399 - Objective function: 23.856611220817385 - Loss: 23.80242145307278 - Gradient norm: 4.246925896694308\n",
      "Epoch 400 - Objective function: 23.838510588459172 - Loss: 23.784302659186146 - Gradient norm: 4.24495258416679\n",
      "Epoch 401 - Objective function: 23.82042617770912 - Loss: 23.766200041886822 - Gradient norm: 4.242963732872208\n",
      "Epoch 402 - Objective function: 23.802358113515503 - Loss: 23.74811372618562 - Gradient norm: 4.240959357791158\n",
      "Epoch 403 - Objective function: 23.784306520562286 - Loss: 23.73004383683306 - Gradient norm: 4.238939474957295\n",
      "Epoch 404 - Objective function: 23.766271523258464 - Loss: 23.711990498308655 - Gradient norm: 4.2369041014477675\n",
      "Epoch 405 - Objective function: 23.748253245727575 - Loss: 23.69395383481028 - Gradient norm: 4.2348532553737375\n",
      "Epoch 406 - Objective function: 23.73025181179725 - Loss: 23.675933970243666 - Gradient norm: 4.232786955870938\n",
      "Epoch 407 - Objective function: 23.712267344988895 - Loss: 23.657931028212 - Gradient norm: 4.230705223090303\n",
      "Epoch 408 - Objective function: 23.694299968507543 - Loss: 23.639945132005657 - Gradient norm: 4.228608078188635\n",
      "Epoch 409 - Objective function: 23.67634980523173 - Loss: 23.621976404592015 - Gradient norm: 4.226495543319308\n",
      "Epoch 410 - Objective function: 23.658416977703535 - Loss: 23.60402496860542 - Gradient norm: 4.224367641623026\n",
      "Epoch 411 - Objective function: 23.640501608118704 - Loss: 23.586090946337197 - Gradient norm: 4.222224397218586\n",
      "Epoch 412 - Objective function: 23.622603818316875 - Loss: 23.56817445972584 - Gradient norm: 4.220065835193688\n",
      "Epoch 413 - Objective function: 23.604723729771955 - Loss: 23.55027563034728 - Gradient norm: 4.21789198159576\n",
      "Epoch 414 - Objective function: 23.58686146358253 - Loss: 23.53239457940524 - Gradient norm: 4.2157028634227975\n",
      "Epoch 415 - Objective function: 23.569017140462453 - Loss: 23.514531427721735 - Gradient norm: 4.2134985086142285\n",
      "Epoch 416 - Objective function: 23.55119088073153 - Loss: 23.496686295727702 - Gradient norm: 4.211278946041777\n",
      "Epoch 417 - Objective function: 23.533382804306235 - Loss: 23.478859303453643 - Gradient norm: 4.209044205500352\n",
      "Epoch 418 - Objective function: 23.515593030690646 - Loss: 23.46105057052048 - Gradient norm: 4.206794317698928\n",
      "Epoch 419 - Objective function: 23.49782167896741 - Loss: 23.443260216130472 - Gradient norm: 4.20452931425144\n",
      "Epoch 420 - Objective function: 23.480068867788848 - Loss: 23.42548835905825 - Gradient norm: 4.202249227667678\n",
      "Epoch 421 - Objective function: 23.46233471536811 - Loss: 23.407735117641913 - Gradient norm: 4.199954091344182\n",
      "Epoch 422 - Objective function: 23.444619339470588 - Loss: 23.390000609774358 - Gradient norm: 4.197643939555139\n",
      "Epoch 423 - Objective function: 23.426922857405202 - Loss: 23.372284952894542 - Gradient norm: 4.195318807443269\n",
      "Epoch 424 - Objective function: 23.409245386016014 - Loss: 23.35458826397902 - Gradient norm: 4.192978731010724\n",
      "Epoch 425 - Objective function: 23.391587041673823 - Loss: 23.336910659533476 - Gradient norm: 4.190623747109968\n",
      "Epoch 426 - Objective function: 23.37394794026792 - Loss: 23.319252255584427 - Gradient norm: 4.188253893434654\n",
      "Epoch 427 - Objective function: 23.356328197197897 - Loss: 23.301613167670983 - Gradient norm: 4.185869208510508\n",
      "Epoch 428 - Objective function: 23.338727927365618 - Loss: 23.283993510836773 - Gradient norm: 4.183469731686187\n",
      "Epoch 429 - Objective function: 23.32114724516731 - Loss: 23.266393399621965 - Gradient norm: 4.181055503124147\n",
      "Epoch 430 - Objective function: 23.303586264485663 - Loss: 23.248812948055317 - Gradient norm: 4.1786265637915\n",
      "Epoch 431 - Objective function: 23.28604509868216 - Loss: 23.23125226964648 - Gradient norm: 4.1761829554508525\n",
      "Epoch 432 - Objective function: 23.268523860589426 - Loss: 23.213711477378272 - Gradient norm: 4.173724720651156\n",
      "Epoch 433 - Objective function: 23.25102266250373 - Loss: 23.196190683699147 - Gradient norm: 4.171251902718532\n",
      "Epoch 434 - Objective function: 23.233541616177565 - Loss: 23.17869000051574 - Gradient norm: 4.168764545747108\n",
      "Epoch 435 - Objective function: 23.216080832812402 - Loss: 23.16120953918553 - Gradient norm: 4.166262694589834\n",
      "Epoch 436 - Objective function: 23.198640423051433 - Loss: 23.143749410509606 - Gradient norm: 4.163746394849288\n",
      "Epoch 437 - Objective function: 23.181220496972553 - Loss: 23.126309724725548 - Gradient norm: 4.1612156928685\n",
      "Epoch 438 - Objective function: 23.16382116408134 - Loss: 23.10889059150039 - Gradient norm: 4.158670635721742\n",
      "Epoch 439 - Objective function: 23.146442533304263 - Loss: 23.09149211992378 - Gradient norm: 4.156111271205331\n",
      "Epoch 440 - Objective function: 23.129084712981822 - Loss: 23.0741144185011 - Gradient norm: 4.153537647828424\n",
      "Epoch 441 - Objective function: 23.111747810862006 - Loss: 23.056757595146852 - Gradient norm: 4.150949814803805\n",
      "Epoch 442 - Objective function: 23.094431934093702 - Loss: 23.039421757178054 - Gradient norm: 4.148347822038669\n",
      "Epoch 443 - Objective function: 23.07713718922029 - Loss: 23.022107011307767 - Gradient norm: 4.145731720125422\n",
      "Epoch 444 - Objective function: 23.059863682173297 - Loss: 23.004813463638747 - Gradient norm: 4.143101560332446\n",
      "Epoch 445 - Objective function: 23.042611518266217 - Loss: 22.987541219657206 - Gradient norm: 4.140457394594894\n",
      "Epoch 446 - Objective function: 23.025380802188376 - Loss: 22.97029038422666 - Gradient norm: 4.137799275505471\n",
      "Epoch 447 - Objective function: 23.008171637998974 - Loss: 22.953061061581906 - Gradient norm: 4.135127256305217\n",
      "Epoch 448 - Objective function: 22.990984129121184 - Loss: 22.935853355323104 - Gradient norm: 4.132441390874304\n",
      "Epoch 449 - Objective function: 22.973818378336354 - Loss: 22.918667368409963 - Gradient norm: 4.12974173372281\n",
      "Epoch 450 - Objective function: 22.956674487778375 - Loss: 22.901503203156043 - Gradient norm: 4.12702833998153\n",
      "Epoch 451 - Objective function: 22.939552558928078 - Loss: 22.884360961223138 - Gradient norm: 4.12430126539277\n",
      "Epoch 452 - Objective function: 22.92245269260786 - Loss: 22.867240743615863 - Gradient norm: 4.121560566301152\n",
      "Epoch 453 - Objective function: 22.90537498897623 - Loss: 22.850142650676187 - Gradient norm: 4.118806299644432\n",
      "Epoch 454 - Objective function: 22.888319547522656 - Loss: 22.83306678207822 - Gradient norm: 4.116038522944318\n",
      "Epoch 455 - Objective function: 22.87128646706241 - Loss: 22.816013236823053 - Gradient norm: 4.113257294297305\n",
      "Epoch 456 - Objective function: 22.854275845731582 - Loss: 22.798982113233716 - Gradient norm: 4.110462672365519\n",
      "Epoch 457 - Objective function: 22.837287780982063 - Loss: 22.781973508950177 - Gradient norm: 4.107654716367565\n",
      "Epoch 458 - Objective function: 22.820322369576886 - Loss: 22.764987520924606 - Gradient norm: 4.104833486069406\n",
      "Epoch 459 - Objective function: 22.803379707585403 - Loss: 22.74802424541654 - Gradient norm: 4.101999041775227\n",
      "Epoch 460 - Objective function: 22.786459890378783 - Loss: 22.731083777988378 - Gradient norm: 4.099151444318345\n",
      "Epoch 461 - Objective function: 22.76956301262547 - Loss: 22.71416621350077 - Gradient norm: 4.096290755052114\n",
      "Epoch 462 - Objective function: 22.752689168286814 - Loss: 22.697271646108266 - Gradient norm: 4.093417035840858\n",
      "Epoch 463 - Objective function: 22.73583845061283 - Loss: 22.680400169255016 - Gradient norm: 4.090530349050815\n",
      "Epoch 464 - Objective function: 22.719010952138028 - Loss: 22.663551875670567 - Gradient norm: 4.087630757541106\n",
      "Epoch 465 - Objective function: 22.702206764677296 - Loss: 22.646726857365778 - Gradient norm: 4.084718324654723\n",
      "Epoch 466 - Objective function: 22.685425979322016 - Loss: 22.629925205628837 - Gradient norm: 4.081793114209535\n",
      "Epoch 467 - Objective function: 22.668668686436195 - Loss: 22.61314701102143 - Gradient norm: 4.078855190489325\n",
      "Epoch 468 - Objective function: 22.651934975652686 - Loss: 22.59639236337489 - Gradient norm: 4.075904618234838\n",
      "Epoch 469 - Objective function: 22.635224935869587 - Loss: 22.57966135178661 - Gradient norm: 4.072941462634871\n",
      "Epoch 470 - Objective function: 22.61853865524669 - Loss: 22.562954064616445 - Gradient norm: 4.069965789317373\n",
      "Epoch 471 - Objective function: 22.60187622120207 - Loss: 22.546270589483278 - Gradient norm: 4.066977664340578\n",
      "Epoch 472 - Objective function: 22.58523772040869 - Loss: 22.52961101326164 - Gradient norm: 4.063977154184171\n",
      "Epoch 473 - Objective function: 22.56862323879125 - Loss: 22.51297542207848 - Gradient norm: 4.0609643257404775\n",
      "Epoch 474 - Objective function: 22.552032861523042 - Loss: 22.496363901310037 - Gradient norm: 4.057939246305683\n",
      "Epoch 475 - Objective function: 22.53546667302289 - Loss: 22.479776535578747 - Gradient norm: 4.054901983571084\n",
      "Epoch 476 - Objective function: 22.518924756952288 - Loss: 22.46321340875036 - Gradient norm: 4.051852605614377\n",
      "Epoch 477 - Objective function: 22.502407196212513 - Loss: 22.446674603931044 - Gradient norm: 4.04879118089097\n",
      "Epoch 478 - Objective function: 22.48591407294198 - Loss: 22.430160203464695 - Gradient norm: 4.045717778225343\n",
      "Epoch 479 - Objective function: 22.46944546851357 - Loss: 22.413670288930266 - Gradient norm: 4.042632466802422\n",
      "Epoch 480 - Objective function: 22.453001463532125 - Loss: 22.397204941139233 - Gradient norm: 4.039535316159016\n",
      "Epoch 481 - Objective function: 22.436582137832033 - Loss: 22.380764240133175 - Gradient norm: 4.036426396175268\n",
      "Epoch 482 - Objective function: 22.420187570474887 - Loss: 22.36434826518139 - Gradient norm: 4.033305777066156\n",
      "Epoch 483 - Objective function: 22.403817839747322 - Loss: 22.347957094778724 - Gradient norm: 4.030173529373028\n",
      "Epoch 484 - Objective function: 22.387473023158776 - Loss: 22.331590806643334 - Gradient norm: 4.0270297239551835\n",
      "Epoch 485 - Objective function: 22.37115319743959 - Loss: 22.315249477714726 - Gradient norm: 4.023874431981486\n",
      "Epoch 486 - Objective function: 22.35485843853897 - Loss: 22.298933184151736 - Gradient norm: 4.020707724922027\n",
      "Epoch 487 - Objective function: 22.3385888216232 - Loss: 22.282642001330725 - Gradient norm: 4.01752967453982\n",
      "Epoch 488 - Objective function: 22.32234442107393 - Loss: 22.266376003843806 - Gradient norm: 4.014340352882553\n",
      "Epoch 489 - Objective function: 22.306125310486422 - Loss: 22.25013526549714 - Gradient norm: 4.011139832274373\n",
      "Epoch 490 - Objective function: 22.289931562668116 - Loss: 22.233919859309424 - Gradient norm: 4.007928185307722\n",
      "Epoch 491 - Objective function: 22.273763249637092 - Loss: 22.217729857510374 - Gradient norm: 4.004705484835213\n",
      "Epoch 492 - Objective function: 22.257620442620727 - Loss: 22.201565331539353 - Gradient norm: 4.001471803961559\n",
      "Epoch 493 - Objective function: 22.24150321205442 - Loss: 22.185426352044068 - Gradient norm: 3.9982272160355437\n",
      "Epoch 494 - Objective function: 22.225411627580396 - Loss: 22.169312988879387 - Gradient norm: 3.9949717946420438\n",
      "Epoch 495 - Objective function: 22.209345758046634 - Loss: 22.153225311106215 - Gradient norm: 3.9917056135940965\n",
      "Epoch 496 - Objective function: 22.193305671505833 - Loss: 22.137163386990476 - Gradient norm: 3.9884287469250213\n",
      "Epoch 497 - Objective function: 22.177291435214496 - Loss: 22.121127284002174 - Gradient norm: 3.9851412688805845\n",
      "Epoch 498 - Objective function: 22.16130311563214 - Loss: 22.105117068814565 - Gradient norm: 3.9818432539112227\n",
      "Epoch 499 - Objective function: 22.145340778420508 - Loss: 22.08913280730338 - Gradient norm: 3.9785347766643095\n",
      "Epoch 500 - Objective function: 22.12940448844296 - Loss: 22.073174564546196 - Gradient norm: 3.975215911976483\n",
      "Epoch 501 - Objective function: 22.11349430976387 - Loss: 22.057242404821814 - Gradient norm: 3.971886734866016\n",
      "Epoch 502 - Objective function: 22.097610305648182 - Loss: 22.041336391609786 - Gradient norm: 3.968547320525247\n",
      "Epoch 503 - Objective function: 22.08175253856095 - Loss: 22.025456587589986 - Gradient norm: 3.965197744313062\n",
      "Epoch 504 - Objective function: 22.06592107016712 - Loss: 22.00960305464232 - Gradient norm: 3.961838081747431\n",
      "Epoch 505 - Objective function: 22.050115961331215 - Loss: 21.993775853846437 - Gradient norm: 3.958468408497997\n",
      "Epoch 506 - Objective function: 22.03433727211723 - Loss: 21.9779750454816 - Gradient norm: 3.9550888003787272\n",
      "Epoch 507 - Objective function: 22.018585061788578 - Loss: 21.962200689026606 - Gradient norm: 3.9516993333406103\n",
      "Epoch 508 - Objective function: 22.00285938880805 - Loss: 21.94645284315976 - Gradient norm: 3.9483000834644186\n",
      "Epoch 509 - Objective function: 21.987160310838004 - Loss: 21.930731565759018 - Gradient norm: 3.944891126953522\n",
      "Epoch 510 - Objective function: 21.97148788474042 - Loss: 21.915036913902057 - Gradient norm: 3.9414725401267616\n",
      "Epoch 511 - Objective function: 21.955842166577288 - Loss: 21.899368943866634 - Gradient norm: 3.9380443994113805\n",
      "Epoch 512 - Objective function: 21.940223211610828 - Loss: 21.88372771113079 - Gradient norm: 3.9346067813360115\n",
      "Epoch 513 - Objective function: 21.92463107430396 - Loss: 21.86811327037332 - Gradient norm: 3.931159762523726\n",
      "Epoch 514 - Objective function: 21.909065808320772 - Loss: 21.852525675474233 - Gradient norm: 3.927703419685139\n",
      "Epoch 515 - Objective function: 21.893527466527082 - Loss: 21.83696497951528 - Gradient norm: 3.924237829611575\n",
      "Epoch 516 - Objective function: 21.87801610099108 - Loss: 21.82143123478059 - Gradient norm: 3.920763069168294\n",
      "Epoch 517 - Objective function: 21.86253176298401 - Loss: 21.805924492757363 - Gradient norm: 3.917279215287777\n",
      "Epoch 518 - Objective function: 21.84707450298099 - Loss: 21.790444804136655 - Gradient norm: 3.9137863449630697\n",
      "Epoch 519 - Objective function: 21.831644370661838 - Loss: 21.774992218814187 - Gradient norm: 3.91028453524119\n",
      "Epoch 520 - Objective function: 21.816241414912007 - Loss: 21.7595667858913 - Gradient norm: 3.906773863216597\n",
      "Epoch 521 - Objective function: 21.80086568382358 - Loss: 21.744168553675898 - Gradient norm: 3.903254406024718\n",
      "Epoch 522 - Objective function: 21.78551722469633 - Loss: 21.72879756968353 - Gradient norm: 3.899726240835536\n",
      "Epoch 523 - Objective function: 21.77019608403885 - Loss: 21.713453880638497 - Gradient norm: 3.8961894448472467\n",
      "Epoch 524 - Objective function: 21.75490230756979 - Loss: 21.698137532475055 - Gradient norm: 3.8926440952799672\n",
      "Epoch 525 - Objective function: 21.739635940219035 - Loss: 21.68284857033865 - Gradient norm: 3.889090269369517\n",
      "Epoch 526 - Objective function: 21.72439702612916 - Loss: 21.667587038587264 - Gradient norm: 3.8855280443612497\n",
      "Epoch 527 - Objective function: 21.70918560865674 - Loss: 21.652352980792795 - Gradient norm: 3.88195749750396\n",
      "Epoch 528 - Objective function: 21.69400173037383 - Loss: 21.637146439742505 - Gradient norm: 3.878378706043845\n",
      "Epoch 529 - Objective function: 21.67884543306953 - Loss: 21.62196745744054 - Gradient norm: 3.874791747218529\n",
      "Epoch 530 - Objective function: 21.663716757751533 - Loss: 21.606816075109524 - Gradient norm: 3.871196698251155\n",
      "Epoch 531 - Objective function: 21.648615744647785 - Loss: 21.591692333192164 - Gradient norm: 3.8675936363445356\n",
      "Epoch 532 - Objective function: 21.633542433208216 - Loss: 21.576596271352997 - Gradient norm: 3.863982638675373\n",
      "Epoch 533 - Objective function: 21.618496862106497 - Loss: 21.561527928480118 - Gradient norm: 3.860363782388533\n",
      "Epoch 534 - Objective function: 21.60347906924187 - Loss: 21.54648734268702 - Gradient norm: 3.856737144591389\n",
      "Epoch 535 - Objective function: 21.588489091741046 - Loss: 21.53147455131446 - Gradient norm: 3.853102802348233\n",
      "Epoch 536 - Objective function: 21.57352696596015 - Loss: 21.51648959093242 - Gradient norm: 3.8494608326747377\n",
      "Epoch 537 - Objective function: 21.558592727486715 - Loss: 21.50153249734208 - Gradient norm: 3.8458113125324984\n",
      "Epoch 538 - Objective function: 21.543686411141778 - Loss: 21.486603305577887 - Gradient norm: 3.8421543188236225\n",
      "Epoch 539 - Objective function: 21.52880805098194 - Loss: 21.471702049909627 - Gradient norm: 3.8384899283853966\n",
      "Epoch 540 - Objective function: 21.513957680301562 - Loss: 21.456828763844626 - Gradient norm: 3.8348182179850054\n",
      "Epoch 541 - Objective function: 21.49913533163502 - Loss: 21.441983480129952 - Gradient norm: 3.831139264314324\n",
      "Epoch 542 - Objective function: 21.48434103675891 - Loss: 21.42716623075465 - Gradient norm: 3.82745314398477\n",
      "Epoch 543 - Objective function: 21.469574826694426 - Loss: 21.4123770469521 - Gradient norm: 3.8237599335222163\n",
      "Epoch 544 - Objective function: 21.454836731709737 - Loss: 21.397615959202366 - Gradient norm: 3.8200597093619724\n",
      "Epoch 545 - Objective function: 21.44012678132237 - Loss: 21.38288299723461 - Gradient norm: 3.816352547843827\n",
      "Epoch 546 - Objective function: 21.42544500430172 - Loss: 21.36817819002956 - Gradient norm: 3.812638525207151\n",
      "Epoch 547 - Objective function: 21.410791428671587 - Loss: 21.353501565822043 - Gradient norm: 3.8089177175860733\n",
      "Epoch 548 - Objective function: 21.3961660817127 - Loss: 21.33885315210352 - Gradient norm: 3.8051902010047094\n",
      "Epoch 549 - Objective function: 21.381568989965373 - Loss: 21.324232975624717 - Gradient norm: 3.8014560513724565\n",
      "Epoch 550 - Objective function: 21.367000179232146 - Loss: 21.30964106239826 - Gradient norm: 3.7977153444793585\n",
      "Epoch 551 - Objective function: 21.352459674580505 - Loss: 21.295077437701394 - Gradient norm: 3.7939681559915206\n",
      "Epoch 552 - Objective function: 21.337947500345624 - Loss: 21.28054212607871 - Gradient norm: 3.790214561446603\n",
      "Epoch 553 - Objective function: 21.32346368013316 - Loss: 21.266035151344944 - Gradient norm: 3.7864546362493616\n",
      "Epoch 554 - Objective function: 21.309008236822095 - Loss: 21.25155653658779 - Gradient norm: 3.782688455667261\n",
      "Epoch 555 - Objective function: 21.294581192567613 - Loss: 21.237106304170787 - Gradient norm: 3.778916094826151\n",
      "Epoch 556 - Objective function: 21.28018256880401 - Loss: 21.22268447573622 - Gradient norm: 3.775137628705996\n",
      "Epoch 557 - Objective function: 21.265812386247656 - Loss: 21.208291072208073 - Gradient norm: 3.7713531321366767\n",
      "Epoch 558 - Objective function: 21.251470664899994 - Loss: 21.193926113795015 - Gradient norm: 3.7675626797938464\n",
      "Epoch 559 - Objective function: 21.23715742405058 - Loss: 21.179589619993433 - Gradient norm: 3.7637663461948545\n",
      "Epoch 560 - Objective function: 21.222872682280126 - Loss: 21.165281609590497 - Gradient norm: 3.759964205694728\n",
      "Epoch 561 - Objective function: 21.208616457463652 - Loss: 21.15100210066725 - Gradient norm: 3.756156332482213\n",
      "Epoch 562 - Objective function: 21.194388766773613 - Loss: 21.13675111060177 - Gradient norm: 3.752342800575884\n",
      "Epoch 563 - Objective function: 21.180189626683063 - Loss: 21.122528656072326 - Gradient norm: 3.7485236838203058\n",
      "Epoch 564 - Objective function: 21.16601905296887 - Loss: 21.108334753060564 - Gradient norm: 3.7446990558822613\n",
      "Epoch 565 - Objective function: 21.15187706071499 - Loss: 21.094169416854804 - Gradient norm: 3.7408689902470376\n",
      "Epoch 566 - Objective function: 21.137763664315735 - Loss: 21.080032662053252 - Gradient norm: 3.7370335602147726\n",
      "Epoch 567 - Objective function: 21.123678877479044 - Loss: 21.06592450256734 - Gradient norm: 3.733192838896864\n",
      "Epoch 568 - Objective function: 21.10962271322989 - Loss: 21.051844951625057 - Gradient norm: 3.7293468992124295\n",
      "Epoch 569 - Objective function: 21.095595183913613 - Loss: 21.037794021774307 - Gradient norm: 3.7254958138848417\n",
      "Epoch 570 - Objective function: 21.081596301199323 - Loss: 21.023771724886306 - Gradient norm: 3.7216396554383016\n",
      "Epoch 571 - Objective function: 21.067626076083343 - Loss: 21.009778072159015 - Gradient norm: 3.717778496194491\n",
      "Epoch 572 - Objective function: 21.053684518892663 - Loss: 20.995813074120584 - Gradient norm: 3.7139124082692683\n",
      "Epoch 573 - Objective function: 21.039771639288404 - Loss: 20.981876740632828 - Gradient norm: 3.7100414635694317\n",
      "Epoch 574 - Objective function: 21.02588744626939 - Loss: 20.967969080894754 - Gradient norm: 3.706165733789535\n",
      "Epoch 575 - Objective function: 21.012031948175597 - Loss: 20.954090103446063 - Gradient norm: 3.7022852904087635\n",
      "Epoch 576 - Objective function: 20.998205152691806 - Loss: 20.940239816170738 - Gradient norm: 3.698400204687868\n",
      "Epoch 577 - Objective function: 20.984407066851116 - Loss: 20.926418226300598 - Gradient norm: 3.6945105476661486\n",
      "Epoch 578 - Objective function: 20.970637697038615 - Loss: 20.91262534041892 - Gradient norm: 3.6906163901585076\n",
      "Epoch 579 - Objective function: 20.95689704899496 - Loss: 20.898861164464066 - Gradient norm: 3.686717802752548\n",
      "Epoch 580 - Objective function: 20.943185127820076 - Loss: 20.88512570373313 - Gradient norm: 3.68281485580573\n",
      "Epoch 581 - Objective function: 20.929501937976816 - Loss: 20.871418962885617 - Gradient norm: 3.6789076194425863\n",
      "Epoch 582 - Objective function: 20.91584748329464 - Loss: 20.857740945947107 - Gradient norm: 3.674996163551992\n",
      "Epoch 583 - Objective function: 20.90222176697336 - Loss: 20.84409165631301 - Gradient norm: 3.6710805577844847\n",
      "Epoch 584 - Objective function: 20.888624791586885 - Loss: 20.830471096752284 - Gradient norm: 3.6671608715496453\n",
      "Epoch 585 - Objective function: 20.87505655908692 - Loss: 20.81687926941116 - Gradient norm: 3.6632371740135286\n",
      "Epoch 586 - Objective function: 20.861517070806826 - Loss: 20.803316175816946 - Gradient norm: 3.65930953409615\n",
      "Epoch 587 - Objective function: 20.848006327465328 - Loss: 20.789781816881792 - Gradient norm: 3.655378020469021\n",
      "Epoch 588 - Objective function: 20.834524329170385 - Loss: 20.776276192906515 - Gradient norm: 3.6514427015527486\n",
      "Epoch 589 - Objective function: 20.82107107542298 - Loss: 20.76279930358439 - Gradient norm: 3.6475036455146737\n",
      "Epoch 590 - Objective function: 20.807646565120958 - Loss: 20.749351148005008 - Gradient norm: 3.643560920266568\n",
      "Epoch 591 - Objective function: 20.794250796562906 - Loss: 20.73593172465811 - Gradient norm: 3.6396145934623894\n",
      "Epoch 592 - Objective function: 20.780883767452 - Loss: 20.722541031437466 - Gradient norm: 3.6356647324960707\n",
      "Epoch 593 - Objective function: 20.76754547489989 - Loss: 20.709179065644747 - Gradient norm: 3.6317114044993843\n",
      "Epoch 594 - Objective function: 20.754235915430602 - Loss: 20.695845823993402 - Gradient norm: 3.6277546763398325\n",
      "Epoch 595 - Objective function: 20.740955084984442 - Loss: 20.68254130261259 - Gradient norm: 3.6237946146186077\n",
      "Epoch 596 - Objective function: 20.727702978921904 - Loss: 20.669265497051065 - Gradient norm: 3.6198312856685892\n",
      "Epoch 597 - Objective function: 20.714479592027615 - Loss: 20.65601840228112 - Gradient norm: 3.615864755552398\n",
      "Epoch 598 - Objective function: 20.70128491851427 - Loss: 20.642800012702516 - Gradient norm: 3.6118950900604956\n",
      "Epoch 599 - Objective function: 20.688118952026574 - Loss: 20.629610322146426 - Gradient norm: 3.6079223547093333\n",
      "Epoch 600 - Objective function: 20.67498168564522 - Loss: 20.61644932387941 - Gradient norm: 3.60394661473955\n",
      "Epoch 601 - Objective function: 20.661873111890834 - Loss: 20.603317010607345 - Gradient norm: 3.599967935114216\n",
      "Epoch 602 - Objective function: 20.64879322272796 - Loss: 20.59021337447943 - Gradient norm: 3.5959863805171293\n",
      "Epoch 603 - Objective function: 20.635742009569054 - Loss: 20.577138407092143 - Gradient norm: 3.5920020153511483\n",
      "Epoch 604 - Objective function: 20.62271946327846 - Loss: 20.564092099493248 - Gradient norm: 3.5880149037365867\n",
      "Epoch 605 - Objective function: 20.60972557417642 - Loss: 20.551074442185772 - Gradient norm: 3.584025109509638\n",
      "Epoch 606 - Objective function: 20.59676033204307 - Loss: 20.538085425132024 - Gradient norm: 3.580032696220857\n",
      "Epoch 607 - Objective function: 20.58382372612243 - Loss: 20.525125037757565 - Gradient norm: 3.576037727133683\n",
      "Epoch 608 - Objective function: 20.57091574512647 - Loss: 20.51219326895527 - Gradient norm: 3.572040265223008\n",
      "Epoch 609 - Objective function: 20.558036377239063 - Loss: 20.49929010708929 - Gradient norm: 3.568040373173785\n",
      "Epoch 610 - Objective function: 20.54518561012005 - Loss: 20.486415539999115 - Gradient norm: 3.5640381133796937\n",
      "Epoch 611 - Objective function: 20.532363430909268 - Loss: 20.473569555003568 - Gradient norm: 3.5600335479418304\n",
      "Epoch 612 - Objective function: 20.519569826230523 - Loss: 20.46075213890483 - Gradient norm: 3.556026738667458\n",
      "Epoch 613 - Objective function: 20.506804782195708 - Loss: 20.447963277992493 - Gradient norm: 3.5520177470687875\n",
      "Epoch 614 - Objective function: 20.494068284408737 - Loss: 20.435202958047554 - Gradient norm: 3.548006634361812\n",
      "Epoch 615 - Objective function: 20.481360317969667 - Loss: 20.422471164346476 - Gradient norm: 3.543993461465171\n",
      "Epoch 616 - Objective function: 20.468680867478657 - Loss: 20.409767881665193 - Gradient norm: 3.539978288999064\n",
      "Epoch 617 - Objective function: 20.45602991704007 - Loss: 20.397093094283168 - Gradient norm: 3.5359611772842054\n",
      "Epoch 618 - Objective function: 20.44340745026646 - Loss: 20.384446785987418 - Gradient norm: 3.531942186340814\n",
      "Epoch 619 - Objective function: 20.43081345028262 - Loss: 20.37182894007652 - Gradient norm: 3.527921375887648\n",
      "Epoch 620 - Objective function: 20.41824789972961 - Loss: 20.359239539364673 - Gradient norm: 3.5238988053410796\n",
      "Epoch 621 - Objective function: 20.4057107807688 - Loss: 20.346678566185716 - Gradient norm: 3.519874533814204\n",
      "Epoch 622 - Objective function: 20.3932020750859 - Loss: 20.334146002397155 - Gradient norm: 3.515848620115993\n",
      "Epoch 623 - Objective function: 20.380721763894968 - Loss: 20.321641829384188 - Gradient norm: 3.5118211227504825\n",
      "Epoch 624 - Objective function: 20.368269827942463 - Loss: 20.309166028063736 - Gradient norm: 3.507792099916002\n",
      "Epoch 625 - Objective function: 20.355846247511234 - Loss: 20.296718578888438 - Gradient norm: 3.503761609504439\n",
      "Epoch 626 - Objective function: 20.343451002424562 - Loss: 20.284299461850694 - Gradient norm: 3.499729709100541\n",
      "Epoch 627 - Objective function: 20.331084072050178 - Loss: 20.271908656486676 - Gradient norm: 3.4956964559812542\n",
      "Epoch 628 - Objective function: 20.318745435304244 - Loss: 20.25954614188031 - Gradient norm: 3.491661907115099\n",
      "Epoch 629 - Objective function: 20.30643507065539 - Loss: 20.247211896667313 - Gradient norm: 3.4876261191615816\n",
      "Epoch 630 - Objective function: 20.294152956128695 - Loss: 20.234905899039166 - Gradient norm: 3.4835891484706387\n",
      "Epoch 631 - Objective function: 20.28189906930968 - Loss: 20.222628126747114 - Gradient norm: 3.4795510510821206\n",
      "Epoch 632 - Objective function: 20.269673387348316 - Loss: 20.21037855710616 - Gradient norm: 3.4755118827253075\n",
      "Epoch 633 - Objective function: 20.25747588696298 - Loss: 20.198157166999035 - Gradient norm: 3.4714716988184575\n",
      "Epoch 634 - Objective function: 20.24530654444445 - Loss: 20.18596393288019 - Gradient norm: 3.4674305544683914\n",
      "Epoch 635 - Objective function: 20.233165335659876 - Loss: 20.173798830779738 - Gradient norm: 3.4633885044701107\n",
      "Epoch 636 - Objective function: 20.221052236056714 - Loss: 20.16166183630743 - Gradient norm: 3.4593456033064487\n",
      "Epoch 637 - Objective function: 20.20896722066673 - Loss: 20.14955292465662 - Gradient norm: 3.45530190514775\n",
      "Epoch 638 - Objective function: 20.196910264109867 - Loss: 20.137472070608172 - Gradient norm: 3.4512574638515856\n",
      "Epoch 639 - Objective function: 20.184881340598288 - Loss: 20.125419248534442 - Gradient norm: 3.447212332962503\n",
      "Epoch 640 - Objective function: 20.172880423940207 - Loss: 20.11339443240316 - Gradient norm: 3.4431665657117967\n",
      "Epoch 641 - Objective function: 20.160907487543867 - Loss: 20.1013975957814 - Gradient norm: 3.439120215017323\n",
      "Epoch 642 - Objective function: 20.148962504421434 - Loss: 20.08942871183944 - Gradient norm: 3.435073333483336\n",
      "Epoch 643 - Objective function: 20.137045447192882 - Loss: 20.077487753354692 - Gradient norm: 3.431025973400355\n",
      "Epoch 644 - Objective function: 20.125156288089926 - Loss: 20.06557469271559 - Gradient norm: 3.426978186745065\n",
      "Epoch 645 - Objective function: 20.113294998959862 - Loss: 20.05368950192547 - Gradient norm: 3.4229300251802406\n",
      "Epoch 646 - Objective function: 20.10146155126947 - Loss: 20.041832152606418 - Gradient norm: 3.418881540054708\n",
      "Epoch 647 - Objective function: 20.089655916108832 - Loss: 20.030002616003163 - Gradient norm: 3.414832782403322\n",
      "Epoch 648 - Objective function: 20.077878064195218 - Loss: 20.018200862986905 - Gradient norm: 3.4107838029469857\n",
      "Epoch 649 - Objective function: 20.06612796587693 - Loss: 20.006426864059158 - Gradient norm: 3.406734652092684\n",
      "Epoch 650 - Objective function: 20.054405591137094 - Loss: 19.99468058935557 - Gradient norm: 3.4026853799335575\n",
      "Epoch 651 - Objective function: 20.042710909597496 - Loss: 19.982962008649757 - Gradient norm: 3.39863603624899\n",
      "Epoch 652 - Objective function: 20.03104389052238 - Loss: 19.971271091357067 - Gradient norm: 3.394586670504732\n",
      "Epoch 653 - Objective function: 20.019404502822248 - Loss: 19.959607806538415 - Gradient norm: 3.3905373318530496\n",
      "Epoch 654 - Objective function: 20.007792715057615 - Loss: 19.947972122904023 - Gradient norm: 3.386488069132891\n",
      "Epoch 655 - Objective function: 19.996208495442794 - Loss: 19.936364008817208 - Gradient norm: 3.3824389308700864\n",
      "Epoch 656 - Objective function: 19.984651811849645 - Loss: 19.92478343229812 - Gradient norm: 3.378389965277571\n",
      "Epoch 657 - Objective function: 19.9731226318113 - Loss: 19.913230361027487 - Gradient norm: 3.374341220255633\n",
      "Epoch 658 - Objective function: 19.96162092252592 - Loss: 19.901704762350345 - Gradient norm: 3.3702927433921768\n",
      "Epoch 659 - Objective function: 19.950146650860358 - Loss: 19.890206603279733 - Gradient norm: 3.3662445819630267\n",
      "Epoch 660 - Objective function: 19.938699783353893 - Loss: 19.878735850500398 - Gradient norm: 3.362196782932238\n",
      "Epoch 661 - Objective function: 19.927280286221915 - Loss: 19.867292470372497 - Gradient norm: 3.3581493929524413\n",
      "Epoch 662 - Objective function: 19.915888125359576 - Loss: 19.85587642893524 - Gradient norm: 3.354102458365202\n",
      "Epoch 663 - Objective function: 19.904523266345443 - Loss: 19.844487691910548 - Gradient norm: 3.3500560252014098\n",
      "Epoch 664 - Objective function: 19.893185674445164 - Loss: 19.83312622470671 - Gradient norm: 3.3460101391816846\n",
      "Epoch 665 - Objective function: 19.881875314615062 - Loss: 19.821791992421996 - Gradient norm: 3.341964845716803\n",
      "Epoch 666 - Objective function: 19.870592151505758 - Loss: 19.81048495984826 - Gradient norm: 3.3379201899081536\n",
      "Epoch 667 - Objective function: 19.859336149465793 - Loss: 19.799205091474562 - Gradient norm: 3.3338762165482\n",
      "Epoch 668 - Objective function: 19.848107272545146 - Loss: 19.787952351490713 - Gradient norm: 3.3298329701209806\n",
      "Epoch 669 - Objective function: 19.83690548449886 - Loss: 19.776726703790864 - Gradient norm: 3.3257904948026136\n",
      "Epoch 670 - Objective function: 19.825730748790544 - Loss: 19.76552811197704 - Gradient norm: 3.32174883446183\n",
      "Epoch 671 - Objective function: 19.81458302859594 - Loss: 19.754356539362686 - Gradient norm: 3.3177080326605255\n",
      "Epoch 672 - Objective function: 19.80346228680642 - Loss: 19.743211948976167 - Gradient norm: 3.313668132654327\n",
      "Epoch 673 - Objective function: 19.792368486032483 - Loss: 19.732094303564296 - Gradient norm: 3.3096291773931856\n",
      "Epoch 674 - Objective function: 19.781301588607256 - Loss: 19.721003565595772 - Gradient norm: 3.3055912095219777\n",
      "Epoch 675 - Objective function: 19.770261556589944 - Loss: 19.709939697264694 - Gradient norm: 3.301554271381133\n",
      "Epoch 676 - Objective function: 19.759248351769283 - Loss: 19.698902660493992 - Gradient norm: 3.2975184050072754\n",
      "Epoch 677 - Objective function: 19.748261935666996 - Loss: 19.68789241693886 - Gradient norm: 3.293483652133885\n",
      "Epoch 678 - Objective function: 19.73730226954117 - Loss: 19.676908927990166 - Gradient norm: 3.2894500541919705\n",
      "Epoch 679 - Objective function: 19.726369314389682 - Loss: 19.66595215477787 - Gradient norm: 3.2854176523107697\n",
      "Epoch 680 - Objective function: 19.715463030953586 - Loss: 19.655022058174403 - Gradient norm: 3.2813864873184495\n",
      "Epoch 681 - Objective function: 19.704583379720464 - Loss: 19.64411859879802 - Gradient norm: 3.277356599742844\n",
      "Epoch 682 - Objective function: 19.693730320927788 - Loss: 19.633241737016167 - Gradient norm: 3.2733280298121854\n",
      "Epoch 683 - Objective function: 19.682903814566224 - Loss: 19.6223914329488 - Gradient norm: 3.2693008174558686\n",
      "Epoch 684 - Objective function: 19.672103820382972 - Loss: 19.611567646471688 - Gradient norm: 3.2652750023052195\n",
      "Epoch 685 - Objective function: 19.66133029788506 - Loss: 19.60077033721975 - Gradient norm: 3.261250623694285\n",
      "Epoch 686 - Objective function: 19.650583206342603 - Loss: 19.589999464590285 - Gradient norm: 3.2572277206606333\n",
      "Epoch 687 - Objective function: 19.639862504792085 - Loss: 19.57925498774626 - Gradient norm: 3.2532063319461706\n",
      "Epoch 688 - Objective function: 19.62916815203956 - Loss: 19.56853686561954 - Gradient norm: 3.24918649599797\n",
      "Epoch 689 - Objective function: 19.61850010666396 - Loss: 19.55784505691414 - Gradient norm: 3.245168250969117\n",
      "Epoch 690 - Objective function: 19.60785832702019 - Loss: 19.547179520109385 - Gradient norm: 3.2411516347195675\n",
      "Epoch 691 - Objective function: 19.5972427712424 - Loss: 19.536540213463134 - Gradient norm: 3.2371366848170138\n",
      "Epoch 692 - Objective function: 19.58665339724711 - Loss: 19.525927095014943 - Gradient norm: 3.233123438537771\n",
      "Epoch 693 - Objective function: 19.5760901627364 - Loss: 19.515340122589215 - Gradient norm: 3.2291119328676734\n",
      "Epoch 694 - Objective function: 19.565553025200995 - Loss: 19.504779253798322 - Gradient norm: 3.2251022045029774\n",
      "Epoch 695 - Objective function: 19.555041941923403 - Loss: 19.494244446045737 - Gradient norm: 3.2210942898512873\n",
      "Epoch 696 - Objective function: 19.544556869981026 - Loss: 19.483735656529127 - Gradient norm: 3.2170882250324806\n",
      "Epoch 697 - Objective function: 19.534097766249204 - Loss: 19.47325284224341 - Gradient norm: 3.213084045879654\n",
      "Epoch 698 - Objective function: 19.523664587404294 - Loss: 19.46279595998386 - Gradient norm: 3.2090817879400815\n",
      "Epoch 699 - Objective function: 19.513257289926713 - Loss: 19.452364966349094 - Gradient norm: 3.205081486476167\n",
      "Epoch 700 - Objective function: 19.502875830103935 - Loss: 19.44195981774413 - Gradient norm: 3.201083176466434\n",
      "Epoch 701 - Objective function: 19.492520164033532 - Loss: 19.43158047038338 - Gradient norm: 3.1970868926065\n",
      "Epoch 702 - Objective function: 19.482190247626097 - Loss: 19.421226880293627 - Gradient norm: 3.193092669310079\n",
      "Epoch 703 - Objective function: 19.471886036608275 - Loss: 19.410899003317002 - Gradient norm: 3.189100540709984\n",
      "Epoch 704 - Objective function: 19.46160748652567 - Loss: 19.400596795113923 - Gradient norm: 3.1851105406591422\n",
      "Epoch 705 - Objective function: 19.451354552745777 - Loss: 19.390320211166028 - Gradient norm: 3.1811227027316225\n",
      "Epoch 706 - Objective function: 19.441127190460897 - Loss: 19.380069206779083 - Gradient norm: 3.1771370602236644\n",
      "Epoch 707 - Objective function: 19.43092535469101 - Loss: 19.369843737085866 - Gradient norm: 3.173153646154724\n",
      "Epoch 708 - Objective function: 19.42074900028667 - Loss: 19.359643757049046 - Gradient norm: 3.169172493268524\n",
      "Epoch 709 - Objective function: 19.41059808193181 - Loss: 19.349469221464016 - Gradient norm: 3.1651936340341114\n",
      "Epoch 710 - Objective function: 19.400472554146653 - Loss: 19.33932008496176 - Gradient norm: 3.1612171006469287\n",
      "Epoch 711 - Objective function: 19.390372371290425 - Loss: 19.329196302011642 - Gradient norm: 3.1572429250298857\n",
      "Epoch 712 - Objective function: 19.380297487564235 - Loss: 19.3190978269242 - Gradient norm: 3.1532711388344437\n",
      "Epoch 713 - Objective function: 19.370247857013787 - Loss: 19.30902461385393 - Gradient norm: 3.149301773441707\n",
      "Epoch 714 - Objective function: 19.360223433532177 - Loss: 19.29897661680204 - Gradient norm: 3.1453348599635187\n",
      "Epoch 715 - Objective function: 19.3502241708626 - Loss: 19.28895378961919 - Gradient norm: 3.141370429243568\n",
      "Epoch 716 - Objective function: 19.340250022601083 - Loss: 19.278956086008204 - Gradient norm: 3.1374085118584967\n",
      "Epoch 717 - Objective function: 19.330300942199187 - Loss: 19.268983459526776 - Gradient norm: 3.1334491381190244\n",
      "Epoch 718 - Objective function: 19.32037688296667 - Loss: 19.259035863590157 - Gradient norm: 3.1294923380710657\n",
      "Epoch 719 - Objective function: 19.310477798074157 - Loss: 19.249113251473787 - Gradient norm: 3.125538141496866\n",
      "Epoch 720 - Objective function: 19.30060364055577 - Loss: 19.239215576315967 - Gradient norm: 3.1215865779161347\n",
      "Epoch 721 - Objective function: 19.290754363311766 - Loss: 19.22934279112048 - Gradient norm: 3.1176376765871874\n",
      "Epoch 722 - Objective function: 19.280929919111138 - Loss: 19.219494848759172 - Gradient norm: 3.1136914665080955\n",
      "Epoch 723 - Objective function: 19.271130260594163 - Loss: 19.209671701974568 - Gradient norm: 3.1097479764178377\n",
      "Epoch 724 - Objective function: 19.261355340275028 - Loss: 19.199873303382404 - Gradient norm: 3.105807234797457\n",
      "Epoch 725 - Objective function: 19.25160511054431 - Loss: 19.190099605474206 - Gradient norm: 3.101869269871226\n",
      "Epoch 726 - Objective function: 19.241879523671567 - Loss: 19.180350560619804 - Gradient norm: 3.0979341096078112\n",
      "Epoch 727 - Objective function: 19.232178531807786 - Loss: 19.17062612106983 - Gradient norm: 3.0940017817214507\n",
      "Epoch 728 - Objective function: 19.222502086987895 - Loss: 19.16092623895823 - Gradient norm: 3.0900723136731236\n",
      "Epoch 729 - Objective function: 19.212850141133252 - Loss: 19.151250866304714 - Gradient norm: 3.0861457326717368\n",
      "Epoch 730 - Objective function: 19.203222646054073 - Loss: 19.14159995501723 - Gradient norm: 3.082222065675304\n",
      "Epoch 731 - Objective function: 19.193619553451867 - Loss: 19.13197345689438 - Gradient norm: 3.078301339392141\n",
      "Epoch 732 - Objective function: 19.184040814921843 - Loss: 19.122371323627835 - Gradient norm: 3.074383580282051\n",
      "Epoch 733 - Objective function: 19.174486381955326 - Loss: 19.112793506804746 - Gradient norm: 3.0704688145575236\n",
      "Epoch 734 - Objective function: 19.164956205942104 - Loss: 19.10323995791011 - Gradient norm: 3.0665570681849346\n",
      "Epoch 735 - Objective function: 19.1554502381728 - Loss: 19.09371062832912 - Gradient norm: 3.0626483668857443\n",
      "Epoch 736 - Objective function: 19.145968429841226 - Loss: 19.08420546934953 - Gradient norm: 3.0587427361377033\n",
      "Epoch 737 - Objective function: 19.136510732046652 - Loss: 19.074724432163944 - Gradient norm: 3.054840201176064\n",
      "Epoch 738 - Objective function: 19.127077095796178 - Loss: 19.06526746787215 - Gradient norm: 3.050940786994786\n",
      "Epoch 739 - Objective function: 19.11766747200694 - Loss: 19.05583452748338 - Gradient norm: 3.0470445183477506\n",
      "Epoch 740 - Objective function: 19.10828181150842 - Loss: 19.046425561918575 - Gradient norm: 3.043151419749974\n",
      "Epoch 741 - Objective function: 19.0989200650447 - Loss: 19.037040522012664 - Gradient norm: 3.039261515478828\n",
      "Epoch 742 - Objective function: 19.08958218327665 - Loss: 19.02767935851675 - Gradient norm: 3.0353748295752565\n",
      "Epoch 743 - Objective function: 19.08026811678416 - Loss: 19.01834202210035 - Gradient norm: 3.031491385844995\n",
      "Epoch 744 - Objective function: 19.070977816068332 - Loss: 19.009028463353566 - Gradient norm: 3.0276112078597963\n",
      "Epoch 745 - Objective function: 19.061711231553627 - Loss: 18.999738632789278 - Gradient norm: 3.0237343189586525\n",
      "Epoch 746 - Objective function: 19.05246831359006 - Loss: 18.990472480845284 - Gradient norm: 3.0198607422490205\n",
      "Epoch 747 - Objective function: 19.043249012455284 - Loss: 18.981229957886445 - Gradient norm: 3.0159905006080487\n",
      "Epoch 748 - Objective function: 19.03405327835677 - Loss: 18.97201101420681 - Gradient norm: 3.0121236166838057\n",
      "Epoch 749 - Objective function: 19.02488106143383 - Loss: 18.962815600031686 - Gradient norm: 3.0082601128965085\n",
      "Epoch 750 - Objective function: 19.015732311759773 - Loss: 18.95364366551977 - Gradient norm: 3.0044000114397504\n",
      "Epoch 751 - Objective function: 19.006606979343896 - Loss: 18.94449516076517 - Gradient norm: 3.0005433342817334\n",
      "Epoch 752 - Objective function: 18.99750501413362 - Loss: 18.935370035799487 - Gradient norm: 2.9966901031665003\n",
      "Epoch 753 - Objective function: 18.9884263660164 - Loss: 18.9262682405938 - Gradient norm: 2.99284033961516\n",
      "Epoch 754 - Objective function: 18.979370984821845 - Loss: 18.917189725060734 - Gradient norm: 2.988994064927126\n",
      "Epoch 755 - Objective function: 18.970338820323636 - Loss: 18.908134439056404 - Gradient norm: 2.9851513001813434\n",
      "Epoch 756 - Objective function: 18.961329822241545 - Loss: 18.899102332382423 - Gradient norm: 2.981312066237523\n",
      "Epoch 757 - Objective function: 18.952343940243367 - Loss: 18.890093354787847 - Gradient norm: 2.9774763837373723\n",
      "Epoch 758 - Objective function: 18.943381123946846 - Loss: 18.881107455971115 - Gradient norm: 2.9736442731058266\n",
      "Epoch 759 - Objective function: 18.934441322921657 - Loss: 18.87214458558198 - Gradient norm: 2.9698157545522808\n",
      "Epoch 760 - Objective function: 18.925524486691234 - Loss: 18.863204693223416 - Gradient norm: 2.965990848071819\n",
      "Epoch 761 - Objective function: 18.91663056473471 - Loss: 18.854287728453492 - Gradient norm: 2.962169573446449\n",
      "Epoch 762 - Objective function: 18.907759506488766 - Loss: 18.845393640787268 - Gradient norm: 2.958351950246323\n",
      "Epoch 763 - Objective function: 18.89891126134948 - Loss: 18.83652237969863 - Gradient norm: 2.9545379978309803\n",
      "Epoch 764 - Objective function: 18.890085778674177 - Loss: 18.827673894622126 - Gradient norm: 2.950727735350558\n",
      "Epoch 765 - Objective function: 18.881283007783253 - Loss: 18.81884813495482 - Gradient norm: 2.9469211817470353\n",
      "Epoch 766 - Objective function: 18.872502897961926 - Loss: 18.810045050058044 - Gradient norm: 2.9431183557554457\n",
      "Epoch 767 - Objective function: 18.8637453984621 - Loss: 18.80126458925923 - Gradient norm: 2.9393192759051114\n",
      "Epoch 768 - Objective function: 18.855010458504072 - Loss: 18.792506701853654 - Gradient norm: 2.9355239605208627\n",
      "Epoch 769 - Objective function: 18.846298027278305 - Loss: 18.783771337106195 - Gradient norm: 2.9317324277242585\n",
      "Epoch 770 - Objective function: 18.83760805394716 - Loss: 18.77505844425309 - Gradient norm: 2.9279446954348094\n",
      "Epoch 771 - Objective function: 18.82894048764663 - Loss: 18.76636797250363 - Gradient norm: 2.9241607813712003\n",
      "Epoch 772 - Objective function: 18.820295277487993 - Loss: 18.757699871041858 - Gradient norm: 2.920380703052499\n",
      "Epoch 773 - Objective function: 18.81167237255957 - Loss: 18.749054089028295 - Gradient norm: 2.916604477799382\n",
      "Epoch 774 - Objective function: 18.803071721928333 - Loss: 18.740430575601575 - Gradient norm: 2.9128321227353413\n",
      "Epoch 775 - Objective function: 18.794493274641574 - Loss: 18.731829279880113 - Gradient norm: 2.9090636547878987\n",
      "Epoch 776 - Objective function: 18.785936979728568 - Loss: 18.723250150963747 - Gradient norm: 2.9052990906898177\n",
      "Epoch 777 - Objective function: 18.777402786202163 - Loss: 18.714693137935352 - Gradient norm: 2.901538446980309\n",
      "Epoch 778 - Objective function: 18.768890643060395 - Loss: 18.70615818986246 - Gradient norm: 2.8977817400062373\n",
      "Epoch 779 - Objective function: 18.760400499288064 - Loss: 18.69764525579883 - Gradient norm: 2.8940289859233244\n",
      "Epoch 780 - Objective function: 18.75193230385835 - Loss: 18.689154284786046 - Gradient norm: 2.890280200697352\n",
      "Epoch 781 - Objective function: 18.743486005734297 - Loss: 18.680685225855058 - Gradient norm: 2.8865354001053576\n",
      "Epoch 782 - Objective function: 18.73506155387044 - Loss: 18.672238028027742 - Gradient norm: 2.8827945997368345\n",
      "Epoch 783 - Objective function: 18.726658897214254 - Loss: 18.66381264031841 - Gradient norm: 2.879057814994922\n",
      "Epoch 784 - Objective function: 18.71827798470771 - Loss: 18.655409011735337 - Gradient norm: 2.8753250610976\n",
      "Epoch 785 - Objective function: 18.709918765288762 - Loss: 18.647027091282254 - Gradient norm: 2.871596353078873\n",
      "Epoch 786 - Objective function: 18.70158118789282 - Loss: 18.638666827959828 - Gradient norm: 2.867871705789962\n",
      "Epoch 787 - Objective function: 18.693265201454206 - Loss: 18.63032817076712 - Gradient norm: 2.864151133900481\n",
      "Epoch 788 - Objective function: 18.68497075490763 - Loss: 18.622011068703063 - Gradient norm: 2.860434651899621\n",
      "Epoch 789 - Objective function: 18.676697797189604 - Loss: 18.61371547076787 - Gradient norm: 2.8567222740973257\n",
      "Epoch 790 - Objective function: 18.66844627723984 - Loss: 18.60544132596446 - Gradient norm: 2.8530140146254657\n",
      "Epoch 791 - Objective function: 18.660216144002725 - Loss: 18.59718858329989 - Gradient norm: 2.849309887439009\n",
      "Epoch 792 - Objective function: 18.65200734642863 - Loss: 18.58895719178671 - Gradient norm: 2.845609906317189\n",
      "Epoch 793 - Objective function: 18.643819833475327 - Loss: 18.580747100444373 - Gradient norm: 2.8419140848646687\n",
      "Epoch 794 - Objective function: 18.635653554109354 - Loss: 18.572558258300568 - Gradient norm: 2.838222436512702\n",
      "Epoch 795 - Objective function: 18.62750845730734 - Loss: 18.564390614392586 - Gradient norm: 2.8345349745202917\n",
      "Epoch 796 - Objective function: 18.61938449205733 - Loss: 18.556244117768653 - Gradient norm: 2.8308517119753445\n",
      "Epoch 797 - Objective function: 18.611281607360148 - Loss: 18.548118717489245 - Gradient norm: 2.827172661795821\n",
      "Epoch 798 - Objective function: 18.60319975223064 - Loss: 18.540014362628384 - Gradient norm: 2.8234978367308834\n",
      "Epoch 799 - Objective function: 18.595138875699003 - Loss: 18.531931002274952 - Gradient norm: 2.8198272493620395\n",
      "Epoch 800 - Objective function: 18.587098926812047 - Loss: 18.52386858553394 - Gradient norm: 2.816160912104285\n",
      "Epoch 801 - Objective function: 18.57907985463444 - Loss: 18.515827061527723 - Gradient norm: 2.812498837207237\n",
      "Epoch 802 - Objective function: 18.571081608249997 - Loss: 18.50780637939732 - Gradient norm: 2.8088410367562693\n",
      "Epoch 803 - Objective function: 18.56310413676285 - Loss: 18.4998064883036 - Gradient norm: 2.80518752267364\n",
      "Epoch 804 - Objective function: 18.555147389298728 - Loss: 18.491827337428543 - Gradient norm: 2.801538306719621\n",
      "Epoch 805 - Objective function: 18.54721131500612 - Loss: 18.483868875976395 - Gradient norm: 2.797893400493614\n",
      "Epoch 806 - Objective function: 18.53929586305748 - Loss: 18.475931053174914 - Gradient norm: 2.794252815435271\n",
      "Epoch 807 - Objective function: 18.531400982650418 - Loss: 18.468013818276518 - Gradient norm: 2.79061656282561\n",
      "Epoch 808 - Objective function: 18.52352662300884 - Loss: 18.46011712055946 - Gradient norm: 2.7869846537881235\n",
      "Epoch 809 - Objective function: 18.515672733384125 - Loss: 18.45224090932898 - Gradient norm: 2.7833570992898817\n",
      "Epoch 810 - Objective function: 18.507839263056223 - Loss: 18.44438513391845 - Gradient norm: 2.7797339101426393\n",
      "Epoch 811 - Objective function: 18.50002616133484 - Loss: 18.43654974369049 - Gradient norm: 2.7761150970039288\n",
      "Epoch 812 - Objective function: 18.492233377560495 - Loss: 18.428734688038112 - Gradient norm: 2.7725006703781587\n",
      "Epoch 813 - Objective function: 18.484460861105646 - Loss: 18.420939916385766 - Gradient norm: 2.7688906406176983\n",
      "Epoch 814 - Objective function: 18.476708561375766 - Loss: 18.413165378190488 - Gradient norm: 2.7652850179239685\n",
      "Epoch 815 - Objective function: 18.46897642781043 - Loss: 18.405411022942936 - Gradient norm: 2.761683812348516\n",
      "Epoch 816 - Objective function: 18.46126440988435 - Loss: 18.397676800168462 - Gradient norm: 2.758087033794097\n",
      "Epoch 817 - Objective function: 18.453572457108457 - Loss: 18.38996265942818 - Gradient norm: 2.7544946920157467\n",
      "Epoch 818 - Objective function: 18.445900519030893 - Loss: 18.382268550319967 - Gradient norm: 2.7509067966218477\n",
      "Epoch 819 - Objective function: 18.438248545238082 - Loss: 18.374594422479525 - Gradient norm: 2.7473233570751936\n",
      "Epoch 820 - Objective function: 18.430616485355717 - Loss: 18.366940225581384 - Gradient norm: 2.743744382694051\n",
      "Epoch 821 - Objective function: 18.423004289049736 - Loss: 18.35930590933987 - Gradient norm: 2.7401698826532117\n",
      "Epoch 822 - Objective function: 18.415411906027348 - Loss: 18.35169142351015 - Gradient norm: 2.7365998659850455\n",
      "Epoch 823 - Objective function: 18.407839286037998 - Loss: 18.344096717889165 - Gradient norm: 2.7330343415805465\n",
      "Epoch 824 - Objective function: 18.400286378874295 - Loss: 18.336521742316595 - Gradient norm: 2.7294733181903736\n",
      "Epoch 825 - Objective function: 18.392753134373017 - Loss: 18.328966446675857 - Gradient norm: 2.72591680442589\n",
      "Epoch 826 - Objective function: 18.385239502415995 - Loss: 18.321430780894975 - Gradient norm: 2.722364808760194\n",
      "Epoch 827 - Objective function: 18.377745432931086 - Loss: 18.313914694947574 - Gradient norm: 2.71881733952915\n",
      "Epoch 828 - Objective function: 18.370270875893052 - Loss: 18.306418138853758 - Gradient norm: 2.71527440493241\n",
      "Epoch 829 - Objective function: 18.36281578132449 - Loss: 18.298941062681028 - Gradient norm: 2.711736013034432\n",
      "Epoch 830 - Objective function: 18.355380099296706 - Loss: 18.291483416545173 - Gradient norm: 2.7082021717654987\n",
      "Epoch 831 - Objective function: 18.347963779930602 - Loss: 18.28404515061116 - Gradient norm: 2.7046728889227247\n",
      "Epoch 832 - Objective function: 18.340566773397555 - Loss: 18.276626215094 - Gradient norm: 2.7011481721710626\n",
      "Epoch 833 - Objective function: 18.33318902992025 - Loss: 18.269226560259604 - Gradient norm: 2.6976280290443015\n",
      "Epoch 834 - Objective function: 18.32583049977355 - Loss: 18.261846136425635 - Gradient norm: 2.6941124669460677\n",
      "Epoch 835 - Objective function: 18.318491133285335 - Loss: 18.25448489396236 - Gradient norm: 2.6906014931508113\n",
      "Epoch 836 - Objective function: 18.3111708808373 - Loss: 18.24714278329345 - Gradient norm: 2.6870951148047975\n",
      "Epoch 837 - Objective function: 18.3038696928658 - Loss: 18.239819754896825 - Gradient norm: 2.6835933389270847\n",
      "Epoch 838 - Objective function: 18.296587519862626 - Loss: 18.23251575930543 - Gradient norm: 2.6800961724105035\n",
      "Epoch 839 - Objective function: 18.289324312375822 - Loss: 18.225230747108057 - Gradient norm: 2.676603622022631\n",
      "Epoch 840 - Objective function: 18.282080021010444 - Loss: 18.217964668950106 - Gradient norm: 2.673115694406753\n",
      "Epoch 841 - Objective function: 18.27485459642936 - Loss: 18.21071747553439 - Gradient norm: 2.6696323960828354\n",
      "Epoch 842 - Objective function: 18.26764798935398 - Loss: 18.203489117621853 - Gradient norm: 2.6661537334484744\n",
      "Epoch 843 - Objective function: 18.260460150565017 - Loss: 18.196279546032365 - Gradient norm: 2.662679712779855\n",
      "Epoch 844 - Objective function: 18.253291030903252 - Loss: 18.18908871164544 - Gradient norm: 2.6592103402326983\n",
      "Epoch 845 - Objective function: 18.246140581270218 - Loss: 18.18191656540097 - Gradient norm: 2.655745621843205\n",
      "Epoch 846 - Objective function: 18.23900875262896 - Loss: 18.174763058299963 - Gradient norm: 2.6522855635289964\n",
      "Epoch 847 - Objective function: 18.231895496004743 - Loss: 18.167628141405245 - Gradient norm: 2.6488301710900446\n",
      "Epoch 848 - Objective function: 18.224800762485703 - Loss: 18.160511765842145 - Gradient norm: 2.645379450209606\n",
      "Epoch 849 - Objective function: 18.217724503223604 - Loss: 18.153413882799217 - Gradient norm: 2.641933406455147\n",
      "Epoch 850 - Objective function: 18.210666669434477 - Loss: 18.146334443528907 - Gradient norm: 2.6384920452792575\n",
      "Epoch 851 - Objective function: 18.203627212399304 - Loss: 18.139273399348227 - Gradient norm: 2.6350553720205725\n",
      "Epoch 852 - Objective function: 18.196606083464676 - Loss: 18.13223070163942 - Gradient norm: 2.6316233919046796\n",
      "Epoch 853 - Objective function: 18.189603234043442 - Loss: 18.1252063018506 - Gradient norm: 2.628196110045025\n",
      "Epoch 854 - Objective function: 18.182618615615365 - Loss: 18.118200151496435 - Gradient norm: 2.6247735314438128\n",
      "Epoch 855 - Objective function: 18.175652179727724 - Loss: 18.11121220215872 - Gradient norm: 2.621355660992901\n",
      "Epoch 856 - Objective function: 18.168703877995963 - Loss: 18.10424240548706 - Gradient norm: 2.617942503474694\n",
      "Epoch 857 - Objective function: 18.16177366210432 - Loss: 18.097290713199463 - Gradient norm: 2.6145340635630285\n",
      "Epoch 858 - Objective function: 18.154861483806386 - Loss: 18.09035707708294 - Gradient norm: 2.611130345824051\n",
      "Epoch 859 - Objective function: 18.147967294925728 - Loss: 18.0834414489941 - Gradient norm: 2.6077313547170977\n",
      "Epoch 860 - Objective function: 18.141091047356497 - Loss: 18.076543780859776 - Gradient norm: 2.604337094595568\n",
      "Epoch 861 - Objective function: 18.134232693063936 - Loss: 18.069664024677543 - Gradient norm: 2.6009475697077837\n",
      "Epoch 862 - Objective function: 18.127392184085046 - Loss: 18.06280213251635 - Gradient norm: 2.597562784197859\n",
      "Epoch 863 - Objective function: 18.12056947252906 - Loss: 18.05595805651704 - Gradient norm: 2.5941827421065513\n",
      "Epoch 864 - Objective function: 18.113764510578054 - Loss: 18.049131748892933 - Gradient norm: 2.5908074473721174\n",
      "Epoch 865 - Objective function: 18.106977250487443 - Loss: 18.042323161930337 - Gradient norm: 2.5874369038311578\n",
      "Epoch 866 - Objective function: 18.10020764458655 - Loss: 18.035532247989117 - Gradient norm: 2.5840711152194618\n",
      "Epoch 867 - Objective function: 18.093455645279132 - Loss: 18.028758959503214 - Gradient norm: 2.580710085172844\n",
      "Epoch 868 - Objective function: 18.08672120504386 - Loss: 18.022003248981147 - Gradient norm: 2.5773538172279755\n",
      "Epoch 869 - Objective function: 18.080004276434874 - Loss: 18.01526506900655 - Gradient norm: 2.5740023148232143\n",
      "Epoch 870 - Objective function: 18.073304812082252 - Loss: 18.008544372238646 - Gradient norm: 2.570655581299427\n",
      "Epoch 871 - Objective function: 18.06662276469252 - Loss: 18.00184111141278 - Gradient norm: 2.5673136199008084\n",
      "Epoch 872 - Objective function: 18.059958087049136 - Loss: 17.995155239340864 - Gradient norm: 2.563976433775694\n",
      "Epoch 873 - Objective function: 18.053310732012946 - Loss: 17.988486708911886 - Gradient norm: 2.5606440259773695\n",
      "Epoch 874 - Objective function: 18.046680652522685 - Loss: 17.981835473092357 - Gradient norm: 2.5573163994648738\n",
      "Epoch 875 - Objective function: 18.04006780159542 - Loss: 17.975201484926806 - Gradient norm: 2.553993557103798\n",
      "Epoch 876 - Objective function: 18.033472132326985 - Loss: 17.9685846975382 - Gradient norm: 2.550675501667082\n",
      "Epoch 877 - Objective function: 18.02689359789247 - Loss: 17.961985064128413 - Gradient norm: 2.5473622358358026\n",
      "Epoch 878 - Objective function: 18.02033215154662 - Loss: 17.95540253797865 - Gradient norm: 2.544053762199958\n",
      "Epoch 879 - Objective function: 18.01378774662428 - Loss: 17.94883707244989 - Gradient norm: 2.5407500832592484\n",
      "Epoch 880 - Objective function: 18.00726033654082 - Loss: 17.94228862098332 - Gradient norm: 2.537451201423852\n",
      "Epoch 881 - Objective function: 18.000749874792554 - Loss: 17.935757137100726 - Gradient norm: 2.534157119015194\n",
      "Epoch 882 - Objective function: 17.994256314957116 - Loss: 17.929242574404917 - Gradient norm: 2.5308678382667154\n",
      "Epoch 883 - Objective function: 17.987779610693906 - Loss: 17.922744886580134 - Gradient norm: 2.52758336132463\n",
      "Epoch 884 - Objective function: 17.981319715744455 - Loss: 17.916264027392426 - Gradient norm: 2.5243036902486873\n",
      "Epoch 885 - Objective function: 17.974876583932822 - Loss: 17.909799950690058 - Gradient norm: 2.521028827012915\n",
      "Epoch 886 - Objective function: 17.96845016916595 - Loss: 17.90335261040388 - Gradient norm: 2.5177587735063764\n",
      "Epoch 887 - Objective function: 17.962040425434076 - Loss: 17.89692196054769 - Gradient norm: 2.5144935315339056\n",
      "Epoch 888 - Objective function: 17.955647306811063 - Loss: 17.890507955218638 - Gradient norm: 2.5112331028168486\n",
      "Epoch 889 - Objective function: 17.949270767454777 - Loss: 17.884110548597533 - Gradient norm: 2.5079774889937942\n",
      "Epoch 890 - Objective function: 17.94291076160742 - Loss: 17.87772969494924 - Gradient norm: 2.5047266916213062\n",
      "Epoch 891 - Objective function: 17.9365672435959 - Loss: 17.871365348623012 - Gradient norm: 2.501480712174643\n",
      "Epoch 892 - Objective function: 17.930240167832135 - Loss: 17.8650174640528 - Gradient norm: 2.4982395520484797\n",
      "Epoch 893 - Objective function: 17.923929488813414 - Loss: 17.85868599575765 - Gradient norm: 2.4950032125576227\n",
      "Epoch 894 - Objective function: 17.91763516112272 - Loss: 17.852370898341963 - Gradient norm: 2.491771694937717\n",
      "Epoch 895 - Objective function: 17.911357139429004 - Loss: 17.84607212649584 - Gradient norm: 2.488545000345956\n",
      "Epoch 896 - Objective function: 17.905095378487562 - Loss: 17.83978963499542 - Gradient norm: 2.4853231298617784\n",
      "Epoch 897 - Objective function: 17.89884983314028 - Loss: 17.83352337870314 - Gradient norm: 2.4821060844875675\n",
      "Epoch 898 - Objective function: 17.892620458315978 - Loss: 17.82727331256807 - Gradient norm: 2.4788938651493426\n",
      "Epoch 899 - Objective function: 17.88640720903068 - Loss: 17.821039391626194 - Gradient norm: 2.475686472697443\n",
      "Epoch 900 - Objective function: 17.880210040387883 - Loss: 17.814821571000692 - Gradient norm: 2.472483907907218\n",
      "Epoch 901 - Objective function: 17.87402890757889 - Loss: 17.80861980590224 - Gradient norm: 2.469286171479695\n",
      "Epoch 902 - Objective function: 17.867863765883023 - Loss: 17.80243405162926 - Gradient norm: 2.466093264042265\n",
      "Epoch 903 - Objective function: 17.861714570667925 - Loss: 17.79626426356821 - Gradient norm: 2.4629051861493423\n",
      "Epoch 904 - Objective function: 17.855581277389824 - Loss: 17.790110397193846 - Gradient norm: 2.4597219382830353\n",
      "Epoch 905 - Objective function: 17.849463841593767 - Loss: 17.78397240806946 - Gradient norm: 2.456543520853804\n",
      "Epoch 906 - Objective function: 17.843362218913885 - Loss: 17.777850251847152 - Gradient norm: 2.4533699342011177\n",
      "Epoch 907 - Objective function: 17.83727636507364 - Loss: 17.771743884268073 - Gradient norm: 2.4502011785941065\n",
      "Epoch 908 - Objective function: 17.831206235886054 - Loss: 17.765653261162658 - Gradient norm: 2.4470372542322067\n",
      "Epoch 909 - Objective function: 17.825151787253937 - Loss: 17.759578338450854 - Gradient norm: 2.443878161245807\n",
      "Epoch 910 - Objective function: 17.81911297517014 - Loss: 17.753519072142375 - Gradient norm: 2.4407238996968856\n",
      "Epoch 911 - Objective function: 17.813089755717744 - Loss: 17.747475418336894 - Gradient norm: 2.4375744695796415\n",
      "Epoch 912 - Objective function: 17.807082085070288 - Loss: 17.741447333224276 - Gradient norm: 2.4344298708211314\n",
      "Epoch 913 - Objective function: 17.801089919492 - Loss: 17.735434773084798 - Gradient norm: 2.4312901032818877\n",
      "Epoch 914 - Objective function: 17.795113215337967 - Loss: 17.72943769428934 - Gradient norm: 2.4281551667565444\n",
      "Epoch 915 - Objective function: 17.789151929054363 - Loss: 17.72345605329959 - Gradient norm: 2.4250250609744524\n",
      "Epoch 916 - Objective function: 17.783206017178625 - Loss: 17.71748980666825 - Gradient norm: 2.421899785600294\n",
      "Epoch 917 - Objective function: 17.777275436339647 - Loss: 17.71153891103921 - Gradient norm: 2.4187793402346904\n",
      "Epoch 918 - Objective function: 17.77136014325799 - Loss: 17.705603323147763 - Gradient norm: 2.4156637244148036\n",
      "Epoch 919 - Objective function: 17.765460094746008 - Loss: 17.699682999820745 - Gradient norm: 2.4125529376149406\n",
      "Epoch 920 - Objective function: 17.75957524770807 - Loss: 17.693777897976737 - Gradient norm: 2.409446979247149\n",
      "Epoch 921 - Objective function: 17.753705559140705 - Loss: 17.687887974626243 - Gradient norm: 2.4063458486618043\n",
      "Epoch 922 - Objective function: 17.74785098613278 - Loss: 17.68201318687183 - Gradient norm: 2.4032495451482023\n",
      "Epoch 923 - Objective function: 17.742011485865632 - Loss: 17.676153491908305 - Gradient norm: 2.4001580679351404\n",
      "Epoch 924 - Objective function: 17.73618701561326 - Loss: 17.670308847022866 - Gradient norm: 2.397071416191499\n",
      "Epoch 925 - Objective function: 17.730377532742455 - Loss: 17.66447920959526 - Gradient norm: 2.393989589026811\n",
      "Epoch 926 - Objective function: 17.724582994712936 - Loss: 17.658664537097913 - Gradient norm: 2.3909125854918423\n",
      "Epoch 927 - Objective function: 17.718803359077494 - Loss: 17.652864787096092 - Gradient norm: 2.38784040457915\n",
      "Epoch 928 - Objective function: 17.713038583482152 - Loss: 17.64707991724803 - Gradient norm: 2.3847730452236497\n",
      "Epoch 929 - Objective function: 17.707288625666273 - Loss: 17.641309885305056 - Gradient norm: 2.3817105063031754\n",
      "Epoch 930 - Objective function: 17.70155344346266 - Loss: 17.635554649111725 - Gradient norm: 2.378652786639033\n",
      "Epoch 931 - Objective function: 17.69583299479775 - Loss: 17.62981416660595 - Gradient norm: 2.3755998849965505\n",
      "Epoch 932 - Objective function: 17.690127237691662 - Loss: 17.624088395819115 - Gradient norm: 2.3725518000856254\n",
      "Epoch 933 - Objective function: 17.684436130258337 - Loss: 17.61837729487618 - Gradient norm: 2.3695085305612698\n",
      "Epoch 934 - Objective function: 17.67875963070566 - Loss: 17.6126808219958 - Gradient norm: 2.3664700750241474\n",
      "Epoch 935 - Objective function: 17.673097697335542 - Loss: 17.606998935490434 - Gradient norm: 2.3634364320211096\n",
      "Epoch 936 - Objective function: 17.66745028854402 - Loss: 17.601331593766446 - Gradient norm: 2.3604076000457272\n",
      "Epoch 937 - Objective function: 17.661817362821377 - Loss: 17.595678755324187 - Gradient norm: 2.3573835775388168\n",
      "Epoch 938 - Objective function: 17.656198878752193 - Loss: 17.5900403787581 - Gradient norm: 2.354364362888965\n",
      "Epoch 939 - Objective function: 17.65059479501549 - Loss: 17.584416422756824 - Gradient norm: 2.351349954433048\n",
      "Epoch 940 - Objective function: 17.64500507038476 - Loss: 17.578806846103248 - Gradient norm: 2.348340350456748\n",
      "Epoch 941 - Objective function: 17.639429663728063 - Loss: 17.573211607674608 - Gradient norm: 2.3453355491950654\n",
      "Epoch 942 - Objective function: 17.633868534008112 - Loss: 17.567630666442565 - Gradient norm: 2.3423355488328252\n",
      "Epoch 943 - Objective function: 17.628321640282348 - Loss: 17.56206398147328 - Gradient norm: 2.339340347505184\n",
      "Epoch 944 - Objective function: 17.62278894170299 - Loss: 17.55651151192747 - Gradient norm: 2.3363499432981305\n",
      "Epoch 945 - Objective function: 17.6172703975171 - Loss: 17.550973217060495 - Gradient norm: 2.333364334248981\n",
      "Epoch 946 - Objective function: 17.61176596706666 - Loss: 17.545449056222388 - Gradient norm: 2.3303835183468733\n",
      "Epoch 947 - Objective function: 17.606275609788604 - Loss: 17.539938988857937 - Gradient norm: 2.3274074935332583\n",
      "Epoch 948 - Objective function: 17.600799285214897 - Loss: 17.534442974506746 - Gradient norm: 2.3244362577023825\n",
      "Epoch 949 - Objective function: 17.595336952972577 - Loss: 17.52896097280325 - Gradient norm: 2.321469808701774\n",
      "Epoch 950 - Objective function: 17.58988857278377 - Loss: 17.523492943476796 - Gradient norm: 2.3185081443327196\n",
      "Epoch 951 - Objective function: 17.584454104465777 - Loss: 17.518038846351672 - Gradient norm: 2.315551262350739\n",
      "Epoch 952 - Objective function: 17.57903350793108 - Loss: 17.512598641347143 - Gradient norm: 2.3125991604660565\n",
      "Epoch 953 - Objective function: 17.573626743187386 - Loss: 17.507172288477495 - Gradient norm: 2.3096518363440715\n",
      "Epoch 954 - Objective function: 17.56823377033767 - Loss: 17.501759747852063 - Gradient norm: 2.3067092876058184\n",
      "Epoch 955 - Objective function: 17.562854549580173 - Loss: 17.49636097967525 - Gradient norm: 2.3037715118284328\n",
      "Epoch 956 - Objective function: 17.55748904120847 - Loss: 17.490975944246586 - Gradient norm: 2.300838506545604\n",
      "Epoch 957 - Objective function: 17.55213720561145 - Loss: 17.48560460196071 - Gradient norm: 2.2979102692480304\n",
      "Epoch 958 - Objective function: 17.546799003273353 - Loss: 17.480246913307408 - Gradient norm: 2.294986797383873\n",
      "Epoch 959 - Objective function: 17.54147439477377 - Loss: 17.47490283887163 - Gradient norm: 2.2920680883591955\n",
      "Epoch 960 - Objective function: 17.536163340787688 - Loss: 17.469572339333503 - Gradient norm: 2.2891541395384145\n",
      "Epoch 961 - Objective function: 17.530865802085454 - Loss: 17.464255375468326 - Gradient norm: 2.286244948244735\n",
      "Epoch 962 - Objective function: 17.525581739532818 - Loss: 17.458951908146602 - Gradient norm: 2.283340511760589\n",
      "Epoch 963 - Objective function: 17.520311114090898 - Loss: 17.453661898334015 - Gradient norm: 2.2804408273280696\n",
      "Epoch 964 - Objective function: 17.515053886816215 - Loss: 17.448385307091442 - Gradient norm: 2.2775458921493565\n",
      "Epoch 965 - Objective function: 17.509810018860673 - Loss: 17.443122095574957 - Gradient norm: 2.274655703387149\n",
      "Epoch 966 - Objective function: 17.504579471471533 - Loss: 17.4378722250358 - Gradient norm: 2.2717702581650827\n",
      "Epoch 967 - Objective function: 17.499362205991435 - Loss: 17.432635656820406 - Gradient norm: 2.2688895535681546\n",
      "Epoch 968 - Objective function: 17.494158183858364 - Loss: 17.42741235237035 - Gradient norm: 2.266013586643135\n",
      "Epoch 969 - Objective function: 17.488967366605653 - Loss: 17.422202273222375 - Gradient norm: 2.2631423543989833\n",
      "Epoch 970 - Objective function: 17.48378971586192 - Loss: 17.41700538100833 - Gradient norm: 2.26027585380726\n",
      "Epoch 971 - Objective function: 17.478625193351114 - Loss: 17.411821637455198 - Gradient norm: 2.257414081802529\n",
      "Epoch 972 - Objective function: 17.473473760892425 - Loss: 17.406651004385022 - Gradient norm: 2.254557035282765\n",
      "Epoch 973 - Objective function: 17.468335380400305 - Loss: 17.401493443714926 - Gradient norm: 2.2517047111097503\n",
      "Epoch 974 - Objective function: 17.463210013884407 - Loss: 17.39634891745705 - Gradient norm: 2.2488571061094755\n",
      "Epoch 975 - Objective function: 17.45809762344956 - Loss: 17.391217387718537 - Gradient norm: 2.2460142170725317\n",
      "Epoch 976 - Objective function: 17.452998171295746 - Loss: 17.386098816701494 - Gradient norm: 2.243176040754501\n",
      "Epoch 977 - Objective function: 17.44791161971806 - Loss: 17.38099316670296 - Gradient norm: 2.2403425738763465\n",
      "Epoch 978 - Objective function: 17.44283793110664 - Loss: 17.37590040011486 - Gradient norm: 2.237513813124793\n",
      "Epoch 979 - Objective function: 17.437777067946666 - Loss: 17.37082047942396 - Gradient norm: 2.234689755152714\n",
      "Epoch 980 - Objective function: 17.432728992818284 - Loss: 17.365753367211834 - Gradient norm: 2.231870396579504\n",
      "Epoch 981 - Objective function: 17.427693668396586 - Loss: 17.360699026154816 - Gradient norm: 2.22905573399146\n",
      "Epoch 982 - Objective function: 17.422671057451517 - Loss: 17.355657419023938 - Gradient norm: 2.2262457639421487\n",
      "Epoch 983 - Objective function: 17.41766112284787 - Loss: 17.350628508684892 - Gradient norm: 2.2234404829527805\n",
      "Epoch 984 - Objective function: 17.41266382754519 - Loss: 17.34561225809797 - Gradient norm: 2.2206398875125726\n",
      "Epoch 985 - Objective function: 17.407679134597764 - Loss: 17.340608630318002 - Gradient norm: 2.2178439740791136\n",
      "Epoch 986 - Objective function: 17.402707007154493 - Loss: 17.335617588494312 - Gradient norm: 2.215052739078725\n",
      "Epoch 987 - Objective function: 17.3977474084589 - Loss: 17.330639095870648 - Gradient norm: 2.212266178906817\n",
      "Epoch 988 - Objective function: 17.392800301849014 - Loss: 17.3256731157851 - Gradient norm: 2.209484289928245\n",
      "Epoch 989 - Objective function: 17.387865650757345 - Loss: 17.320719611670086 - Gradient norm: 2.2067070684776593\n",
      "Epoch 990 - Objective function: 17.382943418710767 - Loss: 17.315778547052222 - Gradient norm: 2.203934510859855\n",
      "Epoch 991 - Objective function: 17.37803356933051 - Loss: 17.310849885552305 - Gradient norm: 2.2011666133501193\n",
      "Epoch 992 - Objective function: 17.373136066332005 - Loss: 17.305933590885196 - Gradient norm: 2.198403372194573\n",
      "Epoch 993 - Objective function: 17.368250873524893 - Loss: 17.301029626859787 - Gradient norm: 2.1956447836105104\n",
      "Epoch 994 - Objective function: 17.363377954812893 - Loss: 17.29613795737889 - Gradient norm: 2.19289084378674\n",
      "Epoch 995 - Objective function: 17.358517274193733 - Loss: 17.29125854643918 - Gradient norm: 2.1901415488839144\n",
      "Epoch 996 - Objective function: 17.35366879575908 - Loss: 17.286391358131112 - Gradient norm: 2.187396895034867\n",
      "Epoch 997 - Objective function: 17.348832483694466 - Loss: 17.28153635663883 - Gradient norm: 2.1846568783449376\n",
      "Epoch 998 - Objective function: 17.344008302279168 - Loss: 17.276693506240097 - Gradient norm: 2.1819214948923022\n",
      "Epoch 999 - Objective function: 17.339196215886133 - Loss: 17.271862771306182 - Gradient norm: 2.179190740728294\n",
      "Epoch 1000 - Objective function: 17.334396188981913 - Loss: 17.267044116301804 - Gradient norm: 2.1764646118777247\n",
      "Epoch 1001 - Objective function: 17.329608186126563 - Loss: 17.262237505785027 - Gradient norm: 2.173743104339205\n",
      "Epoch 1002 - Objective function: 17.324832171973522 - Loss: 17.257442904407167 - Gradient norm: 2.1710262140854586\n",
      "Epoch 1003 - Objective function: 17.320068111269546 - Loss: 17.25266027691269 - Gradient norm: 2.168313937063636\n",
      "Epoch 1004 - Objective function: 17.31531596885461 - Loss: 17.247889588139145 - Gradient norm: 2.165606269195623\n",
      "Epoch 1005 - Objective function: 17.31057570966179 - Loss: 17.243130803017028 - Gradient norm: 2.1629032063783535\n",
      "Epoch 1006 - Objective function: 17.305847298717183 - Loss: 17.23838388656972 - Gradient norm: 2.160204744484108\n",
      "Epoch 1007 - Objective function: 17.301130701139805 - Loss: 17.23364880391336 - Gradient norm: 2.1575108793608213\n",
      "Epoch 1008 - Objective function: 17.296425882141463 - Loss: 17.228925520256748 - Gradient norm: 2.1548216068323804\n",
      "Epoch 1009 - Objective function: 17.291732807026683 - Loss: 17.224214000901252 - Gradient norm: 2.1521369226989226\n",
      "Epoch 1010 - Objective function: 17.287051441192578 - Loss: 17.219514211240693 - Gradient norm: 2.1494568227371325\n",
      "Epoch 1011 - Objective function: 17.282381750128764 - Loss: 17.21482611676124 - Gradient norm: 2.1467813027005294\n",
      "Epoch 1012 - Objective function: 17.277723699417205 - Loss: 17.21014968304129 - Gradient norm: 2.1441103583197627\n",
      "Epoch 1013 - Objective function: 17.273077254732165 - Loss: 17.20548487575138 - Gradient norm: 2.141443985302896\n",
      "Epoch 1014 - Objective function: 17.268442381840035 - Loss: 17.200831660654057 - Gradient norm: 2.1387821793356934\n",
      "Epoch 1015 - Objective function: 17.263819046599256 - Loss: 17.196190003603757 - Gradient norm: 2.1361249360819037\n",
      "Epoch 1016 - Objective function: 17.259207214960192 - Loss: 17.19155987054673 - Gradient norm: 2.133472251183538\n",
      "Epoch 1017 - Objective function: 17.254606852964997 - Loss: 17.18694122752086 - Gradient norm: 2.130824120261148\n",
      "Epoch 1018 - Objective function: 17.250017926747525 - Loss: 17.182334040655608 - Gradient norm: 2.128180538914103\n",
      "Epoch 1019 - Objective function: 17.245440402533195 - Loss: 17.17773827617186 - Gradient norm: 2.125541502720861\n",
      "Epoch 1020 - Objective function: 17.24087424663885 - Loss: 17.173153900381795 - Gradient norm: 2.122907007239241\n",
      "Epoch 1021 - Objective function: 17.236319425472672 - Loss: 17.168580879688808 - Gradient norm: 2.120277048006691\n",
      "Epoch 1022 - Objective function: 17.231775905534025 - Loss: 17.164019180587335 - Gradient norm: 2.117651620540553\n",
      "Epoch 1023 - Objective function: 17.227243653413332 - Loss: 17.15946876966275 - Gradient norm: 2.1150307203383267\n",
      "Epoch 1024 - Objective function: 17.22272263579198 - Loss: 17.154929613591257 - Gradient norm: 2.112414342877934\n",
      "Epoch 1025 - Objective function: 17.21821281944214 - Loss: 17.150401679139723 - Gradient norm: 2.109802483617972\n",
      "Epoch 1026 - Objective function: 17.213714171226687 - Loss: 17.14588493316558 - Gradient norm: 2.107195137997978\n",
      "Epoch 1027 - Objective function: 17.209226658099023 - Loss: 17.141379342616677 - Gradient norm: 2.1045923014386756\n",
      "Epoch 1028 - Objective function: 17.204750247102986 - Loss: 17.136884874531162 - Gradient norm: 2.101993969342234\n",
      "Epoch 1029 - Objective function: 17.200284905372683 - Loss: 17.132401496037335 - Gradient norm: 2.099400137092513\n",
      "Epoch 1030 - Objective function: 17.19583060013238 - Loss: 17.12792917435353 - Gradient norm: 2.096810800055316\n",
      "Epoch 1031 - Objective function: 17.19138729869633 - Loss: 17.123467876787952 - Gradient norm: 2.0942259535786305\n",
      "Epoch 1032 - Objective function: 17.186954968468697 - Loss: 17.119017570738585 - Gradient norm: 2.091645592992877\n",
      "Epoch 1033 - Objective function: 17.182533576943356 - Loss: 17.114578223693005 - Gradient norm: 2.0890697136111434\n",
      "Epoch 1034 - Objective function: 17.178123091703775 - Loss: 17.11014980322827 - Gradient norm: 2.086498310729435\n",
      "Epoch 1035 - Objective function: 17.173723480422886 - Loss: 17.105732277010784 - Gradient norm: 2.0839313796268994\n",
      "Epoch 1036 - Objective function: 17.16933471086293 - Loss: 17.101325612796135 - Gradient norm: 2.0813689155660713\n",
      "Epoch 1037 - Objective function: 17.164956750875323 - Loss: 17.096929778428965 - Gradient norm: 2.078810913793102\n",
      "Epoch 1038 - Objective function: 17.160589568400493 - Loss: 17.09254474184283 - Gradient norm: 2.076257369537986\n",
      "Epoch 1039 - Objective function: 17.15623313146776 - Loss: 17.08817047106005 - Gradient norm: 2.073708278014799\n",
      "Epoch 1040 - Objective function: 17.15188740819518 - Loss: 17.083806934191557 - Gradient norm: 2.0711636344219175\n",
      "Epoch 1041 - Objective function: 17.147552366789384 - Loss: 17.079454099436777 - Gradient norm: 2.068623433942246\n",
      "Epoch 1042 - Objective function: 17.143227975545454 - Loss: 17.075111935083445 - Gradient norm: 2.06608767174344\n",
      "Epoch 1043 - Objective function: 17.138914202846752 - Loss: 17.07078040950747 - Gradient norm: 2.063556342978125\n",
      "Epoch 1044 - Objective function: 17.13461101716479 - Loss: 17.06645949117281 - Gradient norm: 2.0610294427841196\n",
      "Epoch 1045 - Objective function: 17.13031838705906 - Loss: 17.06214914863129 - Gradient norm: 2.058506966284647\n",
      "Epoch 1046 - Objective function: 17.12603628117689 - Loss: 17.057849350522456 - Gradient norm: 2.0559889085885525\n",
      "Epoch 1047 - Objective function: 17.121764668253313 - Loss: 17.05356006557345 - Gradient norm: 2.053475264790518\n",
      "Epoch 1048 - Objective function: 17.117503517110862 - Loss: 17.049281262598818 - Gradient norm: 2.050966029971269\n",
      "Epoch 1049 - Objective function: 17.113252796659474 - Loss: 17.04501291050039 - Gradient norm: 2.048461199197787\n",
      "Epoch 1050 - Objective function: 17.109012475896282 - Loss: 17.0407549782671 - Gradient norm: 2.045960767523515\n",
      "Epoch 1051 - Objective function: 17.104782523905506 - Loss: 17.036507434974848 - Gradient norm: 2.0434647299885644\n",
      "Epoch 1052 - Objective function: 17.100562909858255 - Loss: 17.032270249786333 - Gradient norm: 2.040973081619916\n",
      "Epoch 1053 - Objective function: 17.096353603012403 - Loss: 17.02804339195091 - Gradient norm: 2.0384858174316243\n",
      "Epoch 1054 - Objective function: 17.09215457271241 - Loss: 17.02382683080441 - Gradient norm: 2.0360029324250144\n",
      "Epoch 1055 - Objective function: 17.08796578838917 - Loss: 17.019620535769008 - Gradient norm: 2.0335244215888824\n",
      "Epoch 1056 - Objective function: 17.083787219559845 - Loss: 17.015424476353044 - Gradient norm: 2.0310502798996897\n",
      "Epoch 1057 - Objective function: 17.079618835827716 - Loss: 17.011238622150866 - Gradient norm: 2.028580502321758\n",
      "Epoch 1058 - Objective function: 17.075460606881993 - Loss: 17.00706294284267 - Gradient norm: 2.0261150838074613\n",
      "Epoch 1059 - Objective function: 17.0713125024977 - Loss: 17.00289740819435 - Gradient norm: 2.0236540192974184\n",
      "Epoch 1060 - Objective function: 17.067174492535482 - Loss: 16.998741988057326 - Gradient norm: 2.0211973037206787\n",
      "Epoch 1061 - Objective function: 17.063046546941425 - Loss: 16.99459665236837 - Gradient norm: 2.0187449319949127\n",
      "Epoch 1062 - Objective function: 17.05892863574693 - Loss: 16.99046137114947 - Gradient norm: 2.016296899026594\n",
      "Epoch 1063 - Objective function: 17.05482072906853 - Loss: 16.98633611450764 - Gradient norm: 2.013853199711186\n",
      "Epoch 1064 - Objective function: 17.050722797107717 - Loss: 16.98222085263477 - Gradient norm: 2.011413828933324\n",
      "Epoch 1065 - Objective function: 17.04663481015078 - Loss: 16.978115555807456 - Gradient norm: 2.008978781566991\n",
      "Epoch 1066 - Objective function: 17.042556738568656 - Loss: 16.974020194386828 - Gradient norm: 2.006548052475703\n",
      "Epoch 1067 - Objective function: 17.038488552816734 - Loss: 16.9699347388184 - Gradient norm: 2.00412163651268\n",
      "Epoch 1068 - Objective function: 17.034430223434715 - Loss: 16.965859159631883 - Gradient norm: 2.0016995285210264\n",
      "Epoch 1069 - Objective function: 17.03038172104641 - Loss: 16.961793427441027 - Gradient norm: 1.9992817233338998\n",
      "Epoch 1070 - Objective function: 17.02634301635961 - Loss: 16.957737512943464 - Gradient norm: 1.9968682157746873\n",
      "Epoch 1071 - Objective function: 17.0223140801659 - Loss: 16.953691386920514 - Gradient norm: 1.994459000657172\n",
      "Epoch 1072 - Objective function: 17.01829488334045 - Loss: 16.949655020237024 - Gradient norm: 1.9920540727857066\n",
      "Epoch 1073 - Objective function: 17.014285396841935 - Loss: 16.94562838384123 - Gradient norm: 1.9896534269553745\n",
      "Epoch 1074 - Objective function: 17.010285591712258 - Loss: 16.941611448764522 - Gradient norm: 1.9872570579521618\n",
      "Epoch 1075 - Objective function: 17.006295439076478 - Loss: 16.93760418612135 - Gradient norm: 1.984864960553116\n",
      "Epoch 1076 - Objective function: 17.002314910142537 - Loss: 16.93360656710898 - Gradient norm: 1.982477129526513\n",
      "Epoch 1077 - Objective function: 16.99834397620118 - Loss: 16.929618563007374 - Gradient norm: 1.9800935596320133\n",
      "Epoch 1078 - Objective function: 16.994382608625735 - Loss: 16.925640145178992 - Gradient norm: 1.977714245620826\n",
      "Epoch 1079 - Objective function: 16.990430778871932 - Loss: 16.921671285068626 - Gradient norm: 1.9753391822358648\n",
      "Epoch 1080 - Objective function: 16.986488458477755 - Loss: 16.917711954203234 - Gradient norm: 1.972968364211903\n",
      "Epoch 1081 - Objective function: 16.982555619063252 - Loss: 16.913762124191752 - Gradient norm: 1.9706017862757297\n",
      "Epoch 1082 - Objective function: 16.978632232330376 - Loss: 16.90982176672493 - Gradient norm: 1.9682394431463053\n",
      "Epoch 1083 - Objective function: 16.974718270062777 - Loss: 16.90589085357515 - Gradient norm: 1.9658813295349085\n",
      "Epoch 1084 - Objective function: 16.970813704125668 - Loss: 16.90196935659627 - Gradient norm: 1.963527440145292\n",
      "Epoch 1085 - Objective function: 16.96691850646563 - Loss: 16.898057247723422 - Gradient norm: 1.9611777696738282\n",
      "Epoch 1086 - Objective function: 16.963032649110403 - Loss: 16.894154498972846 - Gradient norm: 1.958832312809657\n",
      "Epoch 1087 - Objective function: 16.959156104168787 - Loss: 16.890261082441732 - Gradient norm: 1.9564910642348337\n",
      "Epoch 1088 - Objective function: 16.955288843830385 - Loss: 16.886376970308017 - Gradient norm: 1.9541540186244724\n",
      "Epoch 1089 - Objective function: 16.95143084036548 - Loss: 16.88250213483023 - Gradient norm: 1.951821170646891\n",
      "Epoch 1090 - Objective function: 16.947582066124824 - Loss: 16.878636548347295 - Gradient norm: 1.9494925149637492\n",
      "Epoch 1091 - Objective function: 16.943742493539492 - Loss: 16.87478018327837 - Gradient norm: 1.947168046230194\n",
      "Epoch 1092 - Objective function: 16.93991209512067 - Loss: 16.87093301212267 - Gradient norm: 1.9448477590949949\n",
      "Epoch 1093 - Objective function: 16.936090843459485 - Loss: 16.86709500745927 - Gradient norm: 1.942531648200686\n",
      "Epoch 1094 - Objective function: 16.932278711226857 - Loss: 16.863266141946944 - Gradient norm: 1.9402197081836972\n",
      "Epoch 1095 - Objective function: 16.92847567117328 - Loss: 16.859446388323992 - Gradient norm: 1.9379119336744932\n",
      "Epoch 1096 - Objective function: 16.924681696128665 - Loss: 16.855635719408042 - Gradient norm: 1.9356083192977072\n",
      "Epoch 1097 - Objective function: 16.920896759002154 - Loss: 16.85183410809588 - Gradient norm: 1.9333088596722703\n",
      "Epoch 1098 - Objective function: 16.917120832781933 - Loss: 16.84804152736327 - Gradient norm: 1.9310135494115463\n",
      "Epoch 1099 - Objective function: 16.91335389053508 - Loss: 16.8442579502648 - Gradient norm: 1.9287223831234608\n",
      "Epoch 1100 - Objective function: 16.909595905407343 - Loss: 16.840483349933645 - Gradient norm: 1.9264353554106268\n",
      "Epoch 1101 - Objective function: 16.905846850622986 - Loss: 16.836717699581435 - Gradient norm: 1.9241524608704779\n",
      "Epoch 1102 - Objective function: 16.90210669948461 - Loss: 16.832960972498064 - Gradient norm: 1.9218736940953869\n",
      "Epoch 1103 - Objective function: 16.898375425372972 - Loss: 16.82921314205151 - Gradient norm: 1.9195990496727973\n",
      "Epoch 1104 - Objective function: 16.89465300174678 - Loss: 16.82547418168764 - Gradient norm: 1.9173285221853436\n",
      "Epoch 1105 - Objective function: 16.890939402142532 - Loss: 16.82174406493004 - Gradient norm: 1.915062106210974\n",
      "Epoch 1106 - Objective function: 16.88723460017436 - Loss: 16.818022765379855 - Gradient norm: 1.9127997963230727\n",
      "Epoch 1107 - Objective function: 16.88353856953378 - Loss: 16.81431025671556 - Gradient norm: 1.9105415870905773\n",
      "Epoch 1108 - Objective function: 16.879851283989577 - Loss: 16.810606512692825 - Gradient norm: 1.908287473078101\n",
      "Epoch 1109 - Objective function: 16.876172717387583 - Loss: 16.806911507144306 - Gradient norm: 1.9060374488460468\n",
      "Epoch 1110 - Objective function: 16.872502843650533 - Loss: 16.803225213979484 - Gradient norm: 1.9037915089507265\n",
      "Epoch 1111 - Objective function: 16.868841636777834 - Loss: 16.79954760718446 - Gradient norm: 1.9015496479444736\n",
      "Epoch 1112 - Objective function: 16.86518907084541 - Loss: 16.7958786608218 - Gradient norm: 1.8993118603757597\n",
      "Epoch 1113 - Objective function: 16.861545120005527 - Loss: 16.792218349030318 - Gradient norm: 1.8970781407893051\n",
      "Epoch 1114 - Objective function: 16.857909758486606 - Loss: 16.788566646024933 - Gradient norm: 1.8948484837261919\n",
      "Epoch 1115 - Objective function: 16.854282960593018 - Loss: 16.784923526096467 - Gradient norm: 1.8926228837239742\n",
      "Epoch 1116 - Objective function: 16.850664700704936 - Loss: 16.78128896361146 - Gradient norm: 1.8904013353167874\n",
      "Epoch 1117 - Objective function: 16.84705495327813 - Loss: 16.777662933011992 - Gradient norm: 1.8881838330354572\n",
      "Epoch 1118 - Objective function: 16.843453692843777 - Loss: 16.77404540881551 - Gradient norm: 1.885970371407605\n",
      "Epoch 1119 - Objective function: 16.83986089400831 - Loss: 16.77043636561463 - Gradient norm: 1.8837609449577568\n",
      "Epoch 1120 - Objective function: 16.836276531453215 - Loss: 16.766835778076963 - Gradient norm: 1.8815555482074464\n",
      "Epoch 1121 - Objective function: 16.832700579934816 - Loss: 16.76324362094493 - Gradient norm: 1.879354175675319\n",
      "Epoch 1122 - Objective function: 16.829133014284174 - Loss: 16.759659869035595 - Gradient norm: 1.8771568218772359\n",
      "Epoch 1123 - Objective function: 16.825573809406816 - Loss: 16.756084497240444 - Gradient norm: 1.8749634813263751\n",
      "Epoch 1124 - Objective function: 16.82202294028262 - Loss: 16.752517480525253 - Gradient norm: 1.8727741485333318\n",
      "Epoch 1125 - Objective function: 16.818480381965575 - Loss: 16.74895879392986 - Gradient norm: 1.8705888180062198\n",
      "Epoch 1126 - Objective function: 16.814946109583644 - Loss: 16.74540841256801 - Gradient norm: 1.8684074842507676\n",
      "Epoch 1127 - Objective function: 16.811420098338566 - Loss: 16.74186631162717 - Gradient norm: 1.8662301417704177\n",
      "Epoch 1128 - Objective function: 16.807902323505637 - Loss: 16.738332466368327 - Gradient norm: 1.8640567850664256\n",
      "Epoch 1129 - Objective function: 16.804392760433608 - Loss: 16.73480685212583 - Gradient norm: 1.8618874086379509\n",
      "Epoch 1130 - Objective function: 16.800891384544414 - Loss: 16.73128944430719 - Gradient norm: 1.8597220069821534\n",
      "Epoch 1131 - Objective function: 16.797398171333047 - Loss: 16.727780218392912 - Gradient norm: 1.8575605745942896\n",
      "Epoch 1132 - Objective function: 16.793913096367355 - Loss: 16.724279149936297 - Gradient norm: 1.8554031059678018\n",
      "Epoch 1133 - Objective function: 16.790436135287862 - Loss: 16.72078621456327 - Gradient norm: 1.8532495955944124\n",
      "Epoch 1134 - Objective function: 16.786967263807565 - Loss: 16.71730138797219 - Gradient norm: 1.8511000379642122\n",
      "Epoch 1135 - Objective function: 16.783506457711788 - Loss: 16.713824645933673 - Gradient norm: 1.8489544275657532\n",
      "Epoch 1136 - Objective function: 16.78005369285798 - Loss: 16.71035596429042 - Gradient norm: 1.8468127588861358\n",
      "Epoch 1137 - Objective function: 16.776608945175525 - Loss: 16.706895318957006 - Gradient norm: 1.8446750264110954\n",
      "Epoch 1138 - Objective function: 16.773172190665555 - Loss: 16.70344268591972 - Gradient norm: 1.8425412246250916\n",
      "Epoch 1139 - Objective function: 16.7697434054008 - Loss: 16.699998041236373 - Gradient norm: 1.8404113480113944\n",
      "Epoch 1140 - Objective function: 16.766322565525375 - Loss: 16.69656136103614 - Gradient norm: 1.838285391052166\n",
      "Epoch 1141 - Objective function: 16.762909647254606 - Loss: 16.69313262151933 - Gradient norm: 1.8361633482285509\n",
      "Epoch 1142 - Objective function: 16.759504626874843 - Loss: 16.689711798957248 - Gradient norm: 1.8340452140207522\n",
      "Epoch 1143 - Objective function: 16.756107480743278 - Loss: 16.68629886969199 - Gradient norm: 1.8319309829081205\n",
      "Epoch 1144 - Objective function: 16.75271818528778 - Loss: 16.68289381013627 - Gradient norm: 1.8298206493692302\n",
      "Epoch 1145 - Objective function: 16.749336717006695 - Loss: 16.679496596773237 - Gradient norm: 1.8277142078819661\n",
      "Epoch 1146 - Objective function: 16.745963052468667 - Loss: 16.676107206156296 - Gradient norm: 1.825611652923596\n",
      "Epoch 1147 - Objective function: 16.742597168312443 - Loss: 16.672725614908906 - Gradient norm: 1.8235129789708553\n",
      "Epoch 1148 - Objective function: 16.739239041246726 - Loss: 16.66935179972443 - Gradient norm: 1.8214181805000236\n",
      "Epoch 1149 - Objective function: 16.735888648049965 - Loss: 16.665985737365936 - Gradient norm: 1.819327251987\n",
      "Epoch 1150 - Objective function: 16.732545965570175 - Loss: 16.662627404666015 - Gradient norm: 1.8172401879073834\n",
      "Epoch 1151 - Objective function: 16.72921097072476 - Loss: 16.659276778526596 - Gradient norm: 1.815156982736544\n",
      "Epoch 1152 - Objective function: 16.725883640500335 - Loss: 16.655933835918784 - Gradient norm: 1.8130776309496999\n",
      "Epoch 1153 - Objective function: 16.72256395195255 - Loss: 16.65259855388266 - Gradient norm: 1.8110021270219927\n",
      "Epoch 1154 - Objective function: 16.719251882205892 - Loss: 16.649270909527115 - Gradient norm: 1.8089304654285574\n",
      "Epoch 1155 - Objective function: 16.715947408453495 - Loss: 16.645950880029638 - Gradient norm: 1.8068626406445976\n",
      "Epoch 1156 - Objective function: 16.71265050795701 - Loss: 16.642638442636187 - Gradient norm: 1.8047986471454542\n",
      "Epoch 1157 - Objective function: 16.709361158046374 - Loss: 16.639333574660967 - Gradient norm: 1.8027384794066792\n",
      "Epoch 1158 - Objective function: 16.706079336119632 - Loss: 16.63603625348625 - Gradient norm: 1.800682131904104\n",
      "Epoch 1159 - Objective function: 16.70280501964279 - Loss: 16.632746456562234 - Gradient norm: 1.798629599113909\n",
      "Epoch 1160 - Objective function: 16.699538186149606 - Loss: 16.629464161406815 - Gradient norm: 1.7965808755126904\n",
      "Epoch 1161 - Objective function: 16.696278813241427 - Loss: 16.626189345605447 - Gradient norm: 1.7945359555775318\n",
      "Epoch 1162 - Objective function: 16.69302687858698 - Loss: 16.62292198681092 - Gradient norm: 1.7924948337860678\n",
      "Epoch 1163 - Objective function: 16.68978235992224 - Loss: 16.61966206274323 - Gradient norm: 1.7904575046165494\n",
      "Epoch 1164 - Objective function: 16.686545235050207 - Loss: 16.616409551189363 - Gradient norm: 1.7884239625479146\n",
      "Epoch 1165 - Objective function: 16.683315481840733 - Loss: 16.613164430003124 - Gradient norm: 1.7863942020598456\n",
      "Epoch 1166 - Objective function: 16.68009307823037 - Loss: 16.609926677104966 - Gradient norm: 1.7843682176328415\n",
      "Epoch 1167 - Objective function: 16.67687800222217 - Loss: 16.60669627048181 - Gradient norm: 1.782346003748272\n",
      "Epoch 1168 - Objective function: 16.673670231885506 - Loss: 16.603473188186854 - Gradient norm: 1.7803275548884492\n",
      "Epoch 1169 - Objective function: 16.67046974535588 - Loss: 16.600257408339413 - Gradient norm: 1.7783128655366827\n",
      "Epoch 1170 - Objective function: 16.6672765208348 - Loss: 16.59704890912473 - Gradient norm: 1.7763019301773437\n",
      "Epoch 1171 - Objective function: 16.66409053658952 - Loss: 16.593847668793796 - Gradient norm: 1.7742947432959255\n",
      "Epoch 1172 - Objective function: 16.660911770952925 - Loss: 16.590653665663172 - Gradient norm: 1.7722912993791018\n",
      "Epoch 1173 - Objective function: 16.657740202323343 - Loss: 16.58746687811483 - Gradient norm: 1.7702915929147878\n",
      "Epoch 1174 - Objective function: 16.65457580916433 - Loss: 16.58428728459595 - Gradient norm: 1.7682956183921967\n",
      "Epoch 1175 - Objective function: 16.65141857000453 - Loss: 16.581114863618748 - Gradient norm: 1.7663033703018987\n",
      "Epoch 1176 - Objective function: 16.648268463437503 - Loss: 16.577949593760323 - Gradient norm: 1.7643148431358768\n",
      "Epoch 1177 - Objective function: 16.645125468121513 - Loss: 16.574791453662456 - Gradient norm: 1.7623300313875854\n",
      "Epoch 1178 - Objective function: 16.64198956277937 - Loss: 16.571640422031425 - Gradient norm: 1.7603489295520032\n",
      "Epoch 1179 - Objective function: 16.638860726198253 - Loss: 16.568496477637858 - Gradient norm: 1.7583715321256923\n",
      "Epoch 1180 - Objective function: 16.635738937229558 - Loss: 16.565359599316547 - Gradient norm: 1.7563978336068475\n",
      "Epoch 1181 - Objective function: 16.63262417478866 - Loss: 16.56222976596625 - Gradient norm: 1.7544278284953543\n",
      "Epoch 1182 - Objective function: 16.629516417854802 - Loss: 16.559106956549556 - Gradient norm: 1.7524615112928406\n",
      "Epoch 1183 - Objective function: 16.626415645470882 - Loss: 16.555991150092666 - Gradient norm: 1.7504988765027298\n",
      "Epoch 1184 - Objective function: 16.62332183674329 - Loss: 16.55288232568525 - Gradient norm: 1.7485399186302926\n",
      "Epoch 1185 - Objective function: 16.62023497084173 - Loss: 16.54978046248026 - Gradient norm: 1.7465846321826963\n",
      "Epoch 1186 - Objective function: 16.617155026999043 - Loss: 16.546685539693755 - Gradient norm: 1.74463301166906\n",
      "Epoch 1187 - Objective function: 16.614081984511046 - Loss: 16.543597536604732 - Gradient norm: 1.7426850516005015\n",
      "Epoch 1188 - Objective function: 16.61101582273633 - Loss: 16.54051643255494 - Gradient norm: 1.7407407464901867\n",
      "Epoch 1189 - Objective function: 16.60795652109611 - Loss: 16.537442206948725 - Gradient norm: 1.7388000908533814\n",
      "Epoch 1190 - Objective function: 16.60490405907405 - Loss: 16.534374839252838 - Gradient norm: 1.7368630792074966\n",
      "Epoch 1191 - Objective function: 16.60185841621608 - Loss: 16.53131430899627 - Gradient norm: 1.7349297060721385\n",
      "Epoch 1192 - Objective function: 16.598819572130207 - Loss: 16.528260595770078 - Gradient norm: 1.7329999659691535\n",
      "Epoch 1193 - Objective function: 16.595787506486392 - Loss: 16.52521367922722 - Gradient norm: 1.7310738534226786\n",
      "Epoch 1194 - Objective function: 16.592762199016317 - Loss: 16.522173539082367 - Gradient norm: 1.7291513629591828\n",
      "Epoch 1195 - Objective function: 16.589743629513265 - Loss: 16.519140155111742 - Gradient norm: 1.727232489107519\n",
      "Epoch 1196 - Objective function: 16.586731777831908 - Loss: 16.51611350715295 - Gradient norm: 1.725317226398961\n",
      "Epoch 1197 - Objective function: 16.58372662388815 - Loss: 16.51309357510479 - Gradient norm: 1.7234055693672543\n",
      "Epoch 1198 - Objective function: 16.580728147658963 - Loss: 16.5100803389271 - Gradient norm: 1.721497512548659\n",
      "Epoch 1199 - Objective function: 16.577736329182212 - Loss: 16.50707377864059 - Gradient norm: 1.7195930504819903\n",
      "Epoch 1200 - Objective function: 16.574751148556462 - Loss: 16.504073874326643 - Gradient norm: 1.717692177708665\n",
      "Epoch 1201 - Objective function: 16.571772585940845 - Loss: 16.50108060612718 - Gradient norm: 1.7157948887727408\n",
      "Epoch 1202 - Objective function: 16.568800621554868 - Loss: 16.498093954244464 - Gradient norm: 1.7139011782209614\n",
      "Epoch 1203 - Objective function: 16.565835235678232 - Loss: 16.495113898940943 - Gradient norm: 1.7120110406027946\n",
      "Epoch 1204 - Objective function: 16.562876408650677 - Loss: 16.492140420539076 - Gradient norm: 1.7101244704704752\n",
      "Epoch 1205 - Objective function: 16.559924120871827 - Loss: 16.489173499421163 - Gradient norm: 1.7082414623790458\n",
      "Epoch 1206 - Objective function: 16.556978352800982 - Loss: 16.486213116029177 - Gradient norm: 1.7063620108863933\n",
      "Epoch 1207 - Objective function: 16.554039084956973 - Loss: 16.483259250864588 - Gradient norm: 1.7044861105532922\n",
      "Epoch 1208 - Objective function: 16.551106297918004 - Loss: 16.480311884488213 - Gradient norm: 1.7026137559434416\n",
      "Epoch 1209 - Objective function: 16.548179972321464 - Loss: 16.477370997520037 - Gradient norm: 1.7007449416235025\n",
      "Epoch 1210 - Objective function: 16.545260088863753 - Loss: 16.474436570639025 - Gradient norm: 1.6988796621631397\n",
      "Epoch 1211 - Objective function: 16.54234662830014 - Loss: 16.471508584583 - Gradient norm: 1.6970179121350537\n",
      "Epoch 1212 - Objective function: 16.53943957144457 - Loss: 16.468587020148423 - Gradient norm: 1.6951596861150215\n",
      "Epoch 1213 - Objective function: 16.536538899169525 - Loss: 16.465671858190273 - Gradient norm: 1.6933049786819336\n",
      "Epoch 1214 - Objective function: 16.533644592405814 - Loss: 16.462763079621848 - Gradient norm: 1.6914537844178252\n",
      "Epoch 1215 - Objective function: 16.530756632142452 - Loss: 16.459860665414617 - Gradient norm: 1.6896060979079166\n",
      "Epoch 1216 - Objective function: 16.527874999426462 - Loss: 16.456964596598038 - Gradient norm: 1.6877619137406465\n",
      "Epoch 1217 - Objective function: 16.524999675362732 - Loss: 16.454074854259407 - Gradient norm: 1.6859212265077088\n",
      "Epoch 1218 - Objective function: 16.522130641113822 - Loss: 16.45119141954369 - Gradient norm: 1.6840840308040819\n",
      "Epoch 1219 - Objective function: 16.51926787789984 - Loss: 16.448314273653352 - Gradient norm: 1.682250321228066\n",
      "Epoch 1220 - Objective function: 16.516411366998213 - Loss: 16.445443397848184 - Gradient norm: 1.680420092381317\n",
      "Epoch 1221 - Objective function: 16.513561089743593 - Loss: 16.442578773445167 - Gradient norm: 1.6785933388688778\n",
      "Epoch 1222 - Objective function: 16.510717027527644 - Loss: 16.439720381818276 - Gradient norm: 1.6767700552992113\n",
      "Epoch 1223 - Objective function: 16.507879161798904 - Loss: 16.43686820439834 - Gradient norm: 1.6749502362842315\n",
      "Epoch 1224 - Objective function: 16.505047474062604 - Loss: 16.43402222267286 - Gradient norm: 1.6731338764393386\n",
      "Epoch 1225 - Objective function: 16.502221945880496 - Loss: 16.431182418185852 - Gradient norm: 1.671320970383444\n",
      "Epoch 1226 - Objective function: 16.499402558870734 - Loss: 16.4283487725377 - Gradient norm: 1.6695115127390083\n",
      "Epoch 1227 - Objective function: 16.496589294707668 - Loss: 16.42552126738497 - Gradient norm: 1.667705498132066\n",
      "Epoch 1228 - Objective function: 16.493782135121698 - Loss: 16.422699884440256 - Gradient norm: 1.665902921192259\n",
      "Epoch 1229 - Objective function: 16.490981061899102 - Loss: 16.419884605472024 - Gradient norm: 1.6641037765528632\n",
      "Epoch 1230 - Objective function: 16.48818605688189 - Loss: 16.417075412304442 - Gradient norm: 1.6623080588508217\n",
      "Epoch 1231 - Objective function: 16.485397101967628 - Loss: 16.41427228681723 - Gradient norm: 1.6605157627267704\n",
      "Epoch 1232 - Objective function: 16.482614179109298 - Loss: 16.411475210945483 - Gradient norm: 1.6587268828250672\n",
      "Epoch 1233 - Objective function: 16.479837270315087 - Loss: 16.408684166679517 - Gradient norm: 1.6569414137938203\n",
      "Epoch 1234 - Objective function: 16.4770663576483 - Loss: 16.40589913606472 - Gradient norm: 1.6551593502849165\n",
      "Epoch 1235 - Objective function: 16.474301423227157 - Loss: 16.40312010120138 - Gradient norm: 1.6533806869540482\n",
      "Epoch 1236 - Objective function: 16.4715424492246 - Loss: 16.400347044244526 - Gradient norm: 1.6516054184607383\n",
      "Epoch 1237 - Objective function: 16.468789417868216 - Loss: 16.397579947403774 - Gradient norm: 1.6498335394683699\n",
      "Epoch 1238 - Objective function: 16.46604231144 - Loss: 16.394818792943152 - Gradient norm: 1.648065044644212\n",
      "Epoch 1239 - Objective function: 16.463301112276245 - Loss: 16.392063563180972 - Gradient norm: 1.6462999286594429\n",
      "Epoch 1240 - Objective function: 16.460565802767366 - Loss: 16.389314240489647 - Gradient norm: 1.6445381861891768\n",
      "Epoch 1241 - Objective function: 16.457836365357743 - Loss: 16.38657080729554 - Gradient norm: 1.6427798119124921\n",
      "Epoch 1242 - Objective function: 16.455112782545555 - Loss: 16.383833246078805 - Gradient norm: 1.641024800512451\n",
      "Epoch 1243 - Objective function: 16.452395036882645 - Loss: 16.38110153937323 - Gradient norm: 1.6392731466761277\n",
      "Epoch 1244 - Objective function: 16.449683110974327 - Loss: 16.37837566976608 - Gradient norm: 1.637524845094631\n",
      "Epoch 1245 - Objective function: 16.44697698747927 - Loss: 16.375655619897955 - Gradient norm: 1.6357798904631282\n",
      "Epoch 1246 - Objective function: 16.444276649109323 - Loss: 16.372941372462606 - Gradient norm: 1.6340382774808686\n",
      "Epoch 1247 - Objective function: 16.44158207862934 - Loss: 16.37023291020679 - Gradient norm: 1.6323000008512087\n",
      "Epoch 1248 - Objective function: 16.43889325885707 - Loss: 16.36753021593015 - Gradient norm: 1.6305650552816298\n",
      "Epoch 1249 - Objective function: 16.436210172662932 - Loss: 16.364833272484976 - Gradient norm: 1.6288334354837666\n",
      "Epoch 1250 - Objective function: 16.43353280296995 - Loss: 16.362142062776154 - Gradient norm: 1.6271051361734237\n",
      "Epoch 1251 - Objective function: 16.430861132753524 - Loss: 16.359456569760926 - Gradient norm: 1.625380152070602\n",
      "Epoch 1252 - Objective function: 16.428195145041297 - Loss: 16.35677677644878 - Gradient norm: 1.6236584778995176\n",
      "Epoch 1253 - Objective function: 16.42553482291302 - Loss: 16.354102665901284 - Gradient norm: 1.6219401083886251\n",
      "Epoch 1254 - Objective function: 16.422880149500376 - Loss: 16.35143422123194 - Gradient norm: 1.6202250382706351\n",
      "Epoch 1255 - Objective function: 16.420231107986844 - Loss: 16.348771425606024 - Gradient norm: 1.6185132622825382\n",
      "Epoch 1256 - Objective function: 16.417587681607525 - Loss: 16.346114262240427 - Gradient norm: 1.616804775165623\n",
      "Epoch 1257 - Objective function: 16.414949853649006 - Loss: 16.343462714403515 - Gradient norm: 1.6150995716654981\n",
      "Epoch 1258 - Objective function: 16.41231760744921 - Loss: 16.340816765414974 - Gradient norm: 1.6133976465321094\n",
      "Epoch 1259 - Objective function: 16.40969092639722 - Loss: 16.33817639864565 - Gradient norm: 1.6116989945197615\n",
      "Epoch 1260 - Objective function: 16.40706979393317 - Loss: 16.335541597517416 - Gradient norm: 1.610003610387136\n",
      "Epoch 1261 - Objective function: 16.404454193548045 - Loss: 16.33291234550299 - Gradient norm: 1.6083114888973107\n",
      "Epoch 1262 - Objective function: 16.40184410878356 - Loss: 16.330288626125828 - Gradient norm: 1.6066226248177768\n",
      "Epoch 1263 - Objective function: 16.39923952323202 - Loss: 16.327670422959933 - Gradient norm: 1.6049370129204605\n",
      "Epoch 1264 - Objective function: 16.396640420536123 - Loss: 16.32505771962971 - Gradient norm: 1.6032546479817353\n",
      "Epoch 1265 - Objective function: 16.394046784388866 - Loss: 16.322450499809865 - Gradient norm: 1.6015755247824468\n",
      "Epoch 1266 - Objective function: 16.39145859853335 - Loss: 16.319848747225173 - Gradient norm: 1.5998996381079242\n",
      "Epoch 1267 - Objective function: 16.388875846762673 - Loss: 16.31725244565041 - Gradient norm: 1.5982269827479996\n",
      "Epoch 1268 - Objective function: 16.38629851291973 - Loss: 16.314661578910155 - Gradient norm: 1.596557553497027\n",
      "Epoch 1269 - Objective function: 16.383726580897125 - Loss: 16.31207613087865 - Gradient norm: 1.5948913451538946\n",
      "Epoch 1270 - Objective function: 16.381160034636974 - Loss: 16.309496085479676 - Gradient norm: 1.593228352522046\n",
      "Epoch 1271 - Objective function: 16.378598858130772 - Loss: 16.30692142668637 - Gradient norm: 1.5915685704094922\n",
      "Epoch 1272 - Objective function: 16.376043035419258 - Loss: 16.3043521385211 - Gradient norm: 1.5899119936288297\n",
      "Epoch 1273 - Objective function: 16.373492550592257 - Loss: 16.301788205055324 - Gradient norm: 1.5882586169972563\n",
      "Epoch 1274 - Objective function: 16.370947387788526 - Loss: 16.29922961040942 - Gradient norm: 1.5866084353365861\n",
      "Epoch 1275 - Objective function: 16.36840753119563 - Loss: 16.29667633875256 - Gradient norm: 1.5849614434732624\n",
      "Epoch 1276 - Objective function: 16.36587296504978 - Loss: 16.29412837430256 - Gradient norm: 1.583317636238376\n",
      "Epoch 1277 - Objective function: 16.363343673635686 - Loss: 16.291585701325733 - Gradient norm: 1.5816770084676801\n",
      "Epoch 1278 - Objective function: 16.360819641286415 - Loss: 16.28904830413673 - Gradient norm: 1.5800395550016002\n",
      "Epoch 1279 - Objective function: 16.35830085238326 - Loss: 16.286516167098434 - Gradient norm: 1.578405270685253\n",
      "Epoch 1280 - Objective function: 16.355787291355572 - Loss: 16.283989274621767 - Gradient norm: 1.5767741503684574\n",
      "Epoch 1281 - Objective function: 16.35327894268063 - Loss: 16.281467611165585 - Gradient norm: 1.5751461889057505\n",
      "Epoch 1282 - Objective function: 16.3507757908835 - Loss: 16.27895116123651 - Gradient norm: 1.5735213811563984\n",
      "Epoch 1283 - Objective function: 16.348277820536875 - Loss: 16.276439909388802 - Gradient norm: 1.571899721984414\n",
      "Epoch 1284 - Objective function: 16.345785016260955 - Loss: 16.273933840224213 - Gradient norm: 1.5702812062585638\n",
      "Epoch 1285 - Objective function: 16.343297362723288 - Loss: 16.271432938391836 - Gradient norm: 1.5686658288523858\n",
      "Epoch 1286 - Objective function: 16.34081484463863 - Loss: 16.26893718858797 - Gradient norm: 1.5670535846441997\n",
      "Epoch 1287 - Objective function: 16.338337446768808 - Loss: 16.266446575555985 - Gradient norm: 1.5654444685171212\n",
      "Epoch 1288 - Objective function: 16.335865153922583 - Loss: 16.263961084086162 - Gradient norm: 1.5638384753590708\n",
      "Epoch 1289 - Objective function: 16.333397950955494 - Loss: 16.26148069901557 - Gradient norm: 1.5622356000627913\n",
      "Epoch 1290 - Objective function: 16.33093582276972 - Loss: 16.259005405227914 - Gradient norm: 1.5606358375258513\n",
      "Epoch 1291 - Objective function: 16.328478754313952 - Loss: 16.256535187653405 - Gradient norm: 1.5590391826506673\n",
      "Epoch 1292 - Objective function: 16.32602673058325 - Loss: 16.25407003126861 - Gradient norm: 1.5574456303445043\n",
      "Epoch 1293 - Objective function: 16.323579736618896 - Loss: 16.251609921096318 - Gradient norm: 1.5558551755194954\n",
      "Epoch 1294 - Objective function: 16.321137757508243 - Loss: 16.249154842205392 - Gradient norm: 1.5542678130926493\n",
      "Epoch 1295 - Objective function: 16.31870077838462 - Loss: 16.246704779710658 - Gradient norm: 1.55268353798586\n",
      "Epoch 1296 - Objective function: 16.316268784427134 - Loss: 16.244259718772724 - Gradient norm: 1.5511023451259192\n",
      "Epoch 1297 - Objective function: 16.313841760860573 - Loss: 16.24181964459787 - Gradient norm: 1.5495242294445264\n",
      "Epoch 1298 - Objective function: 16.31141969295526 - Loss: 16.23938454243791 - Gradient norm: 1.547949185878299\n",
      "Epoch 1299 - Objective function: 16.309002566026912 - Loss: 16.236954397590047 - Gradient norm: 1.546377209368781\n",
      "Epoch 1300 - Objective function: 16.306590365436495 - Loss: 16.234529195396735 - Gradient norm: 1.544808294862455\n",
      "Epoch 1301 - Objective function: 16.304183076590107 - Loss: 16.232108921245548 - Gradient norm: 1.5432424373107527\n",
      "Epoch 1302 - Objective function: 16.301780684938816 - Loss: 16.229693560569032 - Gradient norm: 1.5416796316700585\n",
      "Epoch 1303 - Objective function: 16.299383175978548 - Loss: 16.227283098844588 - Gradient norm: 1.5401198729017254\n",
      "Epoch 1304 - Objective function: 16.296990535249925 - Loss: 16.224877521594326 - Gradient norm: 1.5385631559720818\n",
      "Epoch 1305 - Objective function: 16.294602748338175 - Loss: 16.222476814384926 - Gradient norm: 1.5370094758524384\n",
      "Epoch 1306 - Objective function: 16.292219800872928 - Loss: 16.220080962827495 - Gradient norm: 1.5354588275191006\n",
      "Epoch 1307 - Objective function: 16.289841678528155 - Loss: 16.217689952577473 - Gradient norm: 1.5339112059533737\n",
      "Epoch 1308 - Objective function: 16.287468367021972 - Loss: 16.215303769334444 - Gradient norm: 1.5323666061415746\n",
      "Epoch 1309 - Objective function: 16.285099852116552 - Loss: 16.21292239884204 - Gradient norm: 1.530825023075036\n",
      "Epoch 1310 - Objective function: 16.282736119617965 - Loss: 16.21054582688779 - Gradient norm: 1.5292864517501192\n",
      "Epoch 1311 - Objective function: 16.28037715537604 - Loss: 16.208174039302996 - Gradient norm: 1.5277508871682193\n",
      "Epoch 1312 - Objective function: 16.27802294528427 - Loss: 16.2058070219626 - Gradient norm: 1.526218324335771\n",
      "Epoch 1313 - Objective function: 16.275673475279632 - Loss: 16.203444760785043 - Gradient norm: 1.5246887582642625\n",
      "Epoch 1314 - Objective function: 16.273328731342477 - Loss: 16.201087241732136 - Gradient norm: 1.5231621839702356\n",
      "Epoch 1315 - Objective function: 16.270988699496417 - Loss: 16.19873445080894 - Gradient norm: 1.5216385964752979\n",
      "Epoch 1316 - Objective function: 16.26865336580815 - Loss: 16.196386374063614 - Gradient norm: 1.520117990806129\n",
      "Epoch 1317 - Objective function: 16.266322716387357 - Loss: 16.194042997587307 - Gradient norm: 1.518600361994486\n",
      "Epoch 1318 - Objective function: 16.26399673738658 - Loss: 16.191704307514005 - Gradient norm: 1.5170857050772117\n",
      "Epoch 1319 - Objective function: 16.26167541500107 - Loss: 16.189370290020413 - Gradient norm: 1.5155740150962427\n",
      "Epoch 1320 - Objective function: 16.25935873546866 - Loss: 16.187040931325832 - Gradient norm: 1.5140652870986102\n",
      "Epoch 1321 - Objective function: 16.25704668506965 - Loss: 16.184716217692007 - Gradient norm: 1.5125595161364553\n",
      "Epoch 1322 - Objective function: 16.254739250126658 - Loss: 16.182396135423023 - Gradient norm: 1.5110566972670285\n",
      "Epoch 1323 - Objective function: 16.25243641700452 - Loss: 16.18008067086516 - Gradient norm: 1.5095568255526959\n",
      "Epoch 1324 - Objective function: 16.25013817211011 - Loss: 16.177769810406758 - Gradient norm: 1.5080598960609493\n",
      "Epoch 1325 - Objective function: 16.24784450189228 - Loss: 16.175463540478123 - Gradient norm: 1.5065659038644115\n",
      "Epoch 1326 - Objective function: 16.245555392841666 - Loss: 16.17316184755136 - Gradient norm: 1.5050748440408364\n",
      "Epoch 1327 - Objective function: 16.2432708314906 - Loss: 16.170864718140255 - Gradient norm: 1.5035867116731203\n",
      "Epoch 1328 - Objective function: 16.240990804412988 - Loss: 16.168572138800172 - Gradient norm: 1.5021015018493078\n",
      "Epoch 1329 - Objective function: 16.23871529822415 - Loss: 16.1662840961279 - Gradient norm: 1.5006192096625925\n",
      "Epoch 1330 - Objective function: 16.236444299580725 - Loss: 16.16400057676153 - Gradient norm: 1.4991398302113252\n",
      "Epoch 1331 - Objective function: 16.234177795180504 - Loss: 16.16172156738034 - Gradient norm: 1.49766335859902\n",
      "Epoch 1332 - Objective function: 16.231915771762367 - Loss: 16.159447054704657 - Gradient norm: 1.4961897899343561\n",
      "Epoch 1333 - Objective function: 16.2296582161061 - Loss: 16.157177025495745 - Gradient norm: 1.4947191193311857\n",
      "Epoch 1334 - Objective function: 16.227405115032305 - Loss: 16.154911466555674 - Gradient norm: 1.4932513419085358\n",
      "Epoch 1335 - Objective function: 16.22515645540224 - Loss: 16.15265036472718 - Gradient norm: 1.491786452790616\n",
      "Epoch 1336 - Objective function: 16.222912224117735 - Loss: 16.150393706893563 - Gradient norm: 1.4903244471068198\n",
      "Epoch 1337 - Objective function: 16.22067240812105 - Loss: 16.148141479978563 - Gradient norm: 1.488865319991732\n",
      "Epoch 1338 - Objective function: 16.21843699439473 - Loss: 16.145893670946208 - Gradient norm: 1.487409066585132\n",
      "Epoch 1339 - Objective function: 16.216205969961536 - Loss: 16.14365026680073 - Gradient norm: 1.4859556820319935\n",
      "Epoch 1340 - Objective function: 16.213979321884253 - Loss: 16.141411254586412 - Gradient norm: 1.484505161482497\n",
      "Epoch 1341 - Objective function: 16.21175703726562 - Loss: 16.139176621387474 - Gradient norm: 1.4830575000920265\n",
      "Epoch 1342 - Objective function: 16.209539103248186 - Loss: 16.13694635432796 - Gradient norm: 1.4816126930211768\n",
      "Epoch 1343 - Objective function: 16.20732550701419 - Loss: 16.1347204405716 - Gradient norm: 1.4801707354357554\n",
      "Epoch 1344 - Objective function: 16.20511623578545 - Loss: 16.132498867321704 - Gradient norm: 1.478731622506789\n",
      "Epoch 1345 - Objective function: 16.202911276823205 - Loss: 16.130281621821023 - Gradient norm: 1.4772953494105234\n",
      "Epoch 1346 - Objective function: 16.200710617428065 - Loss: 16.128068691351658 - Gradient norm: 1.475861911328427\n",
      "Epoch 1347 - Objective function: 16.198514244939798 - Loss: 16.1258600632349 - Gradient norm: 1.4744313034471987\n",
      "Epoch 1348 - Objective function: 16.19632214673729 - Loss: 16.123655724831135 - Gradient norm: 1.4730035209587662\n",
      "Epoch 1349 - Objective function: 16.194134310238397 - Loss: 16.12145566353973 - Gradient norm: 1.4715785590602897\n",
      "Epoch 1350 - Objective function: 16.191950722899794 - Loss: 16.11925986679889 - Gradient norm: 1.470156412954168\n",
      "Epoch 1351 - Objective function: 16.1897713722169 - Loss: 16.117068322085544 - Gradient norm: 1.4687370778480382\n",
      "Epoch 1352 - Objective function: 16.187596245723743 - Loss: 16.114881016915255 - Gradient norm: 1.4673205489547798\n",
      "Epoch 1353 - Objective function: 16.185425330992835 - Loss: 16.112697938842068 - Gradient norm: 1.465906821492517\n",
      "Epoch 1354 - Objective function: 16.183258615635054 - Loss: 16.11051907545839 - Gradient norm: 1.4644958906846217\n",
      "Epoch 1355 - Objective function: 16.181096087299547 - Loss: 16.108344414394914 - Gradient norm: 1.463087751759714\n",
      "Epoch 1356 - Objective function: 16.178937733673585 - Loss: 16.106173943320446 - Gradient norm: 1.4616823999516702\n",
      "Epoch 1357 - Objective function: 16.17678354248245 - Loss: 16.104007649941828 - Gradient norm: 1.4602798304996167\n",
      "Epoch 1358 - Objective function: 16.17463350148935 - Loss: 16.101845522003806 - Gradient norm: 1.458880038647941\n",
      "Epoch 1359 - Objective function: 16.17248759849525 - Loss: 16.099687547288916 - Gradient norm: 1.4574830196462856\n",
      "Epoch 1360 - Objective function: 16.17034582133879 - Loss: 16.09753371361736 - Gradient norm: 1.4560887687495587\n",
      "Epoch 1361 - Objective function: 16.168208157896164 - Loss: 16.095384008846903 - Gradient norm: 1.4546972812179273\n",
      "Epoch 1362 - Objective function: 16.166074596081014 - Loss: 16.09323842087275 - Gradient norm: 1.4533085523168248\n",
      "Epoch 1363 - Objective function: 16.163945123844286 - Loss: 16.091096937627437 - Gradient norm: 1.451922577316952\n",
      "Epoch 1364 - Objective function: 16.161819729174137 - Loss: 16.088959547080698 - Gradient norm: 1.4505393514942786\n",
      "Epoch 1365 - Objective function: 16.159698400095817 - Loss: 16.08682623723938 - Gradient norm: 1.4491588701300424\n",
      "Epoch 1366 - Objective function: 16.157581124671555 - Loss: 16.084696996147297 - Gradient norm: 1.4477811285107542\n",
      "Epoch 1367 - Objective function: 16.155467891000434 - Loss: 16.082571811885146 - Gradient norm: 1.4464061219281996\n",
      "Epoch 1368 - Objective function: 16.153358687218308 - Loss: 16.08045067257037 - Gradient norm: 1.445033845679436\n",
      "Epoch 1369 - Objective function: 16.151253501497628 - Loss: 16.078333566357056 - Gradient norm: 1.4436642950667975\n",
      "Epoch 1370 - Objective function: 16.149152322047396 - Loss: 16.07622048143581 - Gradient norm: 1.4422974653978957\n",
      "Epoch 1371 - Objective function: 16.147055137113032 - Loss: 16.07411140603368 - Gradient norm: 1.440933351985622\n",
      "Epoch 1372 - Objective function: 16.14496193497623 - Loss: 16.07200632841399 - Gradient norm: 1.439571950148145\n",
      "Epoch 1373 - Objective function: 16.142872703954882 - Loss: 16.069905236876274 - Gradient norm: 1.4382132552089149\n",
      "Epoch 1374 - Objective function: 16.14078743240295 - Loss: 16.067808119756133 - Gradient norm: 1.4368572624966638\n",
      "Epoch 1375 - Objective function: 16.138706108710366 - Loss: 16.065714965425155 - Gradient norm: 1.4355039673454058\n",
      "Epoch 1376 - Objective function: 16.136628721302902 - Loss: 16.063625762290766 - Gradient norm: 1.4341533650944374\n",
      "Epoch 1377 - Objective function: 16.13455525864208 - Loss: 16.06154049879615 - Gradient norm: 1.432805451088341\n",
      "Epoch 1378 - Objective function: 16.13248570922505 - Loss: 16.059459163420126 - Gradient norm: 1.4314602206769829\n",
      "Epoch 1379 - Objective function: 16.130420061584477 - Loss: 16.05738174467704 - Gradient norm: 1.430117669215515\n",
      "Epoch 1380 - Objective function: 16.128358304288447 - Loss: 16.055308231116662 - Gradient norm: 1.4287777920643747\n",
      "Epoch 1381 - Objective function: 16.12630042594033 - Loss: 16.05323861132406 - Gradient norm: 1.4274405845892857\n",
      "Epoch 1382 - Objective function: 16.12424641517871 - Loss: 16.0511728739195 - Gradient norm: 1.4261060421612595\n",
      "Epoch 1383 - Objective function: 16.12219626067724 - Loss: 16.049111007558356 - Gradient norm: 1.4247741601565942\n",
      "Epoch 1384 - Objective function: 16.120149951144548 - Loss: 16.047053000930962 - Gradient norm: 1.4234449339568762\n",
      "Epoch 1385 - Objective function: 16.118107475324127 - Loss: 16.044998842762535 - Gradient norm: 1.42211835894898\n",
      "Epoch 1386 - Objective function: 16.11606882199424 - Loss: 16.042948521813063 - Gradient norm: 1.4207944305250677\n",
      "Epoch 1387 - Objective function: 16.11403397996779 - Loss: 16.040902026877188 - Gradient norm: 1.41947314408259\n",
      "Epoch 1388 - Objective function: 16.112002938092225 - Loss: 16.0388593467841 - Gradient norm: 1.4181544950242848\n",
      "Epoch 1389 - Objective function: 16.10997568524943 - Loss: 16.036820470397434 - Gradient norm: 1.4168384787581807\n",
      "Epoch 1390 - Objective function: 16.107952210355634 - Loss: 16.034785386615173 - Gradient norm: 1.4155250906975938\n",
      "Epoch 1391 - Objective function: 16.10593250236126 - Loss: 16.032754084369518 - Gradient norm: 1.4142143262611273\n",
      "Epoch 1392 - Objective function: 16.103916550250876 - Loss: 16.030726552626806 - Gradient norm: 1.412906180872675\n",
      "Epoch 1393 - Objective function: 16.101904343043064 - Loss: 16.0287027803874 - Gradient norm: 1.4116006499614175\n",
      "Epoch 1394 - Objective function: 16.099895869790277 - Loss: 16.02668275668555 - Gradient norm: 1.4102977289618222\n",
      "Epoch 1395 - Objective function: 16.09789111957882 - Loss: 16.024666470589352 - Gradient norm: 1.408997413313646\n",
      "Epoch 1396 - Objective function: 16.09589008152865 - Loss: 16.022653911200585 - Gradient norm: 1.4076996984619328\n",
      "Epoch 1397 - Objective function: 16.09389274479336 - Loss: 16.020645067654645 - Gradient norm: 1.406404579857013\n",
      "Epoch 1398 - Objective function: 16.091899098559985 - Loss: 16.018639929120393 - Gradient norm: 1.4051120529545027\n",
      "Epoch 1399 - Objective function: 16.089909132048987 - Loss: 16.016638484800133 - Gradient norm: 1.4038221132153057\n",
      "Epoch 1400 - Objective function: 16.087922834514085 - Loss: 16.014640723929414 - Gradient norm: 1.4025347561056098\n",
      "Epoch 1401 - Objective function: 16.085940195242188 - Loss: 16.012646635777006 - Gradient norm: 1.4012499770968894\n",
      "Epoch 1402 - Objective function: 16.083961203553265 - Loss: 16.010656209644736 - Gradient norm: 1.3999677716659016\n",
      "Epoch 1403 - Objective function: 16.08198584880028 - Loss: 16.008669434867432 - Gradient norm: 1.398688135294688\n",
      "Epoch 1404 - Objective function: 16.080014120369043 - Loss: 16.006686300812788 - Gradient norm: 1.397411063470574\n",
      "Epoch 1405 - Objective function: 16.078046007678157 - Loss: 16.00470679688129 - Gradient norm: 1.3961365516861657\n",
      "Epoch 1406 - Objective function: 16.07608150017888 - Loss: 16.00273091250609 - Gradient norm: 1.3948645954393524\n",
      "Epoch 1407 - Objective function: 16.074120587355036 - Loss: 16.000758637152916 - Gradient norm: 1.3935951902333024\n",
      "Epoch 1408 - Objective function: 16.072163258722906 - Loss: 15.998789960319973 - Gradient norm: 1.3923283315764647\n",
      "Epoch 1409 - Objective function: 16.07020950383115 - Loss: 15.996824871537836 - Gradient norm: 1.3910640149825684\n",
      "Epoch 1410 - Objective function: 16.068259312260686 - Loss: 15.994863360369354 - Gradient norm: 1.3898022359706184\n",
      "Epoch 1411 - Objective function: 16.06631267362459 - Loss: 15.992905416409556 - Gradient norm: 1.3885429900648987\n",
      "Epoch 1412 - Objective function: 16.06436957756801 - Loss: 15.990951029285531 - Gradient norm: 1.3872862727949669\n",
      "Epoch 1413 - Objective function: 16.062430013768047 - Loss: 15.98900018865635 - Gradient norm: 1.3860320796956602\n",
      "Epoch 1414 - Objective function: 16.06049397193368 - Loss: 15.987052884212957 - Gradient norm: 1.3847804063070839\n",
      "Epoch 1415 - Objective function: 16.058561441805644 - Loss: 15.985109105678076 - Gradient norm: 1.38353124817462\n",
      "Epoch 1416 - Objective function: 16.056632413156347 - Loss: 15.983168842806098 - Gradient norm: 1.382284600848921\n",
      "Epoch 1417 - Objective function: 16.05470687578976 - Loss: 15.981232085383004 - Gradient norm: 1.3810404598859098\n",
      "Epoch 1418 - Objective function: 16.05278481954133 - Loss: 15.979298823226245 - Gradient norm: 1.3797988208467789\n",
      "Epoch 1419 - Objective function: 16.050866234277873 - Loss: 15.977369046184663 - Gradient norm: 1.3785596792979877\n",
      "Epoch 1420 - Objective function: 16.048951109897487 - Loss: 15.975442744138384 - Gradient norm: 1.3773230308112643\n",
      "Epoch 1421 - Objective function: 16.04703943632944 - Loss: 15.97351990699872 - Gradient norm: 1.3760888709636008\n",
      "Epoch 1422 - Objective function: 16.045131203534094 - Loss: 15.97160052470808 - Gradient norm: 1.3748571953372526\n",
      "Epoch 1423 - Objective function: 16.043226401502775 - Loss: 15.969684587239858 - Gradient norm: 1.3736279995197394\n",
      "Epoch 1424 - Objective function: 16.041325020257723 - Loss: 15.967772084598362 - Gradient norm: 1.3724012791038427\n",
      "Epoch 1425 - Objective function: 16.03942704985195 - Loss: 15.965863006818696 - Gradient norm: 1.3711770296875996\n",
      "Epoch 1426 - Objective function: 16.037532480369176 - Loss: 15.963957343966664 - Gradient norm: 1.3699552468743108\n",
      "Epoch 1427 - Objective function: 16.035641301923715 - Loss: 15.96205508613869 - Gradient norm: 1.3687359262725305\n",
      "Epoch 1428 - Objective function: 16.0337535046604 - Loss: 15.960156223461718 - Gradient norm: 1.36751906349607\n",
      "Epoch 1429 - Objective function: 16.031869078754465 - Loss: 15.95826074609311 - Gradient norm: 1.3663046541639923\n",
      "Epoch 1430 - Objective function: 16.029988014411465 - Loss: 15.956368644220555 - Gradient norm: 1.365092693900615\n",
      "Epoch 1431 - Objective function: 16.02811030186717 - Loss: 15.954479908061977 - Gradient norm: 1.363883178335505\n",
      "Epoch 1432 - Objective function: 16.02623593138749 - Loss: 15.952594527865438 - Gradient norm: 1.3626761031034769\n",
      "Epoch 1433 - Objective function: 16.02436489326837 - Loss: 15.950712493909053 - Gradient norm: 1.3614714638445953\n",
      "Epoch 1434 - Objective function: 16.022497177835685 - Loss: 15.94883379650088 - Gradient norm: 1.360269256204168\n",
      "Epoch 1435 - Objective function: 16.02063277544517 - Loss: 15.94695842597884 - Gradient norm: 1.3590694758327488\n",
      "Epoch 1436 - Objective function: 16.018771676482306 - Loss: 15.945086372710623 - Gradient norm: 1.3578721183861324\n",
      "Epoch 1437 - Objective function: 16.016913871362256 - Loss: 15.9432176270936 - Gradient norm: 1.356677179525353\n",
      "Epoch 1438 - Objective function: 16.01505935052972 - Loss: 15.941352179554706 - Gradient norm: 1.3554846549166857\n",
      "Epoch 1439 - Objective function: 16.01320810445892 - Loss: 15.93949002055038 - Gradient norm: 1.3542945402316404\n",
      "Epoch 1440 - Objective function: 16.011360123653418 - Loss: 15.937631140566456 - Gradient norm: 1.3531068311469636\n",
      "Epoch 1441 - Objective function: 16.009515398646116 - Loss: 15.935775530118075 - Gradient norm: 1.3519215233446333\n",
      "Epoch 1442 - Objective function: 16.00767391999909 - Loss: 15.933923179749595 - Gradient norm: 1.3507386125118614\n",
      "Epoch 1443 - Objective function: 16.005835678303544 - Loss: 15.932074080034502 - Gradient norm: 1.3495580943410854\n",
      "Epoch 1444 - Objective function: 16.004000664179703 - Loss: 15.930228221575305 - Gradient norm: 1.3483799645299754\n",
      "Epoch 1445 - Objective function: 16.002168868276712 - Loss: 15.92838559500347 - Gradient norm: 1.347204218781423\n",
      "Epoch 1446 - Objective function: 16.00034028127258 - Loss: 15.926546190979312 - Gradient norm: 1.3460308528035447\n",
      "Epoch 1447 - Objective function: 15.998514893874063 - Loss: 15.924710000191915 - Gradient norm: 1.3448598623096795\n",
      "Epoch 1448 - Objective function: 15.996692696816561 - Loss: 15.92287701335903 - Gradient norm: 1.3436912430183852\n",
      "Epoch 1449 - Objective function: 15.994873680864073 - Loss: 15.921047221226996 - Gradient norm: 1.342524990653438\n",
      "Epoch 1450 - Objective function: 15.993057836809072 - Loss: 15.919220614570662 - Gradient norm: 1.3413611009438295\n",
      "Epoch 1451 - Objective function: 15.991245155472424 - Loss: 15.917397184193266 - Gradient norm: 1.3401995696237667\n",
      "Epoch 1452 - Objective function: 15.989435627703308 - Loss: 15.915576920926375 - Gradient norm: 1.3390403924326655\n",
      "Epoch 1453 - Objective function: 15.987629244379123 - Loss: 15.91375981562979 - Gradient norm: 1.3378835651151524\n",
      "Epoch 1454 - Objective function: 15.985825996405401 - Loss: 15.911945859191457 - Gradient norm: 1.3367290834210634\n",
      "Epoch 1455 - Objective function: 15.984025874715712 - Loss: 15.91013504252737 - Gradient norm: 1.3355769431054385\n",
      "Epoch 1456 - Objective function: 15.982228870271594 - Loss: 15.908327356581504 - Gradient norm: 1.3344271399285204\n",
      "Epoch 1457 - Objective function: 15.980434974062442 - Loss: 15.906522792325704 - Gradient norm: 1.3332796696557536\n",
      "Epoch 1458 - Objective function: 15.97864417710544 - Loss: 15.90472134075962 - Gradient norm: 1.3321345280577843\n",
      "Epoch 1459 - Objective function: 15.976856470445478 - Loss: 15.902922992910609 - Gradient norm: 1.3309917109104503\n",
      "Epoch 1460 - Objective function: 15.975071845155032 - Loss: 15.901127739833642 - Gradient norm: 1.329851213994791\n",
      "Epoch 1461 - Objective function: 15.973290292334134 - Loss: 15.899335572611246 - Gradient norm: 1.3287130330970325\n",
      "Epoch 1462 - Objective function: 15.971511803110229 - Loss: 15.897546482353379 - Gradient norm: 1.327577164008595\n",
      "Epoch 1463 - Objective function: 15.969736368638126 - Loss: 15.895760460197375 - Gradient norm: 1.3264436025260868\n",
      "Epoch 1464 - Objective function: 15.967963980099901 - Loss: 15.893977497307844 - Gradient norm: 1.325312344451299\n",
      "Epoch 1465 - Objective function: 15.96619462870481 - Loss: 15.892197584876598 - Gradient norm: 1.3241833855912106\n",
      "Epoch 1466 - Objective function: 15.964428305689209 - Loss: 15.890420714122554 - Gradient norm: 1.323056721757983\n",
      "Epoch 1467 - Objective function: 15.96266500231647 - Loss: 15.888646876291654 - Gradient norm: 1.3219323487689514\n",
      "Epoch 1468 - Objective function: 15.96090470987689 - Loss: 15.88687606265679 - Gradient norm: 1.320810262446634\n",
      "Epoch 1469 - Objective function: 15.959147419687618 - Loss: 15.885108264517712 - Gradient norm: 1.3196904586187213\n",
      "Epoch 1470 - Objective function: 15.957393123092553 - Loss: 15.883343473200927 - Gradient norm: 1.318572933118077\n",
      "Epoch 1471 - Objective function: 15.955641811462296 - Loss: 15.881581680059668 - Gradient norm: 1.3174576817827344\n",
      "Epoch 1472 - Objective function: 15.953893476194013 - Loss: 15.879822876473739 - Gradient norm: 1.3163447004558961\n",
      "Epoch 1473 - Objective function: 15.952148108711398 - Loss: 15.878067053849492 - Gradient norm: 1.3152339849859307\n",
      "Epoch 1474 - Objective function: 15.950405700464588 - Loss: 15.876314203619721 - Gradient norm: 1.3141255312263693\n",
      "Epoch 1475 - Objective function: 15.948666242930042 - Loss: 15.874564317243575 - Gradient norm: 1.3130193350359045\n",
      "Epoch 1476 - Objective function: 15.946929727610497 - Loss: 15.872817386206476 - Gradient norm: 1.3119153922783886\n",
      "Epoch 1477 - Objective function: 15.945196146034876 - Loss: 15.871073402020057 - Gradient norm: 1.31081369882283\n",
      "Epoch 1478 - Objective function: 15.943465489758205 - Loss: 15.86933235622206 - Gradient norm: 1.3097142505433903\n",
      "Epoch 1479 - Objective function: 15.941737750361519 - Loss: 15.867594240376249 - Gradient norm: 1.3086170433193847\n",
      "Epoch 1480 - Objective function: 15.940012919451801 - Loss: 15.865859046072364 - Gradient norm: 1.3075220730352775\n",
      "Epoch 1481 - Objective function: 15.938290988661892 - Loss: 15.864126764925995 - Gradient norm: 1.3064293355806798\n",
      "Epoch 1482 - Objective function: 15.936571949650416 - Loss: 15.86239738857854 - Gradient norm: 1.3053388268503476\n",
      "Epoch 1483 - Objective function: 15.934855794101681 - Loss: 15.860670908697097 - Gradient norm: 1.30425054274418\n",
      "Epoch 1484 - Objective function: 15.933142513725622 - Loss: 15.858947316974398 - Gradient norm: 1.303164479167215\n",
      "Epoch 1485 - Objective function: 15.931432100257714 - Loss: 15.857226605128728 - Gradient norm: 1.3020806320296308\n",
      "Epoch 1486 - Objective function: 15.929724545458877 - Loss: 15.85550876490384 - Gradient norm: 1.3009989972467364\n",
      "Epoch 1487 - Objective function: 15.92801984111543 - Loss: 15.85379378806889 - Gradient norm: 1.2999195707389792\n",
      "Epoch 1488 - Objective function: 15.926317979038974 - Loss: 15.852081666418329 - Gradient norm: 1.2988423484319338\n",
      "Epoch 1489 - Objective function: 15.92461895106633 - Loss: 15.85037239177185 - Gradient norm: 1.297767326256304\n",
      "Epoch 1490 - Objective function: 15.92292274905948 - Loss: 15.848665955974315 - Gradient norm: 1.2966945001479193\n",
      "Epoch 1491 - Objective function: 15.921229364905448 - Loss: 15.846962350895645 - Gradient norm: 1.295623866047732\n",
      "Epoch 1492 - Objective function: 15.919538790516253 - Loss: 15.845261568430764 - Gradient norm: 1.294555419901817\n",
      "Epoch 1493 - Objective function: 15.917851017828824 - Loss: 15.843563600499527 - Gradient norm: 1.2934891576613654\n",
      "Epoch 1494 - Objective function: 15.916166038804914 - Loss: 15.841868439046621 - Gradient norm: 1.2924250752826867\n",
      "Epoch 1495 - Objective function: 15.914483845431029 - Loss: 15.840176076041505 - Gradient norm: 1.2913631687272034\n",
      "Epoch 1496 - Objective function: 15.912804429718356 - Loss: 15.838486503478329 - Gradient norm: 1.2903034339614483\n",
      "Epoch 1497 - Objective function: 15.911127783702673 - Loss: 15.836799713375846 - Gradient norm: 1.2892458669570646\n",
      "Epoch 1498 - Objective function: 15.90945389944428 - Loss: 15.835115697777356 - Gradient norm: 1.2881904636908001\n",
      "Epoch 1499 - Objective function: 15.907782769027934 - Loss: 15.833434448750618 - Gradient norm: 1.287137220144509\n",
      "Epoch 1500 - Objective function: 15.906114384562741 - Loss: 15.831755958387756 - Gradient norm: 1.2860861323051456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x129e42260>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_l2 = 0.001\n",
    "loss = 'MSE'\n",
    "regularization = 'l2'\n",
    "stopping_criterion = 'max_epochs'\n",
    "max_epochs = 1500\n",
    "verbose = True\n",
    "\n",
    "mlp = MLP([40, 20], 9, 2, task = \"regression\", activation_function = 'sigm')\n",
    "optimizer = HBG(mlp, loss, regularization, stopping_criterion)\n",
    "\n",
    "mlp.initialize()\n",
    "optimizer.initialize(alpha = 1e-4, beta = 0.9, stopping_value = max_epochs, alpha_l2 = alpha_l2, verbose = verbose)\n",
    "optimizer.fit_model(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize = (10,5))\n",
    "\n",
    "ax.plot(optimizer.obj_history, label = 'HBG objective function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
